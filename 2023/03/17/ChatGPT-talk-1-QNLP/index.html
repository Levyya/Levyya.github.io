<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>ChatGPT--talk (1) QNLP | Levy's blog</title><meta name="keywords" content="ChatGPT"><meta name="author" content="Levy"><meta name="copyright" content="Levy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一些Demo  点击查看回复           System.out.println(&quot;This is a test code line.&quot;);             Q: 请你基于pytorch设计一个量子启发式语言模型QNN，这个模型需要能够处理文本分类任务   点击查看回复    好的，我将提供一个基本的框架，让您可以根据需要进行调整和完善。 首先，我们需要安装必要的库：PyTorch,">
<meta property="og:type" content="article">
<meta property="og:title" content="ChatGPT--talk (1) QNLP">
<meta property="og:url" content="https://levyya.github.io/2023/03/17/ChatGPT-talk-1-QNLP/index.html">
<meta property="og:site_name" content="Levy&#39;s blog">
<meta property="og:description" content="一些Demo  点击查看回复           System.out.println(&quot;This is a test code line.&quot;);             Q: 请你基于pytorch设计一个量子启发式语言模型QNN，这个模型需要能够处理文本分类任务   点击查看回复    好的，我将提供一个基本的框架，让您可以根据需要进行调整和完善。 首先，我们需要安装必要的库：PyTorch,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg">
<meta property="article:published_time" content="2023-03-17T07:50:55.000Z">
<meta property="article:modified_time" content="2023-03-17T12:08:55.000Z">
<meta property="article:author" content="Levy">
<meta property="article:tag" content="ChatGPT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://levyya.github.io/2023/03/17/ChatGPT-talk-1-QNLP/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ChatGPT--talk (1) QNLP',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-17 20:08:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background_color.css"><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Levy's blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/%E9%98%BF%E5%B0%BC%E4%BA%9A.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">88</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Levy's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ChatGPT--talk (1) QNLP</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-17T07:50:55.000Z" title="发表于 2023-03-17 15:50:55">2023-03-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-17T12:08:55.000Z" title="更新于 2023-03-17 20:08:55">2023-03-17</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ChatGPT--talk (1) QNLP"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一些Demo"><a href="#一些Demo" class="headerlink" title="一些Demo"></a>一些Demo</h2><blockquote>
<details>
<summary>点击查看回复</summary>
    <div style="background-color:#ecf7fe">
    <pre><code>
System.out.println("This is a test code line.");
    </code></pre>
    </div>
</details>
</blockquote>
<p><strong>Q: 请你基于pytorch设计一个量子启发式语言模型QNN，这个模型需要能够处理文本分类任务</strong></p>
<blockquote>
<details>
<summary>点击查看回复</summary>

<div style="background-color:#ecf7fe">

<p>好的，我将提供一个基本的框架，让您可以根据需要进行调整和完善。</p>
<p>首先，我们需要安装必要的库：PyTorch, PennyLane, numpy, sklearn, matplotlib等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install torch pennylane numpy sklearn matplotlib</span><br></pre></td></tr></table></figure>

<p>接下来，我们可以开始处理数据集。这里以IMDB数据集为例，使用PyTorch的torchtext库进行数据处理和加载。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">TEXT = torchtext.data.Field(tokenize=<span class="string">&#x27;spacy&#x27;</span>, lower=<span class="literal">True</span>)</span><br><span class="line">LABEL = torchtext.data.LabelField(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">25000</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">train_iterator, test_iterator = torchtext.data.BucketIterator.splits(</span><br><span class="line">(train_data, test_data), batch_size=BATCH_SIZE, device=device)</span><br></pre></td></tr></table></figure>

<p>接下来，我们可以搭建量子启发式语言模型QNN。这里使用PennyLane库来实现QNN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pennylane <span class="keyword">as</span> qml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定量子神经网络结构</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">qnn_layer</span>(<span class="params">inputs, weights</span>):</span><br><span class="line">    qml.templates.AngleEmbedding(inputs, wires=<span class="built_in">range</span>(n_qubits))</span><br><span class="line">    qml.templates.BasicEntanglerLayers(weights, wires=<span class="built_in">range</span>(n_qubits))</span><br><span class="line">    <span class="keyword">return</span> [qml.expval(qml.PauliZ(i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_qubits)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QNNTextClassifier</span>(torch.nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_qubits, vocab_size, emb_dim, hidden_dim, n_classes</span>):</span><br><span class="line">  <span class="built_in">super</span>().__init__()</span><br><span class="line">  self.embedding = torch.nn.Embedding(vocab_size, emb_dim)</span><br><span class="line">  self.qnn_layer = qml.qnode(qnn_layer, device=qml.device(<span class="string">&quot;default.qubit&quot;</span>, wires=n_qubits))</span><br><span class="line">  self.linear1 = torch.nn.Linear(n_qubits, hidden_dim)</span><br><span class="line">  self.linear2 = torch.nn.Linear(hidden_dim, n_classes)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">  x = self.embedding(inputs)</span><br><span class="line">  x = torch.transpose(x, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">  qnn_inputs = torch.unbind(x, dim=<span class="number">0</span>)</span><br><span class="line">  qnn_weights = [qml.init.strong_ent_layers_normal(n_qubits, layer) <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]</span><br><span class="line">  qnn_results = [self.qnn_layer(qnn_inputs, qnn_weights)]</span><br><span class="line">  qnn_outputs = torch.stack(qnn_results, dim=<span class="number">0</span>)</span><br><span class="line">  qnn_outputs = torch.transpose(qnn_outputs, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">  x = self.linear1(qnn_outputs)</span><br><span class="line">  x = torch.relu(x)</span><br><span class="line">  x = self.linear2(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>接下来，我们可以进行模型训练和测试。这里使用交叉熵损失函数和Adam优化器进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置模型参数</span></span><br><span class="line">n_qubits = <span class="number">4</span></span><br><span class="line">n_layers = <span class="number">2</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(TEXT.vocab)</span><br><span class="line">emb_dim = <span class="number">100</span></span><br><span class="line">hidden_dim = <span class="number">32</span></span><br><span class="line">n_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型并设定优化器和损失函数</span></span><br><span class="line">model = QNNText</span><br></pre></td></tr></table></figure>
</div>

</details>


</blockquote>
<p><strong>Q: 请用python构造一个量子启发式复数值神经网络</strong></p>
<blockquote>
<details>
<summary>点击查看回复</summary>
    <div style="background-color:#ecf7fe">
量子启发式复数值神经网络是一种新兴的深度学习方法，用于解决复杂的分类、回归和优化问题。它利用量子计算的思想，采用复数值权重和门控机制来增强网络的表达能力。在Python中，可以使用TensorFlow Quantum库来构建量子启发式复数值神经网络。

<p>下面是一个简单的代码示例，用于构建一个量子启发式复数值神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_quantum <span class="keyword">as</span> tfq</span><br><span class="line"></span><br><span class="line">qubits = tfq.util.qubit_count([input_shape])</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Input(shape=input_shape),</span><br><span class="line">    tfq.layers.PQC(</span><br><span class="line">        model_circuit=tfq.layers.ControlledPQC(</span><br><span class="line">            control_qids=[<span class="number">0</span>],</span><br><span class="line">            operations=[</span><br><span class="line">                tfq.layers.AddCircuit(</span><br><span class="line">                    cirq.Circuit(</span><br><span class="line">                        cirq.X(q) <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(qubits)</span><br><span class="line">                    )</span><br><span class="line">                ),</span><br><span class="line">                tfq.layers.ControlledPQC(</span><br><span class="line">                    control_qids=[<span class="number">1</span>],</span><br><span class="line">                    operations=[</span><br><span class="line">                        tfq.layers.AddCircuit(</span><br><span class="line">                            cirq.Circuit(</span><br><span class="line">                                cirq.Y(q) <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(qubits)</span><br><span class="line">                            )</span><br><span class="line">                        ),</span><br><span class="line">                        tfq.layers.Dense(</span><br><span class="line">                            <span class="number">2</span>,</span><br><span class="line">                            activation=<span class="string">&quot;softmax&quot;</span>,</span><br><span class="line">                            dtype=tf.complex64,</span><br><span class="line">                            name=<span class="string">&quot;output&quot;</span></span><br><span class="line">                        )</span><br><span class="line">                    ],</span><br><span class="line">                    differentiator=tfq.differentiators.ParameterShift()</span><br><span class="line">                )</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">        initializer=tf.keras.initializers.RandomUniform(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        repetitions=<span class="number">4</span></span><br><span class="line">    )</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss=tf.keras.losses.CategoricalCrossentropy(),</span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">    metrics=[tf.keras.metrics.CategoricalAccuracy()]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>以上代码使用了TensorFlow Quantum库中的tfq.layers.PQC类来定义量子启发式复数值神经网络。这个网络包含两个tfq.layers.ControlledPQC层，其中第一个层实现X门操作，第二个层实现Y门和Dense操作。这个网络还包含一个参数repetitions，用于指定网络的重复次数。</p>
<p>需要注意的是，这个示例代码仅仅是一个简单的示例，实际使用时需要根据具体的问题进行适当的修改。同时，由于量子计算和复数运算都是比较复杂的数学理论，因此建议使用者具备相关的数学和物理基础知识。<br> </p></div><p></p>
</details>
</blockquote>
<p><strong>Q: 量子启发式神经网络和经典神经网络的区别以及它的优势</strong></p>
<blockquote>
<details>
<summary>点击查看回复</summary>
<div style="background-color:#ecf7fe">
量子启发式神经网络（Quantum-inspired Neural Network，QINN）和经典神经网络（Classical Neural Network，CNN）都是一种基于神经网络的模型，但它们在计算原理和性能方面存在显著差异。

<p>首先，量子启发式神经网络利用了量子计算的思想，通过模拟量子行为来完成计算，而经典神经网络则使用传统的计算方式。在实现上，量子启发式神经网络通常需要借助量子计算库或者量子计算器件来实现。</p>
<p>其次，量子启发式神经网络在处理某些问题时具有经典神经网络无法比拟的优势。例如，量子启发式神经网络在处理大规模数据时可以更加高效地进行计算，而且能够更好地处理非线性问题。此外，量子计算的一些特性，如量子并行性、量子纠缠等，也可以为量子启发式神经网络的计算提供额外的优势。</p>
<p>另外，量子启发式神经网络还可以借鉴量子计算的一些算法和技术来优化网络结构和参数调整等方面。例如，量子主成分分析（Quantum Principal Component Analysis，QPCA）可以用来优化神经网络的权重和偏置参数，量子变分算法（Variational Quantum Algorithm，VQA）可以用来训练神经网络的参数。</p>
<p>总的来说，量子启发式神经网络相较于经典神经网络具有更高效、更灵活和更精确的计算能力，这使得它可以在某些任务上取得比经典神经网络更好的表现。然而，目前量子启发式神经网络的应用还处于起步阶段，需要进一步探索和发展。</p>
</div>
</details>

</blockquote>
<p><strong>Q: 请你描述一下量子启发式自然语言处理的研究背景与意义</strong></p>
<blockquote>
<details>
<summary>点击查看回复</summary>
<div style="background-color:#ecf7fe">
自然语言处理（NLP）是人工智能领域的一个重要分支，它致力于让计算机理解和处理人类语言。在NLP中，许多任务都涉及到对大规模文本数据的处理和分析，例如文本分类、情感分析、语言模型等。传统的NLP方法通常基于经典计算机，但随着量子计算的发展，越来越多的研究者开始探索如何将量子计算的优势应用于NLP领域。

<p>量子启发式自然语言处理（Quantum-inspired Natural Language Processing，QINLP）是一种利用量子计算思想和技术进行自然语言处理的新兴研究领域。QINLP并非直接使用量子计算机进行计算，而是受到量子计算理论和算法的启发，设计出一些基于经典计算机的算法和模型来解决NLP中的问题。QINLP的研究意义在于，通过借鉴量子计算的思想，可以提高NLP任务的处理效率和精度，同时为NLP和量子计算之间的交叉学科提供了新的研究方向。</p>
<p>QINLP的研究内容主要包括：量子语言模型、量子词向量表示、基于量子计算的文本分类、基于量子计算的机器翻译、量子信息检索等。例如，一些研究者已经尝试使用量子主成分分析（Quantum Principal Component Analysis，QPCA）来生成词向量，或者使用量子变分算法（Variational Quantum Algorithm，VQA）来训练文本分类模型。</p>
<p>总之，QINLP作为一个新兴的研究领域，不仅有助于提高NLP任务的效率和精度，还为NLP和量子计算之间的交叉学科提供了新的研究方向和思路。</p>
</div>
</details>
</blockquote>
<p><strong>Q: can you build a quantum-inspired language model with Python code</strong></p>
<blockquote>
<details style="background-color:#ecf7fe">
<summary>点击查看回复</summary>

<p>Yes, you can build a quantum-inspired language model using Python code. Here is a simple example of a quantum-inspired language model that uses the Quantum Language Model (QLM) framework developed by IBM:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> qiskit <span class="keyword">import</span> QuantumRegister, ClassicalRegister, QuantumCircuit, Aer</span><br><span class="line"><span class="keyword">from</span> qiskit.circuit.library <span class="keyword">import</span> ZZFeatureMap</span><br><span class="line"><span class="keyword">from</span> qiskit.aqua.components.optimizers <span class="keyword">import</span> L_BFGS_B</span><br><span class="line"><span class="keyword">from</span> qiskit.aqua.algorithms <span class="keyword">import</span> VQE</span><br><span class="line"><span class="keyword">from</span> qiskit.aqua <span class="keyword">import</span> QuantumInstance</span><br><span class="line"><span class="keyword">from</span> qiskit.providers.aer <span class="keyword">import</span> QasmSimulator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a simple corpus of text</span></span><br><span class="line">corpus = [<span class="string">&quot;The quick brown fox jumps over the lazy dog&quot;</span>,</span><br><span class="line">       <span class="string">&quot;This is a test sentence&quot;</span>,</span><br><span class="line">       <span class="string">&quot;Natural language processing is an interesting field&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a feature map using the ZZFeatureMap from Qiskit</span></span><br><span class="line">feature_map = ZZFeatureMap(feature_dimension=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a quantum circuit to encode the text into a quantum state</span></span><br><span class="line">qr = QuantumRegister(feature_map.num_qubits)</span><br><span class="line">cr = ClassicalRegister(feature_map.num_qubits)</span><br><span class="line">qc = QuantumCircuit(qr, cr)</span><br><span class="line">qc.append(feature_map, [qr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_map.num_qubits)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a quantum instance for running the VQE algorithm</span></span><br><span class="line">backend = Aer.get_backend(<span class="string">&#x27;qasm_simulator&#x27;</span>)</span><br><span class="line">quantum_instance = QuantumInstance(backend, shots=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a cost function that measures the similarity between two quantum states</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost_function</span>(<span class="params">params</span>):</span><br><span class="line"> qc_params = np.array(params)</span><br><span class="line"> qc_params = qc_params.reshape(feature_map.num_parameters, -<span class="number">1</span>)</span><br><span class="line"> qc_params = qc_params.T[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"> <span class="comment"># Create a new circuit and apply the parameterized gates to it</span></span><br><span class="line"> qc_new = QuantumCircuit(qr, cr)</span><br><span class="line"> qc_new.append(feature_map.bind_parameters(qc_params), [qr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_map.num_qubits)])</span><br><span class="line"></span><br><span class="line"> <span class="comment"># Compute the fidelity between the original circuit and the new circuit</span></span><br><span class="line"> job = backend.run(qc.combine(qc_new.inverse()), shots=<span class="number">1024</span>)</span><br><span class="line"> result = job.result().get_counts()</span><br><span class="line"> p0 = result.get(<span class="string">&#x27;00&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> p1 = result.get(<span class="string">&#x27;01&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> p2 = result.get(<span class="string">&#x27;10&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> p3 = result.get(<span class="string">&#x27;11&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> f = <span class="number">0.25</span> * (p0 + p3 - p1 - p2)</span><br><span class="line"> <span class="keyword">return</span> <span class="number">1</span> - f</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the VQE algorithm to optimize the circuit parameters</span></span><br><span class="line">optimizer = L_BFGS_B(maxfun=<span class="number">1000</span>)</span><br><span class="line">vqe = VQE(feature_map, optimizer)</span><br><span class="line">result = vqe.run(quantum_instance)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the optimized circuit parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimized circuit parameters:&quot;</span>, result.optimal_point)</span><br></pre></td></tr></table></figure>

<p>This code creates a feature map using the ZZFeatureMap class from Qiskit, which maps the input text to a quantum state. It then defines a cost function that measures the similarity between two quantum states, and uses the Variational Quantum Eigensolver (VQE) algorithm to optimize the parameters of the quantum circuit. Finally, the code prints the optimized circuit parameters.</p>
<p>Note that this is just a simple example, and there are many ways to build a quantum-inspired language model using Python and other programming languages. The details of the implementation will depend on the specific framework and techniques used.</p>
</details>
</blockquote>
<p><strong>Q: what is the quantum-inspired language model</strong></p>
<blockquote>
<details>
<summary>点击查看回复</summary>
    <div style="background-color:#ecf7fe">
A quantum-inspired language model is a type of natural language processing (NLP) model that is inspired by the principles of quantum computing. Unlike a true quantum language model, which uses quantum bits (qubits) and quantum gates to process information, a quantum-inspired language model is a classical model that incorporates some of the mathematical concepts and techniques from quantum computing.

<p>One example of a quantum-inspired language model is the Quantum Language Model (QLM) developed by IBM. The QLM uses a mathematical framework called quantum probability theory to represent and manipulate natural language data. This framework allows the QLM to model the complex relationships between words in a sentence and to capture the inherent uncertainty and ambiguity of natural language.</p>
<p>Another example of a quantum-inspired language model is the Variational Quantum Language Model (VQLM) developed by researchers at the University of Oxford. The VQLM uses a variational quantum circuit to generate word embeddings, which are then used to construct a language model. The VQLM is designed to work with small quantum devices and can be trained using a classical optimizer.</p>
<p>While quantum-inspired language models do not provide the same computational advantages as true quantum language models, they are still an active area of research and have shown promise in certain NLP tasks, such as language modeling and machine translation. As the field of quantum computing continues to evolve, it is likely that we will see more advanced quantum-inspired language models and other quantum-inspired approaches to natural language processing.</p>
 </div>
</details>
</blockquote>
<p><strong>Q: Tell me the latest researches in quantum natural language processing</strong></p>
<blockquote>
<details>
<summary>点击查看回复</summary>
    <div style="background-color:#ecf7fe">
Quantum Natural Language Processing (QNLP) is a rapidly growing field that explores the application of quantum computing to the analysis and processing of natural language data. While still in its early stages, QNLP research has produced some promising results, and there are several ongoing research projects in this area. Here are a few recent examples:<br><br>
    1. Quantum Language Modeling: Researchers at the University of Southern California have developed a quantum language model that can predict the next word in a sentence using quantum computing techniques. The model was tested on several datasets, including the Penn Treebank dataset, and was found to outperform classical language models in terms of accuracy.<br><br>
    2. Quantum Neural Networks for NLP: Researchers at the University of Toronto have developed a quantum neural network for natural language processing. The network uses quantum computing techniques to perform operations such as matrix multiplication and activation functions, and was tested on several NLP tasks, including sentiment analysis and named entity recognition. The researchers found that the quantum neural network outperformed classical neural networks on some of these tasks.<br><br>
    3. Quantum Sentiment Analysis: Researchers at the University of Waterloo have developed a quantum algorithm for sentiment analysis. The algorithm uses a quantum feature mapping technique to convert sentences into quantum states, which are then classified using a quantum support vector machine. The researchers found that the quantum algorithm outperformed classical sentiment analysis methods on some datasets.<br><br>
    4. Quantum Language Generation: Researchers at the University of Maryland have developed a quantum algorithm for generating coherent text. The algorithm uses a quantum language model to generate a distribution over possible sentences, which is then sampled to generate coherent text. The researchers found that the quantum algorithm generated text that was more coherent than classical language generation methods.<br><br>
These are just a few examples of the latest research in quantum natural language processing. As quantum computing technology continues to advance, it is likely that we will see many more exciting developments in this field in the coming years.
 </div>
</details>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Levy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://levyya.github.io/2023/03/17/ChatGPT-talk-1-QNLP/">https://levyya.github.io/2023/03/17/ChatGPT-talk-1-QNLP/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://levyya.github.io" target="_blank">Levy's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ChatGPT/">ChatGPT</a></div><div class="post_share"><div class="social-share" data-image="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/03/19/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">对话系统</div></div></a></div><div class="next-post pull-right"><a href="/2023/03/16/AvWing-record-20230316/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AvWing record--20230316</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/02/22/ChatGPT/" title="ChatGPT"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-22</div><div class="title">ChatGPT</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/%E9%98%BF%E5%B0%BC%E4%BA%9A.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Levy</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">88</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Levyya"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">好好生活！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9BDemo"><span class="toc-number">1.</span> <span class="toc-text">一些Demo</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/11/12/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="Hexo 常用命令"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hexo 常用命令"/></a><div class="content"><a class="title" href="/2023/11/12/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" title="Hexo 常用命令">Hexo 常用命令</a><time datetime="2023-11-12T02:18:11.608Z" title="发表于 2023-11-12 10:18:11">2023-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/12/Vue/" title="无题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2023/11/12/Vue/" title="无题">无题</a><time datetime="2023-11-12T02:17:22.620Z" title="发表于 2023-11-12 10:17:22">2023-11-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/05/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/" title="代码整洁之道"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="代码整洁之道"/></a><div class="content"><a class="title" href="/2023/06/05/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/" title="代码整洁之道">代码整洁之道</a><time datetime="2023-06-05T07:33:02.000Z" title="发表于 2023-06-05 15:33:02">2023-06-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/22/LLaMA%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3/" title="LLaMA模型相关"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLaMA模型相关"/></a><div class="content"><a class="title" href="/2023/05/22/LLaMA%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3/" title="LLaMA模型相关">LLaMA模型相关</a><time datetime="2023-05-22T08:35:00.000Z" title="发表于 2023-05-22 16:35:00">2023-05-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/11/k8s/" title="k8s"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="k8s"/></a><div class="content"><a class="title" href="/2023/04/11/k8s/" title="k8s">k8s</a><time datetime="2023-04-11T08:07:51.000Z" title="发表于 2023-04-11 16:07:51">2023-04-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: #ace0f9"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Levy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="好困,富强,公正,法治,❤️国,敬业,明主,友善,自由,平等,文明,和谐" data-fontsize="20px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>