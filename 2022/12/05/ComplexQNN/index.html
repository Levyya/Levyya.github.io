<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>ComplexQNN | Levy's blog</title><meta name="keywords" content="论文"><meta name="author" content="Levy"><meta name="copyright" content="Levy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ComplexQNN: A Complex-valued Quantum-inspired Language Model   中文版摘要问题：现有的量子启发式模型都是基于振幅相位嵌入，将词语映射到希尔伯特空间中。而量子计算理论中，量子态对应的向量都是复数值，所以需要使用欧拉公式进行转换。 想法：复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等">
<meta property="og:type" content="article">
<meta property="og:title" content="ComplexQNN">
<meta property="og:url" content="https://levyya.github.io/2022/12/05/ComplexQNN/index.html">
<meta property="og:site_name" content="Levy&#39;s blog">
<meta property="og:description" content="ComplexQNN: A Complex-valued Quantum-inspired Language Model   中文版摘要问题：现有的量子启发式模型都是基于振幅相位嵌入，将词语映射到希尔伯特空间中。而量子计算理论中，量子态对应的向量都是复数值，所以需要使用欧拉公式进行转换。 想法：复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg">
<meta property="article:published_time" content="2022-12-05T09:24:03.000Z">
<meta property="article:modified_time" content="2023-02-21T07:57:42.102Z">
<meta property="article:author" content="Levy">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://levyya.github.io/2022/12/05/ComplexQNN/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ComplexQNN',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-21 15:57:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background_color.css"><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="Levy's blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/%E9%98%BF%E5%B0%BC%E4%BA%9A.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">76</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Levy's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ComplexQNN</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-05T09:24:03.000Z" title="发表于 2022-12-05 17:24:03">2022-12-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-21T07:57:42.102Z" title="更新于 2023-02-21 15:57:42">2023-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ComplexQNN"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div style="font-family:verdana;font-size: 22px"> ComplexQNN: A Complex-valued Quantum-inspired Language Model </div>

<h2 id="中文版"><a href="#中文版" class="headerlink" title="中文版"></a>中文版</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>问题：现有的量子启发式模型都是基于振幅相位嵌入，将词语映射到希尔伯特空间中。而量子计算理论中，量子态对应的向量都是复数值，所以需要使用欧拉公式进行转换。</p>
<p>想法：复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等。实际上，复数值神经网络在表示能力上要优于实数值神经网络，适合对复杂的自然语言进行建模。另一方面，量子启发式模型是定义在希尔伯特空间下的，它同样是一个复数空间，因此可以很自然地基于复数值神经网络构建量子启发式模型。于是我们提出了一种新的模型ComplexQNN，它可以在实部和虚部嵌入异构的语义信息，从而提升模型的表达能力。</p>
<p>实验：我们在六个情感分类数据集上进行了实验。相比于一些经典的模型如ELMo，BERT和RoBERTa，ComplexQNN能够取得竞争性的结果。</p>
<hr>
<h3 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h3><blockquote>
<p>引用：”NLP Meets Quantum Physis”</p>
<p>量子启发式神经网络模型和现有的神经网络模型之间的主要区别在于，前者使用描述语言特征的量子理论的数学框架，然后将量子力学概念来描述的这些特征作为神经网络的输入。</p>
<p>使用量子力学概念来描述特征具有更好的可解释性，因为他们具有更透明的物理解释，也更有利于后续的神经网络提取有用的信息。</p>
<p>适用性，借鉴量子力学的数学框架，而处理数据时不受量子计算操作的限制。</p>
</blockquote>
<p>情感分析方法经历了早期的机器学习算法到目前流行的深度学习算法，早期的机器学习方法通过特征选择算法进行文本的特征向量学习，如词袋模型（Bag of Words, BOW)，词频-逆文档频率（Term Frequency-Inverse Document Frequency，TF-IDF），这些方法考虑了词语的出现频率，但是丢失了词语的顺序和上下文信息。目前的深度学习方法基于连续词袋模型（CBOW）和Skip-gram，通过词嵌入层（Word Embedding）将离散的词语映射到高维向量空间，再连接其它网络模块进行特征学习，能够保留文本的上下文信息。按处理的文本序列长度分类，可分为句子级情感分类和篇章级。</p>
<p>量子启发式模型是结合了量子计算理论和深度学习理论构造的一种新的神经网络模型。它是在NISQ时代量子硬件发展受限提出的一种启发式方法，借助量子计算理论的思想，而采用经典的计算方式实现。量子启发式语言模型是对自然语言进行建模，它受到自然语言和量子系统微粒的相似性启发，如语言的一词多义现象和微粒的叠加态，语言随时间、空间变化而变化和微粒随时间改变不断演化，语言在特定场景下语义被确定和微粒被测量后坍塌到某种基态等。量子启发式语言模型相比于经典的神经网络模型，量子启发式语言模型更加符合自然语言的特性，具有更好的可解释性和指数级加速的潜能，而且量子启发式语言模型是定义在希尔伯特空间下的，它具有更强的表示能力。</p>
<p>近年来，自然语言处理（Natural Language Processing, NLP）受益于深度学习的发展，在包括情感分析、问题回答、机器翻译、文本生成等诸多领域取得重大进展。循环神经网络（RNN）以及长短期记忆（LSTM）和循环门控单元（GRU），可以学习上下文之间的关联信息，是NLP中常用的神经网络模块，不过循环神经网络存在梯度爆炸和梯度消失的问题，难以构建深度神经网络。Transformer基于注意力机制（Self-Attention）和残差结构，能够关注到数据中的重点信息并且记忆前面学习的知识，非常适合构建深度神经网络。自然语言处理目前使用的主要技术是基于Transformer构建的预训练语言模型，常见预训练语言模型有BERT、RoBERTa、GPT等。这些预训练模型可用作文本编码器，再通过微调策略，可以轻易适应不同的下游任务。目前，NLP研究方向开始朝向更大的数据集，更大的模型方面发展，像GPT-3，训练的数据集规模达到750GB，参数量更是高达1750亿。虽然计算能力在逐年提升，但面对如此庞大的数据集以及参数量超千亿的模型，研究者仍然会感到束手束脚。</p>
<p>量子计算是一种全新的计算理论，已经有工作证明在一些任务上，量子计算机具有指数级计算复杂度加速优势，比如著名的Shor算法，能够以多项式复杂度完成整数的质因子分解，这威胁到了经典的基于RSA加密算法的通信过程。当前也有一些工作基于量子计算理论对自然语言建模，这主要有两方面原因，一是人类语言和量子系统具有很多的相似性，比如语言的歧义性和量子叠加态，语言的进化性和量子态的演化；二是量子计算具有潜在的指数级加速优势，这对目前需要花费大量资源训练网络的预训练方法来说非常有吸引力。目前已经有一些关于量子自然语言处理的工作，如剑桥的Bob Coecke 提出DisCoCat以及lambeq，将自然语言编码成字符串后，再编码为量子线路，通过参数化量子线路学习，可以实现文本分类任务。现阶段，量子机器学习的实验进展因为量子计算设备中量子比特与纠错能力的限制还停留在初步阶段。因此，运行在真实的量子计算机上，能够处理的文本规模还很小，只能处理包含10几个词的100条句子规模的数据集任务。另一方面，还有研究者基于量子计算理论数学框架构建量子启发式模型，这种算法模型不需要运行在真实的量子计算机上，只是借鉴了量子计算理论中的相关概念来帮助自然语言建模，因此不会受到硬件发展的束缚。</p>
<p>量子启发式模型是基于量子计算理论的数学框架对自然语言进行建模，将自然语言类比于量子系统，同时使用经典神经网络模型去模拟这一过程。然而，现有的量子语言模型如NNQLM，ICWE等，是通过振幅相位编码模拟量子态的构建，再利用欧拉公式得到量子态的复数值表示。实际上，直接使用复数值神经网络进行构建量子启发式模型也是可行的。有许多研究者在研究复数值神经网络，Chiheb Trabelsi et al. 提出了深度复数值卷积网络，并在图像分类、音乐转录和语音频谱预测任务取得了不错的性能。</p>
<p>正是考虑到量子计算理论是定义在希尔伯特空间上，量子态对应的向量表示中每一个维度都是复数值，而且很少有工作基于复数值神经网络来构造。因此，为了更加合理地构建量子启发式模型，以及丰富模型的表达能力，我们基于复数值神经网络，提出了复数值量子启发式模型（ComplexQNN），省略了振幅相位的模拟过程，简化了量子启发式模型的开发。我们基于提出的模型进行了广泛实验，使用多个情感分类数据集对模型进行测评，并和经典的模型进行比较，证明了我们模型的有效性。</p>
<p>综上所述，我们总结了这项工作的核心内容：完全基于复数神经网络构建了一个新的量子启发式模型ComplexQNN，它更符合量子计算理论。情感分类任务上的实验结果验证了ComplexQNN的有效性，在6个情感分类数据集上的实验结果证明，ComplexQNN具有良好的性能，与经典的TextCNN、BiLSTM、ELMo、BERT和RoBERTa相比都有竞争性的优势。</p>
<p>本文的其余部分组织如下。第2节概述了背景和相关工作。启发式程序在第3节中介绍，并在第4节中评估和讨论。第五节对全文进行了总结。</p>
<hr>
<h3 id="第二章-相关工作"><a href="#第二章-相关工作" class="headerlink" title="第二章 相关工作"></a><strong>第二章 相关工作</strong></h3><p>这一章，我们会介绍与复数值量子启发式神经网络有关的知识，包括量子计算,复数值神经网络,和量子启发式神经网络的研究进展。</p>
<p>量子计算是基于量子力学而非经典物理学的思想的一种新型计算方式。1984年，David Deutsch首次提出通用量子计算机的概念，量子计算机从速度上对经典计算机有本质的超越。传统计算机可以模拟量子计算机，但是效率不是很高。1994年，Peter Shor提出量子计算机可以解决寻找整数的素因子问题和解决所谓离散对数问题。1995年，Lov Grover证明量子计算机上可以加速在没有结构的搜索空间上进行搜索的问题。</p>
<p>我们将介绍量子计算中的一些基本概念，如量子态、量子系统、量子态演化和量子测量。</p>
<p><strong>量子机器学习</strong></p>
<p>将数据编码与纠缠量子线路相结合，通过测量来推理数据实例的类标签。对主体线路进行传统的特征描述和存储，并对线路中的参数进行混合量子或传统训练。</p>
<p>量子线路由单量子比特旋转门和双量子比特受控旋转门组成，其中旋转门中的旋转角度是可学习的参数。旋转门和受控旋转门对于量子计算来说是通用的，任何酉矩阵都可以分解成由一组特定量子门构成的线路。</p>
<p><strong>量子启发式模型</strong></p>
<p>Sordoni等人于2013年提出了量子语言模型，这是量子概率理论在自然语言处理的第一个实际应用。QLM将量子理论和自然语言处理语言模型相关研究结合了起来，在理论上具有重大的意义。然而，QLM在许多方面具有局限性。比如，term的表示采用的是one-hot形式的向量。相比于现在常用的分布式词向量(distributed representation), one-hot向量没有能力考虑文本的全局语义信息，会占用更多的存储空间，同时也由于自身独特的表示形式也会造成大量存储资源的浪费。其次，QLM难以将通过迭代计算得到的密度矩阵嵌入到一个端到端的可以通过反向传播算法进行训练的神经网络当中，因而很难得到实际的应用。2018年，Peng Zhang等人在QLM的基础之上，利用类量子力学理论建立深度学习网络，提出端到端的类量子神经网络模型(End-to-End Quantum-like Language Models, NNQLM)来完成NLP中的问答匹配任务，是最早的将类量子力学理论和深度学习技术结合，并实现端到端的神经网络模型在NLP中的一次应用。然而，由于模型采用的是实数词向量，因而未能真正意义上地模拟量子微粒量子态，也没能够充分利用量子力学密度矩阵的概率属性。因此NNQLM的研究和实现目的只是将量子力学和NLP的相关理论进行推广。</p>
<p>同样是2018年，Li等人采用复数词向量模拟量子态，提出量子启发式复数词嵌入，并结合量子力学理论基础，使用密度矩阵的形式表示文本语句，使用投影测量来对密度矩阵形式的文本语句进行观察测量，使用测量得到的概率值来推断文本语句的极性，进而完成NLP中的文本分类任务。使用复数词向量表示的密度矩阵完全契合了量子力学理论，基于量子力学基础提出的网络模型提高了网络在自然语言处理任务的可解释性。同时，相比于一些经典的机器学习模型(比如Unigram-TFIDF)，基于复数基向量模拟量子态设计实现的模型在五个英文二分类的数据集上具有更加优异的表现性能。因此，将量子力学理论和深度神经网络相结合的模型是时下的研究热点之一。2019年，Benyou Wang等人又在使用复数词向量模拟量子态的基础之上，实现端到端的神经网络模型来完成问答匹配任务，Benyou Wang等人分别使用复数形式的密度矩阵表示问句和答句，并分别对密度矩阵形式的问句和答句进行投影测量，最后基于投影测量得到的概率值来计算问句和答句的相似性，从而筛选出问题的正确答案。</p>
<p>2020年，Jiang和Zhang提出了一种量子干涉启发的神经网络匹配模型（QINM）用于处理信息提取任务，可以把干涉现象嵌入到信息提取过程中，实验结果显示要优于先前提出的量子启发式信息提取模型和一些神经网络信息提取模型。</p>
<p>2021年，Peng Zhang等人提出了TextTN，一种基于量子理论构建的文本张量网络，用于处理文本分类任务。TextTN可分为两个子模型，首先用将词生成张量网络（word-GTN）将词语编码为向量，随后用句子判别张量网络（sentence-DTN）对句子进行分类。Yazhou Zhang等人提出一种复数值模糊神经网络用于对话讽刺识别，将量子理论成功与模糊逻辑理论结合在一起。Jinjing Shi et al. 提出两种端到端的量子启发式深度神经网络ICWE-QNN 和 CICWE-QNN用于文本分类。模型采用了GRU，CNN和注意力机制改进了量子启发式模型，可以解决CE-Mix模型中忽略文本内部语言特性的问题。</p>
<blockquote>
<p>在量子复杂词嵌入的激励下，提出了可解释复值词嵌入(ICWE)，设计了两个端到端的量子深度神经网络(ICWE- qnn和CICWE-QNN代表基于ICWE的卷积复值神经网络)用于二进制文本分类。它们在NLP应用中具有被证明的可行性和有效性，可以解决CE-Mix[1]模型中由于忽略文本的重要语言特征而导致的文本信息丢失的问题，因为我们的模型采用深度学习算法进行语言特征提取，其中门控循环单元(GRU)提取句子的序列信息，注意机制使模型关注句子中重要的词，卷积层捕捉投影矩阵的局部特征。ICWE-QNN模型避免了词性符号的随机组合，CICWE-QNN充分考虑了投影矩阵的文本特征。在5个基准分类数据集上的实验表明，所提模型比CaptionRep Bow、DictRep Bow和para - phrase等传统模型有更高的准确率，并且在f1评分上也有很好的表现。特别是对于SST、SuBJ、CR和MPQA四组数据集，CICWE-QNN模型的精度均高于量子激励模型CE-Mix。设计量子深度神经网络以提高文本分类性能是一种有意义和有效的探索。</p>
</blockquote>
<p>量子启发式算法只是使用量子理论的数学框架，所以不用运行在真实的量子计算机上，在实用性上要比量子算法好。近年来不断涌现的量子启发式模型应用在NLP表明，这个研究方向是可行的。相较于神经网络模型，量子启发式算法的优势在于能赋予模型物理意义，使模型具有更好的可解释性。此外，量子启发式算法还能嵌入如量子干涉、量子纠缠等量子特性到模型中，进而增强模型的学习能力。</p>
<p>综上，将结合量子力学理论的神经网络应用到NLP领域是可行的。同时，现在的量子启发式模型很少基于复数值神经网络来构造，没有充分利用量子计算理论。这也是我们做这项工作的原因。</p>
<p><strong>1. 量子启发式模型</strong></p>
<p><strong>(1)量子态构建</strong></p>
<p>自然语言中的单词对应量子系统中的单个微粒，如同微粒存在上旋和下旋状态，以狄拉克符号可表示为$|\psi\rangle &#x3D; \alpha|\uparrow\rangle + \beta |\downarrow\rangle$,单词存在不同的词义或情感意义，如”long”可以表示一段遥远的时间记为$|S_1\rangle$,也可以表示相对较大的空间属性记为$|S_2\rangle$，单词”long”可用量子态表示为$|long\rangle &#x3D; \alpha |S_1\rangle + \beta |S_2\rangle$，其中$\alpha, \beta$都是复数，它们的模长$|\alpha|^2, |\beta|^2$表示对应量子基态的概率，且满足$|\alpha|^2+|\beta|^2&#x3D;1$。</p>
<p>对于任意一个单词$t$，其量子态可以表示为<br>$$<br>|t\rangle &#x3D; \sum w_i |e_i\rangle &#x3D; \sum \alpha_i e^{i\beta_i} |e_i\rangle<br>$$<br>上式中$w_i$为语义权重系数，$|e_i\rangle$为该单词的第$i$个语义，$\alpha_i, \beta_i$为$w_i$的极坐标表示，$\alpha_i$为振幅，$\beta_i$为相位。这样的量子态表示中，振幅表示的为语义信息，相位表示的是隐含的特征信息，如情感信息或者多义词信息。基态$|e_i\rangle$为基本的语义。最终，单词表示为多种基态词义构成的叠加态，可以表示词语的不确定性。</p>
<p><strong>(2)特征嵌入</strong></p>
<p>量子启发式模型相比于经典的NLP模型，一个显著区别就是引入了复数词向量。在实验中，复数词向量是通过构建振幅嵌入层和相位嵌入层来实现的，利用欧拉公式$re^{i\theta} &#x3D; r(cos\theta + i sin\theta)$，可以分别得到复数的实部和虚部系数。在先前的工作中，振幅表示语义信息，相位表示隐含特征，振幅往往采用Glove词向量，相位则是按Xavier正态分布随机初始化。这里限制了复数词向量的表示能力，且随机初始化的结果不适用于不同的任务。</p>
<p>我们改进相位的初始化方式，基于不同的下游任务，为相位嵌入层赋予不同的初始化参数。例如，处理情感分析任务中，相位嵌入层的初始化参数为包含情感信息的相位特征；处理词义消歧任务中，相位嵌入层的初始化参数为包含一次多义信息的相位特征。这种针对任务嵌入的特征能够显著提高模型的效果，在第4节实验结果能够看到这一点。要获得这样的特征参数，可以考虑两种方式：（a）选择公开的大规模语料库预训练词向量，如word2vec，Glove等，然后映射到$[0, 2\pi]$上；（b）训练一个朴素的量子启发式模型，抽取复数词向量中相位嵌入层的参数。在我们的实验中，我们采用的是第二种方式，因为第一种预训练词向量往往是固定词嵌入维度的，而第二种方式可以根据需求自定义词嵌入的维度大小。</p>
<p><strong>(3)量子态演化</strong></p>
<p>量子理论中，表示量子系统的量子态是随时间不断变化的，数学上表示为$|\psi\rangle&#x3D;U|\psi\rangle$，其中$U$是一个酉矩阵$U^\dagger U&#x3D;I$。这一特点也同样存在于自然语言中，某些词语会因为一些社会历史原因改变其原本的含义，如”silly”起源于古英语”saelig”，意为“快乐的、无忧无虑的”，而当今的意义已经变成“愚蠢的”。 这种语义的变化可以通过大型语料库训练学习，也可以根据文本上下文信息推测得到。在神经网络模型中，循环神经网络如长短期记忆（LSTM）、门控单元（GRU），能够学习时序数据中较长的内容，非常适合处理文本这样的前后相关联的数据。可使用循环神经网络模型如GRU来模拟量子态的演化，一个词语的量子态会因周围词语变化而改变，这种变化过程可用GRU模拟。GRU将先前词语的状态和当前词语的状态作为输入，中间经过选择性记忆门操作，输出开始至当前词语的状态向量。</p>
<p><strong>(4)量子态测量</strong></p>
<p>前面我们得到了一个句子中每个词语对应的量子态，记为$S&#x3D;{|t_i\rangle}$，下面我们需要得到整个句子的表示。量子系统中，多个微粒能够构成一个混合系统，通常表示为密度矩阵$\rho&#x3D;\sum w_i |\phi_i\rangle\langle \phi_i|$。同样的，我们将一个句子表示为密度矩阵$\rho&#x3D;\sum w_i |t_i\rangle \langle t_i|$，其中$w_i$表示的是第$i$个单词的权重系数，默认为$w_i&#x3D;\frac{1}{l}$（$l$为句子长度）。这一权重系数也可以通过注意力机制，学习到适合任务的权重。</p>
<p>得到表示句子的密度矩阵之后，我们将对句子进行测量操作。首先，我们选择一组满足完备性（$\sum M_m^\dagger M_m &#x3D; I$）的测量基$M_m &#x3D; |\lambda_m\rangle \langle \lambda_m|$。之后，我们利用该组测量基对密度矩阵进行测量，测量结果为$p(m)&#x3D;\langle \lambda_m| \rho |\lambda_m\rangle &#x3D; tr(\rho |\lambda_m\rangle \langle \lambda_m|)$。</p>
<p><strong>(5)分类器</strong></p>
<p>根据上一个过程得到的测量特征，我们使用一个全连接层将结果映射到一个结果集合大小的向量空间，并使用softmax激活函数将结果表示为预测概率。</p>
<p><strong>2. 复数值神经网络</strong></p>
<p>现有的深度学习技术绝大多数是基于实数值运算和表示的，实际上，复数可能具有更丰富的表示能力，有工作证明复数值神经网络具有一些独特的优势：有可能实现更容易地优化，更好的泛化特征，更快的学习，以及允许噪声鲁棒的记忆机制。2018年，Chiheb Trabelsi等人提出深度复数值神经网络，提出了复数值批量归一化和复数值权重初始化等训练复数值神经网络的关键模块，还提出了复数值卷积神经网络架构，并通过图像分类、音乐转录以及语音频谱预测实验进行了模型效果验证。下面简单介绍复数值神经网络的原理，包括复数值表示、复数值激活函数以及复数值卷积神经网络三部分内容：</p>
<p><strong>复数值线性层</strong></p>
<p>线性层也叫全连接层，每个神经元都和上一层所有神经元相连，是神经网络中最常见的网络结构。其计算公式描述为<br>$$<br>f&#x3D;WX+b<br>$$<br>其中$W$表示网络中的权重矩阵，$b$表示网络层中的偏置。C. Trabelsi et al.基于PyTorch库实现了复数值线性层。复数值线性层使用两个实数值线性层分别计算实部和虚部，在输出时基于复数计算原理得到新的实部和虚部。具体计算公式如下所示：<br>$$<br>f_r(X) &#x3D; W_r X + b_r\<br>f_i(X) &#x3D; W_i X + b_i\<br>f_c&#x3D; f_r(X_r) - f_i(X_i) + \bold{i} [f_r(X_i) + f_i(X_r)] \<br>   &#x3D; W_rX_r - W_iX_i + b_r - b_i + \bold{i}[W_rX_i + W_iX_r + b_r + b_i]<br>$$<br>复数值线性层是复数值神经网络的基本模块，后面复数值卷积神经网络和复数值循环神经网络都依赖这个模块。</p>
<p><strong>复数值激活函数</strong></p>
<p>We call Complex ReLU (CReLU) the complex activation that applies separate ReLUs on both of the real and the imaginary part of a neuron, i.e:<br>$$<br>CReLU(z) &#x3D; ReLU(R(z)) + i ReLU(I(z))<br>$$</p>
<p><strong>复数值批正则化</strong><br>$$<br>BN(\hat{x})&#x3D; \gamma \hat{x} + \beta<br>$$</p>
<p>深度网络通常依赖于批处理归一化(Ioffe和Szegedy, 201s)来加速学习。在某些情况下，批归一化是优化模型的必要条件。批处理归一化的标准公式只适用于实际值。在本节中，我们提出了一个可用于复值的批量归一化公式。要将一组复数标准化为标准正态复数分布，仅仅平移和缩放它们，使它们的均值为o，方差为l是不够的。这种归一化不能确保实分量和虚分量的方差相等，得到的分布也不能保证是圆形的;它将是椭圆形的，可能具有高离心率。相反，我们选择将这个问题视为二维矢量的美白问题，这意味着将数据按两个主成分方差的平方根缩放。这可以通过将以o为中心的数据(æ - E])乘以2x2协方差矩阵V的平方根倒数来实现</p>
<p><strong>复数值循环神经网络</strong></p>
<p>循环神经网络不同于线性层，每层的输出都依赖于前面的输出，能够学习前面数据中的信息。RNN常用来处理序列数据，在自然语言处理中有广泛的应用。LSTM和GRU是两种常用的循环神经网络，我们以复数值LSTM为例介绍复数值循环神经网络。如图1所示，LSTM中含有多个门：遗忘门、输入门和输出门，可以选择性地让信息通过。LSTM的计算过程如下所示：<br>$$<br>f_t &#x3D; \sigma(W_{xf} * X_t + W_{hf} * H_{t-1} + b_f), \<br>    i_t &#x3D; \sigma(W_{xi} * X_t + W_{hi} * H_{t-1} + b_i), \<br>    o_t &#x3D; \sigma(W_{xo} * X_t + W_{ho} * H_{t-1} + b_o), \<br>    \widetilde{c_t} &#x3D; \tanh(W_{xc} * x_t + W_{hc} * h_{t-1} + b_c), \<br>    c_t &#x3D; f_t \bigodot c_{t-1} + i_t \bigodot \widetilde{c_t}, \<br>    h_t &#x3D; o_t \bigodot \tanh(c_t)<br>$$<br>其中$\bigodot$表示向量点乘，$f_t$表示遗忘门，用于丢弃过去不重要的信息，$i_t$表示输入门，$o_t$表示输出门，$\widetilde{c_t}$表示候选记忆单元，$c_t$表示输出的细胞状态，$H_t$表示输出的隐状态。复数值LSTM需要将遗忘门、输入门和输出门使用复数值线性层实现。Sigmoid激活函数以及tanh函数以及向量乘法也要使用复数值计算。这些基于复数值线性层都是比较容易实现的。</p>
<p><strong>复数值卷积神经网路</strong></p>
<p>卷积神经网络常用在计算机视觉领域，如LeNet，AlexNet，VGG，ResNet，Yolo等，也有用于自然语言处理的TextCNN。卷积神经网络通常包括卷积层、池化层和全连接层。其中卷积层是计算机神经网络的核心，一个卷积层通常包括多个尺寸一致的卷积核，卷积核的个数决定输出的大小。类似于经典的CNN，复数值卷积神经网络也包含复数值卷积层。复数值卷积层包含实部卷积与虚部卷积，计算过程如图1所示。$M_R$和$M_I$分别为实部特征图和虚部特征图，$K_R$和$K_I$分别为实部卷积核和虚部卷积核。复数值卷积层的输出为$M_RK_R-M_IK_I+\bold{i}(M_RK_I+M_IK_R)$。</p>
<p><strong>3. 我们的想法</strong></p>
<p>量子启发式模型现有方法都是基于振幅相位嵌入，将词语映射到希尔伯特空间中，而量子计算理论中，量子态对应的向量都是复数值的，所以需要使用欧拉公式进行转换。复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等。实际上，复数值神经网络在表示能力上要优于实数值神经网络，适合对复杂的自然语言进行建模。另一方面，量子启发式模型是定义在希尔伯特空间下的，它同样是一个复数空间，因此可以很自然地基于复数值神经网络构建量子启发式模型。于是我们提出了一种新的模型ComplexQNN，它可以在实部和虚部嵌入异构的语义信息，从而提升模型的表达能力。我们在情感分类数据集上进行了实验，并和基线模型对比，实验结果现实ComplexQNN能够很好地完成任务，并且相比于XX模型有xx%的精度提升。</p>
<hr>
<h3 id="第三章-正文"><a href="#第三章-正文" class="headerlink" title="第三章 正文"></a>第三章 正文</h3><p><strong>数据集</strong></p>
<p>情感二分类数据集</p>
<p>CR, MPQA, MR, SST, SUBJ</p>
<p>情感多分类数据集</p>
<p>SST-5</p>
<p>【方面级情感分类】</p>
<p><strong>特征提取以及数据表示</strong></p>
<p>文本分词</p>
<p>TF-IDF</p>
<p>word embedding</p>
<p>GloVe</p>
<p>Contextual embedding</p>
<blockquote>
<p>《Quantum-Inspired Complex-Valued Language Models for Aspect-Based Sentiment Classification》</p>
<ol start="4">
<li><p>Complex-Valued Language Models</p>
<p>受量子语言模型的启发，一个词被视为量子系统中的一个物理可观测物，它由一个复值向量表示。在此范围下，将三个均以复值嵌入为输入的QLMs进一步构造为三个强实值基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。将得到的三种复值模型与相应的实值模型进行比较，以衡量其性能。通过对比，我们希望看到嵌入的虚部可以携带超出实部的额外信息，进一步强调引入量子语言模型的重要性。首先，我们愿做一个澄清。本文基于三个典型的实基线，建立了三个复值语言模型。然而，除了被选中的，还有大量不同类型的神经网络用于语言任务。我们希望这三种类型的模型能够阐明复值结构对提高模型性能的影响，并显示进一步探索其他类型神经网络结构的可能性。在本节中，我们首先介绍了实向量空间的复化，将单词编码为复值向量的过程，然后介绍了构造三个实基线的复化的方法。</p>
</li>
</ol>
</blockquote>
<p><strong>ComplexQNN</strong></p>
<p><strong>理论部分</strong></p>
<p>我们提出的复数值量子启发式模型同样也是基于量子计算的数学理论，因此模型中的模块都是定义在希尔伯特空间下。量子启发式模型是对自然语言以量子信息的方式进行建模的，因此首先需要将词语表示为量子态。在单个原子模型中，电子可以处于基态或者是激发态，或是介于两者之间的叠加态。类似的，自然语言因一词多义现象，同样可以表示为叠加态。比如词语$w$，具有$n$种不同的语义 ($n&#x3D;2^m, m\ge0$)，记为$e_i$，那么该词的量子态表示为<br>$$<br>|w\rangle &#x3D; \sum_{i&#x3D;1}^n \alpha|e_i\rangle<br>$$<br>其中$\alpha$是$n$维复向量，而$|\alpha_i|^2$表示词语$w$表示词义$e_i$的概率。以n&#x3D;4为例，<br>$$<br>|w\rangle &#x3D; \sum_{i&#x3D;1}^4 \alpha |e_i\rangle \<br>&#x3D; \alpha_{00}|e_{0}\rangle + \alpha_{01}|e_{1}\rangle + \alpha_{10}|e_{2}\rangle + \alpha_{11}|e_{3}\rangle\<br>&#x3D; [\alpha_{00}, \alpha_{01}, \alpha_{10}, \alpha_{11}] \left[\begin{array}{c} |00\rangle \ |01\rangle \|10\rangle \|11\rangle \\end{array}\right] \<br>&#x3D; \left[\begin{array}{c} \alpha_{00}\ \alpha_{01}\ \alpha_{10}\ \alpha_{11} \end{array}\right]<br>$$<br>从公式可以看到，词语$w$的量子态被映射到$n$维复向量空间中。</p>
<p>一句话通常是多个词语组成的，正如一个量子系统由多个微观粒子组成。一个量子系统在量子计算中通常用密度矩阵来表示。假设句子中含有$m$个词语，那么一个句子$S$的密度矩阵表示为<br>$$<br>\rho &#x3D; |S\rangle\langle S| &#x3D; \sum_{i&#x3D;1}^m\beta|w_i\rangle (\sum_{i&#x3D;1}^m\beta|w_i\rangle)^\dagger<br>$$<br>其中$\beta$是$n$维复向量，$|\beta_i|^2$表示词语$w_i$在句子$S$中的权重。类似于注意力机制，不同的权重有利于神经网络关注句子中重要的词语。在情感分类中，一些形容词如“棒”、“糟糕”、“优秀”等，对最终的预测结果有很大的影响，可以分配较大的权重。</p>
<p>句子被表示为密度矩阵之后，我们希望进一步学习句子内部词语的联系。对应于量子系统，这一操作叫做演化，即量子态随时间或是其它外界干扰而发生改变，通过公式表示为<br>$$<br>\rho^\prime&#x3D;U\rho<br>$$<br>其中，$U$是一个$n\times n$的复值矩阵，$\rho\prime$是经过演化后的系统状态。过去的量子启发式语言模型通常是抽取密度矩阵$\rho$中的实部和虚部，使用循环神经网络或者卷积神经网络分别进行训练，最后整合输出的特征。我们认为这一操作会割裂量子系统中的信息，会导致学习到的特征不完整，不能正确地模拟量子系统状态的改变。因此，在构造量子启发式模型时，我们通过复数值神经网络来模拟量子态的改变，整个演化过程将基于复数值进行计算，输出的结果也将保持复数状态。</p>
<p>最后，量子计算中的测量可以得到量子系统坍塌到一组基态下的概率值，这一过程应用到自然语言处理中的文本分类任务上。假设有一组测量算子${M_i},i\in{1,…,k}$，表示$k$种分类标签，那么句子对应第$i$个标签的测量概率为<br>$$<br>p_i &#x3D; \rho^\dagger M_i^\dagger M_i \rho<br>$$<br>。我们将模型应用到情感分析任务中，进行句子情感极性的验证，任务包含二分类和多分类，在第四章可以看到实验的细节。</p>
<p><strong>模型细节</strong></p>
<p>复数值量子启发式神经网络（ComplexQNN）是基于复数值神经网络和量子计算理论构建的语言模型，用于自然语言处理的下游任务，下面将从模型整体架构、模型具体模块的构造、模型适用的场景三个方面介绍该模型。</p>
<p>首先是ComplexQNN的模型整体架构，图片1描述了模型的整体架构，它包含三个模块：复数词嵌入、量子编码器、分类器。此外，我们还可以从 <strong>图片1</strong> 中了解到ComplexQNN处理文本的整个流程：</p>
<p>0）预处理过程：文本首先通过预处理过程（大小写转换、分词、词索引映射、填充和截断）得到令牌序列（token），另一方面，为了屏蔽掉填充序列带来了额外令牌序列，还需要构建由0和1组成的掩码序列（mask），令牌序列和掩码序列即为ComplexQNN的输入数据；</p>
<p>ComplexQNN的体系结构如图\ref{fig_ComplexQNN}所示。可以看到，它由四个模块组成:复杂嵌入，投影，演化和分类器。ComplexQNN的输入数据首先需要经过大小写转换、分词、词索引映射、填充和截断等预处理得到。此外，为了屏蔽填充序列带来的额外token序列，还需要构造由0和1组成的掩码序列。综上所述，token序列和mask序列是ComplexQNN的输入数据。下面介绍ComplexQNN的四个基本模块:</p>
<p>1）复数词嵌入：复数词嵌入层的作用是将词语对应的令牌号（这是一个整数对应词语在词表中的位置）映射到n维复数向量空间中，其对应于量子计算中的量子态构建过程，每一个词语从离散空间映射到高维希尔伯特空间下，对应一个复数值列向量。</p>
<p>2）投影：投影是把句子中多个离散的词语映射到$n\times n$的复数值空间中。上一步骤中，复数词嵌入已经把词语映射到$n$维复向量空间中。通过公式1，可以计算出句子的密度矩阵表示，其中词语的权重$\beta$可以通过注意力机制训练得到，默认所有的词语取相同的权重。</p>
<p>3）演化：演化过程是模拟量子系统的变化。在量子计算理论中，量子态和密度矩阵的变化是利用量子门实现的。量子门对应一个酉矩阵，矩阵的维度对应其操作的量子比特数（$n&#x3D;2^m$）。在ComplexQNN中，我们要通过复数值线性层、复数值循环神经网络以及复数值卷积神经网络来模拟量子系统的变化。我们在设计演化模块时，令输入和输出的维度都是$n\times n$。因此，在学习句子内部的特征后，不会改变原来量子系统的维度。</p>
<p>4）分类器：分类器的作用是利用之前模块学习到的高维特征预测分类结果。我们可以基于量子计算理论通过测量直接输出预测结果，如公式1所示。这里需要构造一组线性无关的测量基，基态的个数取决于要进行分类的数量。最终模型预测的结果取概率值最大对应测量基的标签。</p>
<p>以上，我们介绍了ComplexQNN的四个必要模块，并展示了文本序列从输入到预测输出的过程。接下来，将介绍模块的具体设计。</p>
<hr>
<p>ComplexQNN需要包含复数词嵌入、投影、演化以及分类器，其中复数词嵌入和演化是模型构建的核心。在我们的实现中，我们使用Allennlp库设计了三个模块：ComplexEmbedder，QuantumEncoder和Classifier。量子编码层中包含了投影和演化操作。</p>
<p>ComplexEmbedder是ComplexQNN的第一个模块，它的输入是被预处理后的文本Token序列，这是一个整数向量。复数词嵌入由实部嵌入层和虚部嵌入层组成。文本Token序列分别经过这两个嵌入层，最后通过公式<br>$$<br>[w_1, …, w_i, …, w_n] &#x3D; [r_1, …, r_i, …, r_n] + \bold{i} \times [i_1, …, i_i, …, i_n]<br>$$<br>得到每一个Token的复数词向量表示。实部嵌入层和虚部嵌入层可以方便地使用经典的词嵌入。</p>
<p>经典的词嵌入层有多种不同的嵌入方式。按照分词的级别，可分为字符级、wordpiece级以及词语级。按训练方式可分为非上下文词嵌入（静态词嵌入）和上下文词嵌入（动态词嵌入）。Word2vec 和 GloVe 是两种经典的非上下文词嵌入方法。上下文词嵌入是基于预训练模型思路，通过微调得到数据集相关的上下文词向量。</p>
<p>我们考虑了不同的方案来构造复数词嵌入层：（1）使用不同的预训练模型作为实部和虚部的嵌入层，如实部使用BERT，虚部使用RoBERTa，这样就可以将不同的信息编码到量子态中。（2）实部和虚部编码不同种类的文本信息，如实部编码wordpiece级别的词向量，虚部使用NLTK库编码词语极性等语义信息。（3）实部编码正向语序特征，虚部编码反向语序特征。</p>
<p>上面第一种方法同时使用了不同的预训练模型提取特征，可以取得最好的实验结果，但是训练需要占用很大的显存空间。第二种和第三种方法训练速度很快，但是效果不如预训练模型。综合考虑实验效果以及训练所需要的资源，最终ComplexEmbedder的实现采用实部使用RoBERTa，虚部采用自训练的词嵌入层。总的来说，我们希望实部和虚部嵌入不同类型的文本特征，充分利用复数值神经网络的异构特性，进而提升模型的语义表达能力。</p>
<p>ComplexQNN的第二个模块是量子编码层，负责投影和演化。前面我们提到过，进行编码前，需要使用投影先计算得到句子的密度矩阵表示，公式展示了计算密度矩阵的过程。演化是模拟量子门操作。这需要满足一些条件，即输入输出维度不变且维度是$2^n$。我们通过以下基本模块来构建我们的编码层：复数值全连接层、复数值循环神经网络和复数值卷积神经网络。我们构建了三种用于编码的中间模块层：复数值深度神经网络编码层、复数值循环神经网络编码层（基于ComplexLSTM）和复数值卷积神经网络编码层。</p>
<p>如图1所示，我们构造了ComplexTextCNN。该模块的输入为投影后的表示句子的密度矩阵$\rho$，我们使用了卷积核大小为$[3,4,5]$，每种卷积核数量为$2$，步长为$1$。然后通过最大池化提取特征，并将不同维度学习到的特征拼接起来。到目前为止，ComplexTextCNN和经典TextCNN最大的区别就是，所有的计算操作都是在复数值网络中进行计算的。最后，为了得到学习特征后的新的句子密度矩阵表示，我们使用复数值全连接网络层，把向量维度恢复成输入时的维度。通过外积操作，得到和输入一样维度的矩阵形式。</p>
<p>第三个模块是分类器，它利用量子计算理论的测量方法，预测模型的输出。具体是通过公式1来实现，我们可以设计不同数量的测量基态来进行文本多分类。后面实验中涉及了多分类，需要根据分类结果的种类数确定基向量的个数。最终的分类结果是取预测概率最大的那个作为最终的预测结果。此外，因为测量后的输出是实数值，我们可以取出每一个标签的概率值组成一组预测向量，然后和其它模型的预测结果拼接在一起，实现模型融合。</p>
<p>ComplexQNN 用于情感分类</p>
<p>情感分类是自然语言处理中的一项常见任务，旨在预测句子对应的情感极性。本文开展这项任务来验证ComplexQNN的实验性能。同时，对一些经典的网络模型进行比较。图\ref{fig_ComplexQNNforSA}是ComplexQNN用于情感分类任务的流程图，并对数据维度进行了标注。首先对文本进行预处理，将大小写归一化，分词并去除停用词;其次，通过复杂的词嵌入层模拟单词的量子态;然后，量子编码器层用于投影和演化。最后，通过分类器的模拟测量操作输出预测结果。</p>
<p><strong>评估指标</strong></p>
<p>在我们的情感分析实验中，五个数据集都是有两种分类结果。因此，我们使用准确率、精度、召回率和F1值作为评估指标去分析实验结果。</p>
<p>$$<br>Accuracy &#x3D; \frac{TP + TN}{TP + TN + FP + FN}<br>\<br>Precision &#x3D; \frac{TP}{TP + FP}<br>\<br>Recall &#x3D; \frac{TP}{TP + FN}<br>\<br>F1-score &#x3D; 2 \times \frac{Precision \times Recall}{Precision + Recall}<br>$$</p>
<p>对于多分类任务<br>$$<br>micro-F1 &#x3D; 2 \times \frac{Recall_m \times Precision_m}{Recall_m + Precision_m}<br>\<br>(3分类为例)<br>macro-F1 &#x3D; \frac{F1-score_1 + F1-score_2 + F1-score_3}{3}<br>$$</p>
<p>macro-F1</p>
<p><strong>损失函数</strong></p>
<p><strong>参数设置</strong></p>
<hr>
<h3 id="第四章-实验"><a href="#第四章-实验" class="headerlink" title="第四章 实验"></a>第四章 实验</h3><p>情感分析是自然语言处理中的常见任务，任务要求预测给定的句子的情感极性。实验使用了五个常见的数据级如<strong>表（）</strong>所示。为了衡量ComplexQNN模型的性能，我们使用TextCNN、TextGRU、BERT、RoBERTa作为对比模型。具体地实验结果如<strong>表（）</strong>所示。</p>
<p>对比模型</p>
<p><strong>GRU</strong></p>
<p><strong>TextCNN</strong></p>
<p><strong>ELMo</strong>（Embeddings from Language Models）由双向LSTM作为基本组件构成，以语言模型（Language Model）为训练目标，通过大语料进行预训练，得到通用的语义表示，再迁移到下游的NLP任务中，可以显著提升下游任务的模型性能。ELMo提供了词级别的语义表示，在很多下游任务中表现优异。</p>
<p><strong>BERT</strong>（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型（pre-trained language model, PLM）。像ELMo和GPT都是自回归模型（Auto Regressive，AR），只能考虑单侧信息，也就是根据上文预测下一个单词，或者根据下文预测前一个单词。而BERT是利用上下文信息，从含噪音的数据中重建原始数据，属于自编码模型（Auto Encoding）。预训练过程中使用了两个任务：掩码语言模型（Masked Language Model）和下一句预测（Next Sentence Prediction）。BERT的输出是句子中每个Token对应的768维的向量，以及一个特殊Token（[CLS]）。</p>
<p><strong>RoBERTa</strong>（Robustly Optimized BERT Pretraining Approach）是BERT的精细调优版本，使用了更大的模型参数、更大的批处理大小、更多的预训练数据，同时改进了训练方法，去掉了下一句预测任务，使用动态掩码和BPE编码（Byte-Pair Encoding），在实验上取得了比BERT更好的效果。</p>
<p>实验结果分析：可以看到ComplexQNN相比于流行的预训练模型并不落于下风，在XX数据集上都有更好的预测精度。另外，我们使用了不同的预训练模型以及相同模型的不同尺寸构建ComplexQNN（GloVe, BERT, RoBERTa；BERT-tiny, BERT-base, BERT-large），实验结果如<strong>图（）</strong>所示。</p>
<blockquote>
<p>As shown in Tab , Model is superior to most quantum-inspired models and LSTM&#x2F;CNN-based models and achieves three best performances out of the four metrics on Wiki-QA and TREC-QA. However, it under-performed slightly worse than XX on XX metric about XX. The possible reason is that although Model has surpassed most models, models have larger parameter scales to ensure the effectiveness of learning.</p>
</blockquote>
<p>消融实验：</p>
<p>为了验证量子启发式模型以及复数值神经网络的有效性，我们构建了消融实验。我们构建了四个模型：</p>
<p>（1）经典卷积神经网络（TextCNN）；</p>
<p>（2）量子启发式模型（QNN）；</p>
<p>（3）复数值神经网络模型（ComplexNN）；</p>
<p>（4）复数值量子启发式模型（ComplexQNN）。</p>
<p><strong>表（）</strong>显示，复数值神经网络和量子启发式模型通常具有和TextCNN近似的实验结果；ComplexQNN具有比以上三个模型更好的性能。</p>
<table>
<thead>
<tr>
<th>model</th>
<th>SST-2</th>
<th>SST-5</th>
</tr>
</thead>
<tbody><tr>
<td>TextCNN</td>
<td>80.9</td>
<td>78.8</td>
</tr>
<tr>
<td>QNN</td>
<td>78.8</td>
<td>75.4</td>
</tr>
<tr>
<td>ComplexTextCNN</td>
<td>75.3</td>
<td>71.7</td>
</tr>
<tr>
<td>ComplexQNN</td>
<td>88.4</td>
<td>83.1</td>
</tr>
</tbody></table>
<p>实验结果（2022.11.28-…）</p>
<table>
<thead>
<tr>
<th align="left">model</th>
<th align="right">CR</th>
<th align="right">MPQA</th>
<th align="right">MR</th>
<th align="right">SST</th>
<th align="right">SUBJ</th>
</tr>
</thead>
<tbody><tr>
<td align="left">TextCNN</td>
<td align="right">acc: <strong>78.8</strong><br> prec: 70.2 <br>recall: 72.3<br> f1: 71.2<br>loss: 0.622</td>
<td align="right">acc: <strong>74.4</strong><br> prec: 72.4<br>recall: 78.8<br> f1: 75.5<br> loss: 0.638</td>
<td align="right">acc: <strong>75.0</strong><br> prec: 73.2<br> recall: 78.8<br> f1: 75.9<br> loss: 0.527</td>
<td align="right">acc: <strong>81.4</strong><br> prec: 81.5<br> recall: 80.3<br> f1: 80.9<br> loss: 0.493</td>
<td align="right">acc: <strong>90.3</strong><br> prec: 92.0<br> recall: 88.3<br> f1: 90.1<br> loss: 0.379</td>
</tr>
<tr>
<td align="left">ComplexTextCNN</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr>
<td align="left">GRU</td>
<td align="right">acc: <strong>80.1</strong><br> prec: 75.7 <br>recall: 66.7<br> f1: 71.0<br>loss: 0.637</td>
<td align="right">acc: <strong>84.3</strong><br> prec: 78.3<br>recall: 69.9<br> f1: 73.3<br> loss: 0.424</td>
<td align="right">acc: <strong>76.0</strong><br> prec: 77.0<br>recall: 74.1<br> f1: 75.5<br> loss: 0.537</td>
<td align="right">acc: <strong>82.5</strong><br> prec: 81.6<br> recall: 83.1<br> f1: 82.3<br> loss: 0.512</td>
<td align="right">acc: <strong>91.7</strong><br> prec: 90.7<br> recall: 92.3<br> f1: 91.8<br> loss: 0.381</td>
</tr>
<tr>
<td align="left">ComplexGRU</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr>
<td align="left">ComplexQNN</td>
<td align="right">acc: <strong>78.2</strong><br> prec: 66.3 <br>recall: 81.0<br> f1: 72.9<br>loss: 0.913</td>
<td align="right">acc: <strong>84.4</strong><br> prec: 76.8<br>recall: 71.7<br> f1: 74.2<br> loss: 0.580</td>
<td align="right">acc: <strong>73.6</strong><br> prec: 70.2<br>recall: 82.0<br> f1: 75.6<br> loss: 0.832</td>
<td align="right">acc: <strong>80.4</strong><br> prec: 80.4<br> recall: 79.4<br> f1: 79.9<br> loss: 0.623</td>
<td align="right">acc: <strong>90.1</strong><br> prec: 89.6<br> recall: 90.8<br> f1: 90.2<br> loss: 0.406</td>
</tr>
<tr>
<td align="left">ELMo</td>
<td align="right">acc: <strong>85.4</strong><br> prec: 80.0<br> recall: 79.8<br> f1: 79.9<br> loss: 0.381</td>
<td align="right">acc: <strong>90.5</strong><br> prec: 88.2<br> recall: 80.4<br> f1: 84.1<br> loss: 0.250</td>
<td align="right">acc: <strong>81.0</strong><br> prec: 77.0<br> recall: 88.3<br> f1: 82.3<br> loss: 0.415</td>
<td align="right">acc: <strong>88.8</strong><br> prec: 89.3<br> recall: 87.6<br> f1: 88.4<br> loss: 0.296</td>
<td align="right">acc: <strong>94.9</strong><br> prec: 95.6<br> recall: 94.1<br> f1: 94.9<br> loss: 0.140</td>
</tr>
<tr>
<td align="left">BERT</td>
<td align="right">acc: <strong>88.8</strong><br> prec: 81.8<br> recall: 88.0<br> f1: 85.2<br> loss: 0.289</td>
<td align="right">acc: <strong>89.5</strong><br> prec: 87.3<br> recall: 77.9<br> f1: 82.3<br> loss: 0.362</td>
<td align="right">acc: <strong>84.9</strong><br> prec: 86.0<br> recall: 83.3<br> f1: 84.6<br> loss: 0.360</td>
<td align="right">acc: <strong>88.9</strong><br> prec: 93.0<br> recall: 83.6<br> f1: 88.0<br> loss: 0.298</td>
<td align="right">acc: <strong>95.2</strong><br> prec: 95.5<br> recall: 94.9<br> f1: 95.2<br> loss: 0.142</td>
</tr>
<tr>
<td align="left">RoBERTa</td>
<td align="right">acc: <strong>90.4</strong><br> prec: 92.7<br> recall: 79.8<br> f1: 85.8<br> loss: 0.265</td>
<td align="right">acc: <strong>90.9</strong><br> prec: 85.0<br> recall: 86.1<br> f1: 85.6<br> loss: 0.257</td>
<td align="right">acc: <strong>89.8</strong><br> prec: 89.1<br> recall: 90.7<br> f1: 89.9<br> loss: 0.304</td>
<td align="right">acc: <strong>89.8</strong><br> prec: 88.8<br> recall: 90.7<br> f1: 89.7<br> loss: 0.238</td>
<td align="right">acc: <strong>96.7</strong><br> prec: 97.6<br> recall: 95.8<br> f1: 96.7<br> loss: 0.098</td>
</tr>
<tr>
<td align="left">Complex-QDNN(with <code>bert-tiny</code>)</td>
<td align="right">acc: <strong>83.7</strong><br> prec: 81.0<br> recall: 71.8<br> f1: 76.2<br> loss: 0.607</td>
<td align="right"></td>
<td align="right">acc: <strong>77.4</strong><br> prec: 79.9<br> recall: 73.3<br> f1: 76.4<br> loss: 0.849</td>
<td align="right">acc: <strong>77.2</strong><br> prec: 71.6<br> recall: 88.6<br> f1: 79.2<br> loss: 0.613</td>
<td align="right"></td>
</tr>
</tbody></table>
<p><strong>消融实验</strong></p>
<p>消融实验包括ComplexNN vs NN, ComplexNN vs ComplexQNN</p>
<p>通过复数值神经网络对比TextCNN、BiLSTM，验证复数值神经网络的有效性；</p>
<p>通过复数值神经网络对比复数值量子启发式神经网络，验证复数值量子启发式神经网络的有效性。</p>
<hr>
<p><strong>讨论</strong></p>
<p>% 关于ComplexQNN和RoBERTa的对比<br>% 对比主要从当前优势以及未来可能性</p>
<p>ChatGPT的出现给人们带来极大的震撼，同时它让很多人意识到，基于经典神经网络的自然语言处理技术已经达到了很高的水平。RoBERTa是一个很优秀的模型，在很多的数据集上它的表现都已经达到了人类水平。  因此我们并不意外我们提出的模型ComplexQNN在多个数据集上的结果与RoBERTa接近。下面我们讨论相比于流行的经典模型如RoBERTa，ComplexQNN具有的一些优势，我们总结出以下三点。</p>
<p>首先来看一下ComplexQNN和RoBERTa的区别。RoBERTa是在BERT模型的基础上进行改进，是多层Transformer结构，能够在实数值空间中进行特征提取。ComplexQNN是定义在希尔伯特空间中，词向量会被表示成复值，在表示空间上要大于实数值空间。实部可以表示上下文的语义信息，虚部可以用来表示语义之外的信息，比如句子中词语的位置信息、词语的情感信息和词语的歧义信息。 与实数空间相比，复数值空间给予深度学习算法更多的表示可能性，有利于拓展模型发展的边界。</p>
<p>其次经典模型大多是黑盒模型。现有的自然语言模型，将文本映射到向量后，再通过多层神经网络结构，中间向量的含义只能用文本的低维特征和高维特征描述。而基于量子计算的量子启发式模型，是把自然语言看作是一个量子系统：词语被表示为量子态，句子被表示为密度矩阵，句子中词语的相互作用被表示为量子态演化，句子对应的标签表示为量子态测量后坍塌到基态。这给模型带来了物理意义，有利于人们对模型的理解。自然语言的一些特性可以用量子现象解释，如词语的一词多义现象可以很好地用量子纠缠现象表示，这在一定程度上增加了模型的可解释性。</p>
<p>最后是计算复杂度的问题。从目前来看，要实现量子启发式复数值网络需要两倍的资源(实部和虚部)，但这是因为在经典计算机中模拟量子操作的原因。因为n个量子比特需要用$2^n$个经典比特来模拟(考虑复数值时，需要$2^{n+1}$)，$n$比特量子门需要$2^n\times 2^n$个经典比特来模拟。ComplexQNN设计的神经网络层是基于复数值神经网络，很容易迁移到未来的量子计算机中。而当我们的算法运行在真实的量子计算机中时，花费的存储和计算资源将会指数级减少。现有的量子计算机已经超过100多个量子比特，能处理小规模的自然语言处理数据集上的分类任务，我们期待未来在真实的量子计算机中实现我们提出的算法。</p>
<p>总言之，相比于经典神经网络模型如RoBERTa，ComplexQNN有更强的表示能力，更好的可解释性，复杂度指数级降低的可能性。</p>
<p>% [实验结果只是ComplexQNN的一部分，未来和可能性才是我们算法的优势。]</p>
<h3 id="第五章-结论"><a href="#第五章-结论" class="headerlink" title="第五章 结论"></a>第五章 结论</h3><p>本文中，我们基于复数值神经网络提出了一种新的量子启发式模型ComplexQNN，并在情感分类数据集上验证了模型的有效性。同时，通过消融实验，我们证明复数值量子启发式模型在语言建模上，具有比以往的量子启发式模型和复数值神经网络更加杰出的表现。</p>
<p>未来的研究方向可以考虑两个方向：第一是编码更深层的语义，比如开发出适合于数据集规模更大的复数值Tranformer网络模块，并应用在更加复杂的场景下，比如机器翻译，推荐系统等；第二是使用量子线路模型构建网络模块，虽然这在NISQ时代能处理的数据集是受限的。</p>
<hr>
<p>基金</p>
<p>This research was funded by the National Natural Science Foundation of China (Grant Nos.61872390, 61972418, 62272483) and the Special Foundation for Distinguished Young Scientists of Changsha (Grant Nos.kq1905058).</p>
<p>National Natural Science Foundation of China Nos.61872390, Nos.61972418, Nos.62272483</p>
<p>Special Foundation for Distinguished Young Scientists of Changsha Nos.kq1905058</p>
<p>This research was funded by the National Natural Science Foundation of China (Grant Nos.61872390, 61972418, 62272483) and the Special Foundation for Distinguished Young Scientists of Changsha (Grant Nos.kq1905058)</p>
<p>This work was supported by the National Natural Science Foundation of China(Grant Nos.61972418,61872390,61801522), the Natural Science Foundation of Hunan Province(Grant Nos. 2020JJ4750,2019JJ40352), the Special Foundation for Distinguished Young Scientists of Changsha(Grant Nos.kq1905058) and CCF-Baidu Open Fund (NO.2021031).</p>
<blockquote>
<p>Conceptualization, Wei Lai, Jinjing Shi and Yan Chang; Methodology, Wei Lai, Jinjing Shi and Yan Chang; Software, Wei Lai and Yan Chang; Validation, Wei Lai, Jinjing Shi and Yan Chang; Formal analysis, Wei Lai, Jinjing Shi and Yan Chang; Investigation, Wei Lai, Jinjing Shi and Yan Chang; Resources, Wei Lai and Jinjing Shi; Data curation, Wei Lai, Jinjing Shi and Yan Chang; Writing – original draft, Wei Lai; Writing – review &amp; editing, Wei Lai and Jinjing Shi; Visualization, Wei Lai and Yan Chang; Supervision, Jinjing Shi and Yan Chang; Project administration, Wei Lai and Yan Chang; Funding acquisition, Jinjing Shi and Yan Chang.</p>
</blockquote>
<hr>
<p>Cover Letter</p>
<blockquote>
<p>本文在量子理论的激励下，提出了一种量子预训练特征嵌入方法(QPFE)，设计了两种高效的量子深度神经网络(QPFE- ernie)用于情感分类和词义消歧。在情感分类和词义消歧方面，本文提出的模型对模糊词的叠加状态进行建模，并合并从ERNIE中学习到的语义特征，在情感分类和词义消歧方面都优于之前的量子模型和经典方法BiLSTM和TextCNN。我们相信，本文的上述介绍将使MDPI公理的普通读者感兴趣。</p>
</blockquote>
<p>本文基于复数值神经网络构造了一种全新的，更加符合量子计算理论的量子启发式模型，即ComplexQNN。ComplexQNN没有使用振幅相位编码，而是完全基于复数值神经网络构建复数值嵌入层，量子编码层以及测量层。本文在情感分类任务上进行了实验，包含文本二分类和多分类，并且对比于经典的TextCNN，BiLSTM-CRF，以及ELMo、BERT、RoBERTa都有竞争性的优势。</p>
<hr>
<p>一些细节：</p>
<ol>
<li>量子启发式语言模型（预训练+微调）：基于复数值神经网络 &amp; 预训练语言模型</li>
<li>方法：Complex encode<ol>
<li>ComplexPyTorch</li>
<li>ComplexLinear</li>
<li>ComplexConv2d</li>
<li>ComplexBatchNorm</li>
</ol>
</li>
<li>评估：<ol>
<li>sentiment classification</li>
<li>GLUE + CLUE</li>
</ol>
</li>
</ol>
<p>模型构建</p>
<p>embedder: [batch_sz, seq_len] -&gt; [batch_sz, seq_len, embedding_dim]</p>
<p>encoder: [batch_sz, seq_len, embedding_dim]</p>
<ul>
<li>complexcnn: </li>
<li>qnn</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处可以考虑给tokens随机加上一个虚部，或者虚部使用另外一种embedding!!!</span></span><br><span class="line"><span class="comment"># TODO</span></span><br><span class="line"><span class="comment"># embedding_type: random, GloVe or Word2vec, contextual embedding</span></span><br><span class="line"><span class="comment"># 思路：a + 1j * b</span></span><br><span class="line"><span class="comment"># tokens = ptm_tokens + 1j * ptm_tokens</span></span><br><span class="line"><span class="comment"># 实现：通过real_embedder将tokens -&gt; real_part, 通过imag_encoder将tokens -&gt; imag_part</span></span><br><span class="line"><span class="comment"># final_encoder = real_part + 1j * imag_part</span></span><br><span class="line"><span class="comment"># 注：这是一种简单的实现方式，另外还可以构造complex-valued embedding，但前者更符合模型构建的意义</span></span><br><span class="line"><span class="comment"># 意义：复数词向量空间/希尔伯特空间能够表示更加复杂的意义（相比于经典实数空间），提高模型的异构性，尝试使用实部、虚部分别编码不同的信息，一方面不同信息可以相互纠缠，另一方面模型表达能力得到提升</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># encoder: complex_tokens -&gt; vector</span></span><br><span class="line"><span class="comment"># Process:</span></span><br><span class="line"><span class="comment"># token: [batch_size, seq_len]</span></span><br><span class="line"><span class="comment"># complex_embedder: ptm_tokens + 1j * ptm_tokens</span></span><br><span class="line"><span class="comment"># token: [batch_size, seq_len, emb_dim]</span></span><br><span class="line"><span class="comment"># 分两种方向：其一量子启发式模型，其二复数值神经网络</span></span><br><span class="line"><span class="comment"># 其一：量子启发式模型</span></span><br><span class="line"><span class="comment"># 前面token对应词语的量子态表示，通过外积、求和（加权），可以得到句子的密度矩阵表示</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, emb_dim, emb_dim]</span></span><br><span class="line"><span class="comment"># 演化：学习文本内部特征，不改变维度 [emb_dim, emb_dim]；gru, cnn, fc, transformer</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, emb_dim, emb_dim]</span></span><br><span class="line"><span class="comment"># 测量：tr(PV), 其中P是测量算子对应的密度矩阵</span></span><br><span class="line"><span class="comment"># prediction: [batch_size, num_labels]</span></span><br><span class="line"><span class="comment"># 其二：复数值神经网络</span></span><br><span class="line"><span class="comment"># encoder: [seq2seq], seq2vec; 具体地，complexcnn, complexgru, complexdnn</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, hidden_size]</span></span><br><span class="line"><span class="comment"># classifer: nn.Linear()</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, num_labels]</span></span><br></pre></td></tr></table></figure>

<p><strong>Ｑ　＆　Ａ</strong></p>
<p>Q: datasets processing</p>
<p>A: Allennlp pipeline.</p>
<p>Q: CR, MPQA, MR, SUBJ</p>
<p>A: 先读取所有数据，然后打乱，最后按7:3分割训练集和测试集</p>
<p>Q: version of BERT, RoBERTa</p>
<p>A: RoBERTa: <code>roberta-base</code>, BERT: <code>bert-base-cased</code></p>
<p>Q: RoBERTa 训练出现精度无法提升的问题</p>
<p>A: 学习率不能调的太高，适合lr&#x3D;1e-5</p>
<p>Q: ComplexNN 效果不好，解决方法</p>
<p>A: 解决中…思路：ComplexEmbedding, ComplexCNN, ComplexGRU, ComplexTransformer</p>
<p><strong>TODO</strong></p>
<p><strong>表：情感分类数据集描述，对比实验表（总），消融实验表</strong></p>
<p><strong>图：对比实验结果图</strong></p>
<p><strong>ComplexQNN</strong></p>
<p>量子启发式模型：Embedder, Encoder, Classifier</p>
<ul>
<li><input checked disabled type="checkbox"> ComplexQNN整体模型<ul>
<li><input disabled type="checkbox"> 画一张整体模型图</li>
<li><input checked disabled type="checkbox"> 预处理过程：分词器、实例化、批处理</li>
<li><input disabled type="checkbox"> Embedder:  BagOfWord, SingleId</li>
<li><input disabled type="checkbox"> Encoder: </li>
<li><input disabled type="checkbox"> Classifer: nn.Linear</li>
</ul>
</li>
<li><input checked disabled type="checkbox"> ComplexQNN的每一个模块介绍<ul>
<li><input disabled type="checkbox"> ComplexEmbedder: real_embedder &amp; imag_embedder</li>
<li><input disabled type="checkbox"> QuantumEncoder: Projection &amp; Evolution</li>
<li><input disabled type="checkbox"> Measure: tr(PV)</li>
</ul>
</li>
<li><input checked disabled type="checkbox"> 训练细节<ul>
<li><input checked disabled type="checkbox"> 硬件信息：2080ti GPU 12G</li>
<li><input checked disabled type="checkbox"> 环境：py3.8, torch&#x3D;1.12.1, allennlp&#x3D;2.10.1</li>
<li><input checked disabled type="checkbox"> epoch: [3 - 30]</li>
<li><input checked disabled type="checkbox"> batch: [16, 32, 64]</li>
<li><input checked disabled type="checkbox"> optimizer：Adam, Adamw</li>
<li><input checked disabled type="checkbox"> loss: cross_entropy</li>
<li><input checked disabled type="checkbox"> metric: accuracy, precision,</li>
</ul>
</li>
</ul>
<h2 id="English-Version"><a href="#English-Version" class="headerlink" title="English Version"></a>English Version</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h3><p>(最多200words：背景，方法，结果，结论)</p>
<p>Existing methods of quantum heuristic model are all based on amplitude phase embedding to map words into Hilbert space, while in quantum computing theory, the vectors corresponding to quantum states are complex numerical values, so Euler’s formula is needed for transformation. Complex numerical neural networks have been studied, but their practical applications are few, let alone in the downstream tasks of natural language such as sentiment analysis and language modeling. In fact, the representation capability of complex numerical neural networks is better than that of real numerical neural networks, which is suitable for modeling complex natural languages. On the other hand, quantum heuristic models are defined under Hilbert space, which is also a complex space, so it is natural to construct quantum heuristic models based on complex numerical neural networks. Therefore, we propose a new model ComplexQNN, which can embed heterogeneous semantic information in the real and imaginary parts to improve the representation ability of the model. We conduct experiments on emotion classification data sets, which are compared to the baseline model, and the experimental results show that ComplexQNN is very good at accomplishing the task, and has xx% accuracy improvement compared to xx model.</p>
<p>key words:（三到十个）</p>
<p>sentiment analysis; machine learning; natural language processing</p>
<p>complex neural networks, sentiment analysis, language model, quantum theory.</p>
<hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>A quick background, question thrown out: NLP has benefited from the development of deep learning and has made significant progress in many areas. The main technology currently in use is a pre-training language model built on transformer that ADAPTS to different downstream tasks through fine-tuning strategies. The research direction began to develop towards larger data sets and larger models. Although the computing power was improved year by year, it still felt constrained. Are there any new computational models that can be modeled more reasonably, save resources and learn more knowledge at the same time?</p>
<p>Existing solutions and shortcomings: Quantum computing is a brand new computing theory. It has been proved that in some tasks, quantum computers have exponential computational complexity acceleration advantages. There is also currently some work on speech quantum computing theory for modeling natural language, such as the lambeq work at Cambridge. Limited by the NISQ era, the size of the text processed is small, and can only handle data set tasks of 100 sentence size containing 10 or more words.</p>
<p>Based on what revelation, I came up with my plan, what work I did: trying to benefit from the theory of quantum computing and another way of thinking about it, building quantum heuristic models. The quantum heuristic model is based on the mathematical framework of quantum computing theory to model the natural language, comparing the natural language to the quantum system, and using the classical neural network model to simulate the process. At present, there are some quantum heuristic models such as NNQLM, ICWE, etc., which simulate the construction of quantum states by amplitude phase coding. In fact, quantum states are defined in Hilbert Spaces, where each dimension is a complex value. Therefore, in order to construct a more reasonable quantum heuristic model and enrich the expression ability of the model, we propose complex numerical quantum Heuristic model (ComplexQNN). We have carried out extensive experiments on the proposed model, using multiple sentiment classification data sets to evaluate the model, and compared it with the classical model, which proves the effectiveness of our model.</p>
<p>My method can be summarized into three points: In summary, we summarize the contents of this work, including the following: Based on complex numerical neural network, we construct a new quantum heuristic model which is more consistent with the theory of quantum computing, namely ComplexQNN. The above model is a theoretical framework. In the actual modeling, the model structure can be adjusted adaptively according to the real scene, which will be elaborated in the third chapter. The experimental results in emotion classification prove that our model has a good effect.</p>
<ul>
<li><input disabled type="checkbox"> 加内容</li>
</ul>
<hr>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>**Preliminary about Quantum Computation **</p>
<p>Quantum theory. Quantum state, quantum evolution and quantum measurement.</p>
<p><strong>Preliminary</strong><br>In quantum physics \cite{dirac1928quantum}, unlike classical physics, the quantum is the smallest unit that cannot be divided, which is the first property of quantum. For instance, the energy of an electron in an atom only can be one of several values (known as energy levels). The second property of quantum is transition that a quantum system can change its state. For example, electrons in an atom can jump to different energy levels when they gain or lose energy. The third property is superposition. A particle can be in a superposition state of upspin and downspin. Apart that, quantum states change over time, which calls quantum evolution. A superposition can be measured by a measure operator, after which the superposition state collapses and loses superposition property.</p>
<p>With the mathematical framework of quantum theory, quantum states can be represented by vectors with Dirac notation \cite{dirac1939new}, marking quantum states with $|$ and $\rangle$. Vectors describing quantum states are called as state vectors, which can be divided into ket and bra. Ket is column vector and bra is row vector. For a quantum state, its ket and bra are transpose conjugate of each other. Any quantum state in a quantum system can be represented by a linear combination of one basis. The quantum state evolution of a closed quantum system can be described by unitary transformation. Related details about quantum theory are described in the rest of this section.</p>
<p><strong>Quantum state</strong><br>A quantum state \cite{bennett2000quantum} defined in a Hilbert Space is noted as a ket ($|\psi\rangle$) and its transpose can be noted as a bra ($\langle \psi|$). A quantum bit (qubit) different from classical bit can be in superposition state of $|0\rangle$ and $|1\rangle$. The superposition state can be noted as $|\phi\rangle &#x3D; \alpha |0\rangle + \beta |1\rangle$, where both $\alpha$ and $\beta$ are complex numbers, as well as $\alpha, \beta$ satisfying $|\alpha |^2 + |\beta |^2 &#x3D; 1$. In addition, $|\alpha|^2$ ($|\beta|^2$) represents the probability of state $|0\rangle$ ($|1\rangle$).</p>
<p>In quantum mechanism, the density matrix of single particle represented as a pure state $\rho$ can be described as $\rho &#x3D; |\psi\rangle \langle \psi|$, where $\rho$ satisfies $\rho &#x3D; \rho^2$. For multiple particles, the total system is a mixed state that can not be represented by a superposition state, which only can be described by a density matrix $\rho &#x3D; \sum_i w_i |\psi_i \rangle \langle \psi_i |$. Similarly, a word consists of different basic senses, which can be viewed as a superposition state. For example, $|apple\rangle &#x3D; \alpha |fruit\rangle + \beta |company\rangle$. A sentence can be viewed as a mixed system for its several words in superposition.</p>
<p><strong>Evolution</strong><br>The quantum system always changes its state over time, which is called as evolution \cite{bennett2000quantum}. The evolution process can be mathematically described as<br>$$<br>    |\psi_2 \rangle &#x3D; U | \psi_1 \rangle,<br>$$<br>where $U$ is a unitary matrix satisfying $U^\dagger U&#x3D;I$.</p>
<p>A word or phrase changes its meaning based on historical reasons, which is similar with quantum state evolution. For instance, “silly” stems from Old English “saelig” representing happy and careless, while it represents fool nowadays. We can learn that it changes from context information or big corpus, which can be accomplished by RNN like long and short-term memory (LSTM) \cite{hochreiter1997lstm} and gated recurrent unit (GRU) \cite{chung2014empirical} as well as pretraining model such as BERT and ERNIE.</p>
<p><strong>Quantum Measurement</strong></p>
<p>Unlike evolution, measurement \cite{peres2002quantum} representing a non-unitary operation is an irreversible process. Quantum measurement are described with a group of measure operators ${M_m}$, satisfying $\sum_m M^\dagger_m M_m &#x3D; I$. Suppose quantum system state is $|\psi \rangle$ before measure operation, and the probability of measured result $m$ is shown as follows,<br>$$<br>p(m) &#x3D; \langle \psi | M^\dagger_m M_m | \psi \rangle.<br>$$</p>
<p>After measurement, the system state collapses into<br>$$<br>|\psi ^\prime \rangle &#x3D; \frac{M_m | \psi \rangle} {\sqrt{ \langle \psi | M^\dagger_m M_m | \psi \rangle }},<br>$$</p>
<p>and the sum of all the measured probabilities is 1:<br>$$<br>\sum_m p(m) &#x3D; \sum_m \langle \psi | M_m^\dagger M_m | \psi \rangle &#x3D; 1.<br>$$</p>
<p><strong>Complex Neural Network</strong></p>
<blockquote>
<p>% Complex nn basis: representation of complex numbers, complex convolution, complex differentiability, complex-valued activations, complex batch normalization, complex weight initialization, complex convolutional residual network.</p>
</blockquote>
<p><strong>representation of complex numbers</strong><br>We start by outlining the way in which complex numbers are represented in our framework. A complex number $z &#x3D; a + ib$ has a real component $a$ and an imaginary component $b$. We represent the real part $a$ and the imaginary part $b$ of a complex number as logically distinct real valued entities and simulate complex arithmetic using real-valued arithmetic internally. Consider a typical real-valued $2D$ convolution layer that has $N$ feature maps such that $N$ is divisible by 2; to represent these as complex numbers, we allocate the first $N &#x2F; 2$ feature maps to represent the real components and the remaining $N &#x2F; 2$ to represent the imaginary ones. Thus, for a four dimensional weight tensor $W$ that links $N_{in}$ input feature maps to $N_{out}$ feature maps and whose kernel size is $m \times m$ we would have  a weight tensor of size $(N_{out} \times N_{in} \times m \times m) &#x2F; 2$ complex weights.</p>
<p><strong>complex convolution</strong></p>
<p><strong>convolutional LSTM</strong><br>A Convolutional LSTM is similar to a fully connected LSTM. The only difference is that, instead of using matrix multiplications to perform computation, we use convolutional operations. The computation in a real-valued Convolutional LSTM is defined as follows:<br>$$<br>    i_t &#x3D; \sigma(W_{xi} * x_t + W_{hi} * W_{t-1} + b_i) \<br>    f_t &#x3D; \sigma(W_{xf} * x_t + W_{hf} * h_{t-1} + b_f) \<br>    c_t &#x3D; f_t \circ c_{t-1} + i_t \circ \tanh(W_{xc} * x_t + W_{hc} * h_{t-1} + b_c) \<br>    o_t &#x3D; \sigma(W_{xo} * x_t + W_{ho} * h_{t-1} + b_o) \<br>    t_t &#x3D; o_t \circ \tanh(c_t)<br>$$</p>
<p>Where $\sigma$ denotes the sigmoidal activation function, $\circ$ the elementwise multiplication and $*$ the real-valued convolution. $i_t, f_t, o_t$ represent the vector notation of the input, forget and output gates respectively. $c_t$ and $h_t$ represent the vector notation of the cell and hidden states respectively. the gates and states in a ConvLSTM are tensors whose last two dimensions are spatial dimensions. For each of the gates, $W_{xgate}$  and $W_{hgate}$ are respectively the input and hidden kernels.</p>
<p>For the Complex Convolutional LSTM, we just replace the real-valued convolutional operation by its complex countpart. We maintain the real-valued elementwise multiplication. The sigmoid and tanh are both performed separately on the real and the imaginary parts.</p>
<ul>
<li><p>ComplexGRU</p>
</li>
<li><p>complex differentiability</p>
</li>
<li><p>complex-valued activations</p>
</li>
<li><p>complex batch normalization</p>
</li>
<li><p>complex weight initialization</p>
</li>
<li><p>complex convolutional residual network</p>
</li>
</ul>
<p>Quantum-inspired Model</p>
<ul>
<li><input disabled type="checkbox"> 需要重写</li>
<li><input disabled type="checkbox"> </li>
</ul>
<hr>
<h3 id="ComplexQNN"><a href="#ComplexQNN" class="headerlink" title="ComplexQNN"></a>ComplexQNN</h3><p>In quantum language models, the Hilbert space is the mathematical foundation of physical events studied. Based on this background, our proposed models are constructed.</p>
<p>Since a quantum state is usually complex-valued, we therefore introduce the Semantic Hilbert Space $\mathbb{H}^n$ on a complex vector space $\mathbb{C}^n$, spanned by a set of orthogonal bases ${|e_j\rangle}_{j&#x3D;1}^n$. </p>
<p>Specifically, $ |e_j\rangle$ represents a sememe which is the minimum semantic unit of word meanings in language universals, and is an one-hot vector with only the j-th element in $|e_j\rangle$ being one while all the other elements being zero. A word $w$ is viewed as a physical state in such semantic Hilbert space, and hence can be represented as a superposition of sememes, written as follows:<br>$$<br>|w\rangle &#x3D; \sum_{j&#x3D;1}^n (w_{rj} + i w_{mj}) |e_j \rangle &#x3D; |w_r \rangle + i |w_i\rangle,<br>$$<br>where $|w_r\rangle &#x3D; \sum_{j&#x3D;1}^n w_{rj} |e_j\rangle$ and $|w_i\rangle &#x3D; \sum_{j&#x3D;1}^n w_{mj} |e_j\rangle$ are the real part and imaginary part of the state $|w\rangle$, respectively. And ${ w_{rj} }<em>{j&#x3D;1}^n$, ${w</em>{mj}}_{j&#x3D;1}^n$ are the real part and imaginary part of probability amplitudes along sememes respectively.</p>
<p>Complex numerical quantum Heuristic neural network (ComplexQNN) is a language model based on complex numerical neural network and quantum computing theory, which is used for downstream tasks of natural language processing. This model is introduced from three aspects: the overall architecture of the model, the construction of specific modules of the model, and the scenarios applicable to the model.</p>
<p>The first is the overall architecture of the ComplexQNN model, which is depicted in Figure 1 and consists of three modules: complex word embedding, quantum encoder, and classifier. In addition, you can see the ComplexQNN text processing process from Figure 1)Text preprocessing: The text first obtains token sequence through preprocessing (case conversion, word segmentation, word index mapping, filling and truncation). On the other hand, in order to mask the additional token sequence brought by filling sequence, a mask sequence composed of 0 and 1 is also needed to be constructed. The token sequence and mask sequence are the input data of ComplexQNN; 2) Complex word embedding: The function of complex word embedding layer is to map the token number (integer) corresponding to the word to the n-dimensional complex vector space, which corresponds to the construction process of quantum state in the quantum adjustment. Each word mapped from the discrete space to the high-dimensional Hilbert space corresponds to a column vector (namely the right vector). 3) Quantum encoder: the quantum state representation of words is obtained through the embedding layer, but the whole sentence is still composed of a single discrete word. Before the quantum state evolution encoding information, it is necessary to obtain the quantum state representation of sentences. Specifically, the density matrix representation of sentences is obtained through the formula (). The evolution of quantum systems will be simulated below. The evolution of quantum states and density matrices in quantum computing theory is realized by using quantum gates, which correspond to a unitary square matrix with dimensions corresponding to the number of qubits it operates $m&#x3D;2^n$. In the complex numerical neural network, we want to simulate this process through the complex numerical linear layer and the complex numerical cyclic neural network (Note: In a strict sense, linear layer and cyclic neural network guarantee the invariability of dimension by specifying the dimension of input and output, and cannot guarantee that the operation is reversible. In order to satisfy the unitary operation, it needs to use quantum computer to build the network layer, which is why our model is called quantum heuristic model). 4) Classifier: The function of the classifier is to use the encoded information to output the prediction result. There are two ways to construct the classifier. One is to output the prediction result directly through measurement based on the quantum computing theory; the other is to use XXX based on the previous quantum state and linear output is used to predict the results. The difference between the two ideas lies in that the first one complies with the characteristics of quantum computing theory, while the second one regards the representation of quantum states as the characteristics of coding and makes classification prediction through the classical neural network idea.</p>
<p>The next step is the detailed construction of each of the ComplexQNN modules, the first of which is ComplexEmbedder. As we all know, the classic word embedding layer can be divided into character level, wordpiece level and word level according to the level of word segmentation, and the common implementation is divided into context and non-context word embedding. word2vec and GloVe are two very classic non-context word vectors. Context word embedding is based on the idea of pre-training model. The data set - related contextual word vectors are obtained by fine tuning. We built a plural embedding into our model, Formula $[w_i w_1,…,…, w_n] &#x3D; [r_i r_1,…,…, r_n] + 1 j \ times [i_i i_1,…,…, i_n] $ shows the word embedded process, it consists of two parts, Real part word embedding and imaginary part word embedding, input token sequence, will output the corresponding complex word vector of the sequence. Specifically, we consider different schemes to construct complex word embedding layer. (1) Different pre-training models are used as embedding layers for the real and imaginary parts, BERT for the real part and RoBERTa for the imaginary part. In this way, different information can be encoded into the quantum state and further processed based on the output of complex word embedding in the following modules.</p>
<p>The second module is the quantum coding layer. The quantum coding layer needs to meet some conditions. The dimension is $2^n$and the input and output dimensions are unchanged. We mentioned above, before coding, need the density matrix of sentence, according to formula ($M &#x3D; \sum_i ^ n | w_i \rangle \langle w_i | $) shows the calculation process of density matrix. We construct our coding layer through the following basic modules: complex numerical fully connected layer, complex numerical cyclic neural network, complex numerical batch regularization. We construct two intermediate module layers for coding: deep coding, recurrent neural network coding (based on ComplexGRU), Graph and graph () are the two intermediate module layers we construct.</p>
<p>The third module is the classifier. As mentioned above, we have two ways to construct a classifier. Let’s first describe the first way. The first is a measurement method of using the theory of quantum computing, concrete is through formula ($p &#x3D; tr (PV) $), which the probability of p is predicted, and the tr is matrix trace, $P$ is the projection operator $|p\rangle$corresponding density matrix, the density matrix of V is a sentence said, is a random initialization of the nonzero vector projection operator, By formula (), we can perform text dichotomies. Obviously, a group of projection vector bases is needed for multiclassification. The following experiments involve multiclassification, so the number of basis vectors can be determined according to the number of categories of classification results. The final classification result is to take the one with the highest prediction probability as the final prediction result. Another classification idea is to build a cyclic neural network or use a convolutional neural network for pooling operation according to the classical deep learning method. Finally, the results are mapped to the vector dimension of the number of classification categories through the linear layer, and the subscript corresponding to the maximum value of the result is taken as the prediction result.</p>
<ul>
<li><input disabled type="checkbox"> 框架图</li>
<li><input disabled type="checkbox"> 模块细节图</li>
</ul>
<hr>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>ComplexTextCNN （这一块可以考虑放到后面写）</p>
<ul>
<li><input disabled type="checkbox"> 情感分类</li>
<li><input disabled type="checkbox"> 实验结果分析</li>
<li><input disabled type="checkbox"> 消融实验</li>
</ul>
<hr>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this paper, we propose a new quantum-inspired model ComplexQNN based on complex numerical neural networks, and verify the effectiveness of the model on sentiment classification datasets. At the same time, through the ablation experiment, we prove that the complex numerical quantum-inspired model has more outstanding performance in language modeling than the previous quantum-inspired model and complex numerical neural network.</p>
<p>The future research direction can be considered in two directions: the first is to encode deeper semantics, such as developing complex numerical Tranformer network module suitable for larger data set and applying it in more complex scenarios, such as machine translation and recommendation system; The second is to build network modules using quantum wiring models, although this is limited in the data set that can be processed in the NISQ era.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Levy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://levyya.github.io/2022/12/05/ComplexQNN/">https://levyya.github.io/2022/12/05/ComplexQNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://levyya.github.io" target="_blank">Levy's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a></div><div class="post_share"><div class="social-share" data-image="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/06/github%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">github本地项目上传</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/05/Allennlp%E5%AD%A6%E4%B9%A0/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Allennlp学习</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/04/28/DEBERTA/" title="DEBERTA"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-28</div><div class="title">DEBERTA</div></div></a></div><div><a href="/2022/04/26/Deep-Position-wise-Interaction-Network-for-CTR-Prediction/" title="Deep Position-wise Interaction Network for CTR Prediction"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-26</div><div class="title">Deep Position-wise Interaction Network for CTR Prediction</div></div></a></div><div><a href="/2022/04/28/Learning-Knowledge-Graph-Embedding-With-Heterogeneous-Relation-Attention-Networks/" title="Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-28</div><div class="title">Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks</div></div></a></div><div><a href="/2022/04/25/SimCSE/" title="SimCSE"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-25</div><div class="title">SimCSE</div></div></a></div><div><a href="/2022/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quanthoven/" title="论文阅读--Quanthoven"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-05</div><div class="title">论文阅读--Quanthoven</div></div></a></div><div><a href="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/" title="论文阅读--Quantum-Inspired Complex-valued Language Models..."><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-10</div><div class="title">论文阅读--Quantum-Inspired Complex-valued Language Models...</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/%E9%98%BF%E5%B0%BC%E4%BA%9A.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Levy</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">76</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Levyya"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">准备毕业中......</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E6%96%87%E7%89%88"><span class="toc-number">1.</span> <span class="toc-text">中文版</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%BC%95%E8%A8%80"><span class="toc-number">1.2.</span> <span class="toc-text">第一章 引言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">第二章 相关工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%AD%A3%E6%96%87"><span class="toc-number">1.4.</span> <span class="toc-text">第三章 正文</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.5.</span> <span class="toc-text">第四章 实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%BB%93%E8%AE%BA"><span class="toc-number">1.6.</span> <span class="toc-text">第五章 结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#English-Version"><span class="toc-number">2.</span> <span class="toc-text">English Version</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Abstract"><span class="toc-number">2.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">2.2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-Work"><span class="toc-number">2.3.</span> <span class="toc-text">Related Work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ComplexQNN"><span class="toc-number">2.4.</span> <span class="toc-text">ComplexQNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-number">2.5.</span> <span class="toc-text">Experiment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">2.6.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/21/%E6%AD%8C%E8%AF%8D-%E7%94%9F%E5%91%BD%E5%9B%A0%E4%BD%A0%E8%80%8C%E7%81%AB%E7%83%AD/" title="歌词--生命因你而火热"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="歌词--生命因你而火热"/></a><div class="content"><a class="title" href="/2023/02/21/%E6%AD%8C%E8%AF%8D-%E7%94%9F%E5%91%BD%E5%9B%A0%E4%BD%A0%E8%80%8C%E7%81%AB%E7%83%AD/" title="歌词--生命因你而火热">歌词--生命因你而火热</a><time datetime="2023-02-21T12:15:20.000Z" title="发表于 2023-02-21 20:15:20">2023-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="数据结构"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构"/></a><div class="content"><a class="title" href="/2023/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" title="数据结构">数据结构</a><time datetime="2023-02-20T13:07:08.000Z" title="发表于 2023-02-20 21:07:08">2023-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/20/%E6%AD%8C%E8%AF%8D-%E7%81%AB%E8%BD%A6/" title="歌词--火车"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="歌词--火车"/></a><div class="content"><a class="title" href="/2023/02/20/%E6%AD%8C%E8%AF%8D-%E7%81%AB%E8%BD%A6/" title="歌词--火车">歌词--火车</a><time datetime="2023-02-20T11:24:04.000Z" title="发表于 2023-02-20 19:24:04">2023-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/20/%E6%AD%8C%E8%AF%8D-%E8%8A%B1%E7%81%AB-%E6%96%B0%E8%A3%A4%E5%AD%90/" title="歌词--花火 (新裤子)"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="歌词--花火 (新裤子)"/></a><div class="content"><a class="title" href="/2023/02/20/%E6%AD%8C%E8%AF%8D-%E8%8A%B1%E7%81%AB-%E6%96%B0%E8%A3%A4%E5%AD%90/" title="歌词--花火 (新裤子)">歌词--花火 (新裤子)</a><time datetime="2023-02-20T11:16:16.000Z" title="发表于 2023-02-20 19:16:16">2023-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/20/%E6%AD%8C%E8%AF%8D-%E6%B2%A1%E6%9C%89%E7%90%86%E6%83%B3%E7%9A%84%E4%BA%BA%E4%B8%8D%E4%BC%A4%E5%BF%83/" title="歌词--没有理想的人不伤心"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://levy-blog.oss-cn-hangzhou.aliyuncs.com/pics/TomAndJerry.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="歌词--没有理想的人不伤心"/></a><div class="content"><a class="title" href="/2023/02/20/%E6%AD%8C%E8%AF%8D-%E6%B2%A1%E6%9C%89%E7%90%86%E6%83%B3%E7%9A%84%E4%BA%BA%E4%B8%8D%E4%BC%A4%E5%BF%83/" title="歌词--没有理想的人不伤心">歌词--没有理想的人不伤心</a><time datetime="2023-02-20T11:15:55.000Z" title="发表于 2023-02-20 19:15:55">2023-02-20</time></div></div></div></div></div></div></main><footer id="footer" style="background: #ace0f9"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By Levy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="好困,富强,公正,法治,❤️国,敬业,明主,友善,自由,平等,文明,和谐" data-fontsize="20px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>