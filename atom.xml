<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Levy&#39;s blog</title>
  
  
  <link href="https://levyya.github.io/atom.xml" rel="self"/>
  
  <link href="https://levyya.github.io/"/>
  <updated>2023-01-10T12:35:53.000Z</updated>
  <id>https://levyya.github.io/</id>
  
  <author>
    <name>Levy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hexo 常用命令</title>
    <link href="https://levyya.github.io/2023/11/12/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://levyya.github.io/2023/11/12/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</id>
    <published>2023-11-12T02:18:11.608Z</published>
    <updated>2023-01-10T12:35:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>Q: Hexo 图片无法加载问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image -- save</span><br></pre></td></tr></table></figure><p>TODO:</p><p><a href="https://cloud.tencent.com/developer/article/1970544">https://cloud.tencent.com/developer/article/1970544</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="博客" scheme="https://levyya.github.io/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="博客" scheme="https://levyya.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
    <category term="Blog" scheme="https://levyya.github.io/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>代码整洁之道</title>
    <link href="https://levyya.github.io/2023/06/05/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/"/>
    <id>https://levyya.github.io/2023/06/05/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/</id>
    <published>2023-06-05T07:33:02.000Z</published>
    <updated>2023-06-05T08:23:37.000Z</updated>
    
    
    
    
    
    <category term="工作" scheme="https://levyya.github.io/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>LLaMA模型相关</title>
    <link href="https://levyya.github.io/2023/05/22/LLaMA%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3/"/>
    <id>https://levyya.github.io/2023/05/22/LLaMA%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3/</id>
    <published>2023-05-22T08:35:00.000Z</published>
    <updated>2023-05-23T06:26:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客主要记录2023年一些新出的大规模语言模型。</p><h2 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h2><p>论文<a href="https://arxiv.org/pdf/2302.13971.pdf">link</a></p><blockquote><p>摘要</p><p>我们介绍了LLaMA，一个从7B到65B参数的基础语言模型集合。我们在数万亿个tokens上训练我们的模型，并表明可以使用公开可用的数据集专门训练最先进的模型，而无需诉诸专有和不可访问的数据集。特别是，LLaMA 13B在大多数基准测试中优于GPT-3 (175B)， LLaMA65B与最好的型号Chinchilla-70B和PaLM-540B具有竞争力。我们向研究界发布了所有的模型。”</p></blockquote><p>LLaMa (羊驼) 是一个由Meta AI开源的大规模语言模型，具有如下特性：</p><ul><li>参数量从7B-65B</li><li>13B优于GPT-3</li><li>65B接近<strong>Chinchilla-70B</strong>和<strong>PaLM-540B</strong></li></ul><p>主打一个节能高效。</p><h2 id="LLaVA"><a href="#LLaVA" class="headerlink" title="LLaVA"></a>LLaVA</h2><p>多模态大模型，全程Large Language and Vision Assitant</p><p>模型结构：CLIP VIT-L&#x2F;14 + LLaMA</p><h2 id="Alpaca"><a href="#Alpaca" class="headerlink" title="Alpaca"></a>Alpaca</h2><p>斯坦福根据LLaMA 7B微调</p><h2 id="Vicuna"><a href="#Vicuna" class="headerlink" title="Vicuna"></a>Vicuna</h2><p>小羊驼，号称90%GPT-4，7B，13B</p><h2 id="ChatGLM"><a href="#ChatGLM" class="headerlink" title="ChatGLM"></a>ChatGLM</h2><p>清华，6B</p><h2 id="MiniGPT-4"><a href="#MiniGPT-4" class="headerlink" title="MiniGPT-4"></a>MiniGPT-4</h2><p>4张A100，训练10个小时</p><p>Eleuther AI</p><ul><li><p>Pythia</p></li><li><p>DINOv2</p></li></ul><h3 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h3><p>Python Web框架</p><blockquote><p>FastAPI</p><p>Flask</p></blockquote><p><a href="https://github.com/BlinkDL/RWKV-LM">RNN with Transformer LM</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这篇博客主要记录2023年一些新出的大规模语言模型。&lt;/p&gt;
&lt;h2 id=&quot;LLaMA&quot;&gt;&lt;a href=&quot;#LLaMA&quot; class=&quot;headerlink&quot; title=&quot;LLaMA&quot;&gt;&lt;/a&gt;LLaMA&lt;/h2&gt;&lt;p&gt;论文&lt;a href=&quot;https://arxiv</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://levyya.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>软考--系统架构师</title>
    <link href="https://levyya.github.io/2023/04/02/%E8%BD%AF%E8%80%83-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/"/>
    <id>https://levyya.github.io/2023/04/02/%E8%BD%AF%E8%80%83-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/</id>
    <published>2023-04-02T06:21:03.000Z</published>
    <updated>2023-04-02T06:27:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="备考时间"><a href="#备考时间" class="headerlink" title="备考时间"></a>备考时间</h2><p>start:  2023.4.2</p><p>考试时间：2023.5.27</p><p>备考资料：《系统架构设计师教程（第4版）》</p><h2 id="计组"><a href="#计组" class="headerlink" title="计组"></a>计组</h2><h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><h2 id="数据库系统"><a href="#数据库系统" class="headerlink" title="数据库系统"></a>数据库系统</h2><h2 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h2><h2 id="系统性能评价"><a href="#系统性能评价" class="headerlink" title="系统性能评价"></a>系统性能评价</h2><h2 id="开发方法"><a href="#开发方法" class="headerlink" title="开发方法"></a>开发方法</h2><h2 id="系统规划"><a href="#系统规划" class="headerlink" title="系统规划"></a>系统规划</h2><h2 id="系统分析与设计方法"><a href="#系统分析与设计方法" class="headerlink" title="系统分析与设计方法"></a>系统分析与设计方法</h2><h2 id="软件架构设计"><a href="#软件架构设计" class="headerlink" title="软件架构设计"></a>软件架构设计</h2><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><h2 id="测试评审方法"><a href="#测试评审方法" class="headerlink" title="测试评审方法"></a>测试评审方法</h2><h2 id="嵌入式系统设计"><a href="#嵌入式系统设计" class="headerlink" title="嵌入式系统设计"></a>嵌入式系统设计</h2><h2 id="开发管理"><a href="#开发管理" class="headerlink" title="开发管理"></a>开发管理</h2><h2 id="信息系统基础知识"><a href="#信息系统基础知识" class="headerlink" title="信息系统基础知识"></a>信息系统基础知识</h2><h2 id="基于中间件的开发"><a href="#基于中间件的开发" class="headerlink" title="基于中间件的开发"></a>基于中间件的开发</h2><h2 id="安全性和保密性设计"><a href="#安全性和保密性设计" class="headerlink" title="安全性和保密性设计"></a>安全性和保密性设计</h2><h2 id="系统的可靠性分析与设计"><a href="#系统的可靠性分析与设计" class="headerlink" title="系统的可靠性分析与设计"></a>系统的可靠性分析与设计</h2><h2 id="软件的知识产权保护"><a href="#软件的知识产权保护" class="headerlink" title="软件的知识产权保护"></a>软件的知识产权保护</h2><h2 id="标准化知识"><a href="#标准化知识" class="headerlink" title="标准化知识"></a>标准化知识</h2><h2 id="应用数学"><a href="#应用数学" class="headerlink" title="应用数学"></a>应用数学</h2><h2 id="虚拟化、云计算、物联网"><a href="#虚拟化、云计算、物联网" class="headerlink" title="虚拟化、云计算、物联网"></a>虚拟化、云计算、物联网</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;备考时间&quot;&gt;&lt;a href=&quot;#备考时间&quot; class=&quot;headerlink&quot; title=&quot;备考时间&quot;&gt;&lt;/a&gt;备考时间&lt;/h2&gt;&lt;p&gt;start:  2023.4.2&lt;/p&gt;
&lt;p&gt;考试时间：2023.5.27&lt;/p&gt;
&lt;p&gt;备考资料：《系统架构设计师教程（</summary>
      
    
    
    
    
    <category term="软考" scheme="https://levyya.github.io/tags/%E8%BD%AF%E8%80%83/"/>
    
  </entry>
  
  <entry>
    <title>蓝桥杯</title>
    <link href="https://levyya.github.io/2023/03/20/%E8%93%9D%E6%A1%A5%E6%9D%AF/"/>
    <id>https://levyya.github.io/2023/03/20/%E8%93%9D%E6%A1%A5%E6%9D%AF/</id>
    <published>2023-03-20T03:06:09.000Z</published>
    <updated>2023-04-25T09:46:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="T2110-积木画"><a href="#T2110-积木画" class="headerlink" title="T2110 积木画"></a>T2110 积木画</h2><p>注意求余，中间计算过程也需要%mod</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = (dp2 * <span class="number">2</span> % mod + dp0) % mod;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// 请在此输入您的代码</span></span><br><span class="line">  <span class="type">int</span> n;</span><br><span class="line">  cin &gt;&gt; n;</span><br><span class="line">  <span class="type">int</span> dp0 = <span class="number">1</span>;</span><br><span class="line">  <span class="type">int</span> dp1 = <span class="number">2</span>;</span><br><span class="line">  <span class="type">int</span> dp2 = <span class="number">5</span>;</span><br><span class="line">  <span class="keyword">if</span> (n == <span class="number">1</span>) cout &lt;&lt; dp0;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">2</span>) cout &lt;&lt; dp1;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">3</span>) cout &lt;&lt; dp2;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    n -= <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      n--;</span><br><span class="line">      res = (dp2 * <span class="number">2</span> % mod + dp0) % mod;</span><br><span class="line">      dp0 = dp1;</span><br><span class="line">      dp1 = dp2;</span><br><span class="line">      dp2 = res;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; res;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><h3 id="P3802-小魔女帕琪"><a href="#P3802-小魔女帕琪" class="headerlink" title="P3802 小魔女帕琪"></a>P3802 小魔女帕琪</h3><p><a href="https://www.luogu.com.cn/problem/P3802">link</a></p><p>计算期望</p><ul><li>setprecision的头文件 <code>iomanip</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> a[<span class="number">7</span>];</span><br><span class="line"><span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="type">double</span> res = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">7</span>; i++) &#123;</span><br><span class="line">cin &gt;&gt; a[i];</span><br><span class="line">sum += a[i];</span><br><span class="line"><span class="keyword">if</span> (a[i] == <span class="number">0</span>) &#123;</span><br><span class="line">res = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// res = 7! a1*...*a7 / (N*...*N-6)</span></span><br><span class="line"><span class="keyword">if</span> (res != <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">7</span>; i++) &#123;</span><br><span class="line">res = res * i * a[i<span class="number">-1</span>] * <span class="number">1.0</span> / sum ;</span><br><span class="line">sum--;</span><br><span class="line">&#125;</span><br><span class="line">res *= (sum + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">cout &lt;&lt; fixed &lt;&lt; <span class="built_in">setprecision</span>(<span class="number">3</span>) &lt;&lt; res &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="P1551-亲戚-并查集模板题"><a href="#P1551-亲戚-并查集模板题" class="headerlink" title="P1551 亲戚 (并查集模板题)"></a>P1551 亲戚 (并查集模板题)</h2><p><a href="https://www.luogu.com.cn/problem/P1551">link</a></p><ul><li>memset 需要包含头文件<code>cstring</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5e4</span> + <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> n, m, p;</span><br><span class="line"><span class="type">int</span> fa[N];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (x == fa[x]) &#123;</span><br><span class="line"><span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> fa[x] = <span class="built_in">find</span>(fa[x]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">cin &gt;&gt; n &gt;&gt; m &gt;&gt; p;</span><br><span class="line"><span class="built_in">memset</span>(fa, <span class="number">0</span>, <span class="keyword">sizeof</span> fa);</span><br><span class="line"><span class="comment">//fill(fa, fa+n+1, 0);</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= n; i++) &#123;</span><br><span class="line">fa[i] = i;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line"><span class="type">int</span> x, y;</span><br><span class="line">cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line">fa[<span class="built_in">find</span>(x)] = <span class="built_in">find</span>(y);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; p; i++) &#123;</span><br><span class="line"><span class="type">int</span> x, y;</span><br><span class="line">cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">find</span>(x) == <span class="built_in">find</span>(y)) &#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;Yes\n&quot;</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;No\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="pb-ds库"><a href="#pb-ds库" class="headerlink" title="pb_ds库"></a>pb_ds库</h2><p><strong>哈希表</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ext/pb_ds/assoc_container.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ext/pb_ds/hash_policy.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> __gnu_pbds;</span><br></pre></td></tr></table></figure><p>两种哈希表</p><ul><li>cc_hash_table  拉链法</li><li>gp_hash_table  查探法</li></ul><p><strong>堆</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ext/pb_ds/priority_queue.hpp&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> __gnu_pbds;</span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>&gt;q;<span class="comment">//因为放置和std重复，故需要带上命名空间</span></span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>,greater&lt;<span class="type">int</span>&gt;,pairing_heap_tag&gt; pq;<span class="comment">//最快</span></span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>,greater&lt;<span class="type">int</span>&gt;,binary_heap_tag&gt; pq;</span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>,greater&lt;<span class="type">int</span>&gt;,binomial_heap_tag&gt; pq;</span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>,greater&lt;<span class="type">int</span>&gt;,rc_binomial_heap_tag&gt; pq;</span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>,greater&lt;<span class="type">int</span>&gt;,thin_heap_tag&gt; pq;</span><br><span class="line">__gnu_pbds::priority_queue&lt;<span class="type">int</span>,greater&lt;<span class="type">int</span>&gt; &gt; pq;</span><br></pre></td></tr></table></figure><p><strong>红黑树</strong></p><h2 id="P1337-JSOI2004-平衡点-模拟退火"><a href="#P1337-JSOI2004-平衡点-模拟退火" class="headerlink" title="P1337 [JSOI2004] 平衡点 (模拟退火)"></a>P1337 [JSOI2004] 平衡点 (模拟退火)</h2><ul><li><a href="https://www.luogu.com.cn/problem/P1337">题目</a></li><li><a href="https://oi-wiki.org//misc/simulated-annealing/">资料</a></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e4</span> + <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">double</span> resx, resy, resw;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span> &#123;</span><br><span class="line"><span class="type">int</span> x, y, w;</span><br><span class="line">&#125;obj[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">energy</span><span class="params">(<span class="type">double</span> x, <span class="type">double</span> y)</span> </span>&#123;</span><br><span class="line"><span class="type">double</span> res = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line"><span class="type">double</span> dx = obj[i].x - x;</span><br><span class="line"><span class="type">double</span> dy = obj[i].y - y;</span><br><span class="line">res += <span class="built_in">sqrt</span>(dx*dx + dy*dy) * obj[i].w;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">simulateAnneal</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">double</span> t = <span class="number">20230402</span>;</span><br><span class="line"><span class="keyword">while</span> (t &gt; <span class="number">1e-15</span>) &#123;</span><br><span class="line"><span class="type">double</span> nx = resx + t * (<span class="built_in">rand</span>() * <span class="number">2</span> - RAND_MAX);</span><br><span class="line"><span class="type">double</span> ny = resy + t * (<span class="built_in">rand</span>() * <span class="number">2</span> - RAND_MAX);</span><br><span class="line"><span class="type">double</span> nw = <span class="built_in">energy</span>(nx, ny);</span><br><span class="line"><span class="type">double</span> delta = nw - resw;</span><br><span class="line"><span class="keyword">if</span> (delta &lt; <span class="number">0</span>) &#123;</span><br><span class="line">resw = nw;</span><br><span class="line">resx = nx;</span><br><span class="line">resy = ny;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">exp</span>(- delta / t) * RAND_MAX &gt; <span class="built_in">rand</span>()) &#123;</span><br><span class="line">resx = nx;</span><br><span class="line">resy = ny;</span><br><span class="line">&#125;</span><br><span class="line">t *= <span class="number">0.996</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">cin &gt;&gt; n;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;obj[i].x, &amp;obj[i].y, &amp;obj[i].w);</span><br><span class="line">resx += obj[i].x;</span><br><span class="line">resy += obj[i].y;</span><br><span class="line">&#125;</span><br><span class="line">resx /= n;</span><br><span class="line">resy /= n;</span><br><span class="line">resw = <span class="built_in">energy</span>(resx, resy);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line"><span class="built_in">simulateAnneal</span>();</span><br><span class="line">&#125;</span><br><span class="line">cout &lt;&lt; fixed &lt;&lt; <span class="built_in">setprecision</span>(<span class="number">3</span>) &lt;&lt; resx &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; resy;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="线段树"><a href="#线段树" class="headerlink" title="线段树"></a>线段树</h2><p><a href="https://www.luogu.com.cn/training/116257#information">题单</a></p><h2 id="真题"><a href="#真题" class="headerlink" title="真题"></a>真题</h2><h3 id="十三届"><a href="#十三届" class="headerlink" title="十三届"></a>十三届</h3><p>重复的数</p><p>考点：莫队</p><p>数的拆分</p><p>考点：素筛</p><p>扫描游戏</p><p>考点：线段树</p><p>全排列的价值</p><p>考点：数学</p><p>爬树的甲壳虫</p><p>考点：递推+拓展欧几里得求逆元</p><p>GCD</p><p>考点：gcd性质</p><p>选数异或</p><p>考点：dp + map</p><p>质因数个数</p><p>考点：质数性质</p><p>灭鼠先锋</p><p>考点：博弈论</p><h3 id="十二届"><a href="#十二届" class="headerlink" title="十二届"></a>十二届</h3><p>分果果</p><p>考点：dp</p><p>双向排序</p><p>考点：栈 or 线段树</p><p>异或数列</p><p>考点：异或</p><p>砝码称重</p><p>考点：set</p><p>时间显示</p><p>考点：时间处理</p><p>回路计数</p><p>考点：哈密顿回路，状压dp</p><p>路径</p><p>考点：最短路径</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ios::<span class="built_in">sync_with_stdio</span>(<span class="literal">false</span>);</span><br><span class="line">cin.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br><span class="line">cout.<span class="built_in">tie</span>(<span class="number">0</span>);</span><br></pre></td></tr></table></figure><h2 id="初赛结果"><a href="#初赛结果" class="headerlink" title="初赛结果"></a>初赛结果</h2><p>2023.04.23 湖南省研究生一等奖</p><h2 id="国赛准备"><a href="#国赛准备" class="headerlink" title="国赛准备"></a>国赛准备</h2><p>分石头：博弈论</p><p>括号序列树：树形dp</p><p>广场舞：数论、离散化</p><p>取模：暴力</p><p>数组操作：线段树</p><p>第八大奇迹：树状数组、二分</p><p>轨道炮：贪心、模拟</p><p>填字母游戏：博弈</p><p>估计人数：匈牙利算法、最小路径覆盖、网络流</p><p>切开字符串：Manacher、哈希、平衡树</p><p>翻转括号序列：线段树</p><p>Log大侠：线段树</p><p>异或三角：数位DP</p><p>蓝跳跳：动态规划、循环数组</p><p>区间移位：二分、枚举</p><p>补给：</p><p>冰山：</p><p>积木：NTT、生成函数、快速幂</p><p>铁路建设：</p><p>生成树计数：状压DP、矩阵快速幂、矩阵树定理</p><p>奇偶覆盖：扫描线、线段树</p><p>二进制问题：数位DP</p><p>迷宫：</p><p>车轮轴迹：计算几何</p><p>质数行者：DP</p><p>游园安排：动态规划</p><p>出租车：</p><p>123：二分</p><p>路径之谜：DFS</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;T2110-积木画&quot;&gt;&lt;a href=&quot;#T2110-积木画&quot; class=&quot;headerlink&quot; title=&quot;T2110 积木画&quot;&gt;&lt;/a&gt;T2110 积木画&lt;/h2&gt;&lt;p&gt;注意求余，中间计算过程也需要%mod&lt;/p&gt;
&lt;figure class=&quot;high</summary>
      
    
    
    
    
    <category term="比赛" scheme="https://levyya.github.io/tags/%E6%AF%94%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>服务器运维</title>
    <link href="https://levyya.github.io/2023/03/19/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/"/>
    <id>https://levyya.github.io/2023/03/19/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/</id>
    <published>2023-03-19T05:47:09.000Z</published>
    <updated>2023-03-19T06:24:42.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="服务器运维关注的问题"><a href="#服务器运维关注的问题" class="headerlink" title="服务器运维关注的问题"></a>服务器运维关注的问题</h2><blockquote><p>以下是一些服务器运维需要关注的问题：</p><ol><li>安全性：确保服务器的安全是非常重要的，可以采取一些措施，比如配置防火墙、使用加密通信协议等来保障服务器的安全。</li><li>稳定性：服务器应该具有高可靠性和稳定性，以确保应用程序的正常运行，可以通过监控服务器状态、自动化部署等方式来提高服务器的稳定性。</li><li>资源管理：服务器资源的管理也是关键，包括 CPU、内存、磁盘、网络等，需要对资源使用情况进行监控和管理，以优化服务器性能和资源利用率。</li><li>日志管理：服务器日志可以帮助我们诊断和解决问题，需要设置正确的日志级别、定期清理日志文件等，以便更好地管理服务器。</li><li>应用程序部署和升级：在服务器上部署和升级应用程序是一项重要任务，需要确保应用程序可以正确地运行，并及时更新和升级应用程序，以提高应用程序的性能和安全性。</li><li>数据备份和恢复：服务器上的数据备份是非常重要的，以便在发生灾难性事件时可以快速恢复数据，并保证业务的连续性。</li><li>网络性能：服务器的网络性能也是关键，需要保证网络带宽、延迟和稳定性，以提供良好的用户体验和应用程序性能。</li><li>负载均衡：负载均衡是提高应用程序性能和稳定性的重要手段，需要使用负载均衡器来分发流量，并根据实际情况进行负载均衡策略的优化。</li></ol><p>以上是一些服务器运维需要关注的问题，对于不同的应用场景和需求，还可能存在其他需要关注的问题。</p></blockquote><h2 id="学习路线"><a href="#学习路线" class="headerlink" title="学习路线"></a>学习路线</h2><blockquote><p>以下是一些服务器运维学习路线的建议：</p><ol><li>熟悉 Linux 操作系统的基础知识，包括文件系统、进程管理、网络配置等。</li><li>掌握 Linux 命令行的使用，如文件操作、进程管理、网络配置等。</li><li>学习 Shell 脚本编程，以便自动化一些运维任务。</li><li>学习常见的 Web 服务器的安装、配置和管理，如 Apache、Nginx 等。</li><li>学习常见的数据库的安装、配置和管理，如 MySQL、PostgreSQL 等。</li><li>学习常见的缓存技术的安装、配置和管理，如 Redis、Memcached 等。</li><li>学习常见的监控和日志管理工具的使用，如 Nagios、Zabbix、Grafana、Logrotate 等。</li><li>学习常见的容器技术的使用，如 Docker、Kubernetes 等。</li><li>学习常见的自动化部署工具的使用，如 Ansible、Puppet、Chef 等。</li><li>不断关注新技术和新工具的发展，并及时学习和掌握。</li></ol></blockquote><h2 id="常用的Linux运维命令"><a href="#常用的Linux运维命令" class="headerlink" title="常用的Linux运维命令"></a>常用的Linux运维命令</h2><p><strong>常用的 Linux 运维命令</strong>：</p><blockquote><ol><li>ps：查看当前系统进程，如 ps -ef。</li><li>top：实时查看系统运行状态和进程占用资源情况，如 top。</li><li>netstat：查看网络连接状态，如 netstat -an。</li><li>ifconfig：查看网络接口信息和配置 IP 地址等，如 ifconfig eth0。</li><li>ping：测试网络连通性，如 ping <a href="http://www.baidu.com./">www.baidu.com。</a></li><li>scp：远程复制文件，如 scp &#x2F;path&#x2F;to&#x2F;local&#x2F;file user@remote:&#x2F;path&#x2F;to&#x2F;remote&#x2F;file。</li><li>ssh：远程登录到服务器，如 ssh user@remote。</li><li>crontab：设置定时任务，如 crontab -e。</li><li>tar：打包和压缩文件，如 tar -czvf file.tar.gz &#x2F;path&#x2F;to&#x2F;file。</li><li>rsync：远程同步文件，如 rsync -avz &#x2F;path&#x2F;to&#x2F;local&#x2F;file user@remote:&#x2F;path&#x2F;to&#x2F;remote&#x2F;file。</li><li>systemctl：管理系统服务，如 systemctl start&#x2F;stop&#x2F;restart&#x2F;status service。</li><li>journalctl：查看系统日志，如 journalctl -u service。</li></ol><p>以上是常用的 Linux 运维命令，根据实际情况和需求，可能会有所不同。</p></blockquote><h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><p><strong>常用的Docker命令</strong>：</p><blockquote><ol><li>docker run：启动一个新的容器。</li><li>docker ps：列出正在运行的容器。</li><li>docker images：列出所有的镜像。</li><li>docker stop：停止一个或多个正在运行的容器。</li><li>docker rm：删除一个或多个容器。</li><li>docker rmi：删除一个或多个镜像。</li><li>docker pull：下载一个镜像。</li><li>docker push：上传一个镜像。</li><li>docker logs：查看一个容器的日志。</li><li>docker exec：在运行中的容器中执行命令。</li><li>docker inspect：查看一个容器或镜像的详细信息。</li><li>docker build：使用Dockerfile构建镜像。</li><li>docker-compose：使用docker-compose.yml文件定义和运行多个容器的应用程序。</li></ol><p>这些命令是使用Docker时最常用的命令。使用这些命令，可以管理容器和镜像，并进行常见操作，如启动和停止容器、构建和上传镜像、查看容器和镜像的详细信息等。</p></blockquote><h2 id="K8s"><a href="#K8s" class="headerlink" title="K8s"></a>K8s</h2><p><strong>K8s介绍</strong></p><blockquote><p>Kubernetes（k8s）是一个用于容器化应用程序部署、管理和自动化的开源平台，它能够自动化应用程序的部署、扩展和管理，并提供负载均衡、自动伸缩、故障恢复等功能。以下是 k8s 可以做的一些事情：</p><ol><li>部署和管理容器化应用程序：k8s 可以自动化地部署和管理容器化应用程序，确保应用程序在集群中的运行状态。</li><li>自动化应用程序的扩展：k8s 可以根据应用程序的负载情况自动扩展或缩小应用程序的副本数量，以满足用户需求。</li><li>负载均衡：k8s 提供了负载均衡的功能，可以将流量分配到不同的容器或副本中，提高应用程序的可用性和性能。</li><li>服务发现和服务管理：k8s 可以自动发现并管理集群中的服务，确保应用程序可以相互通信，并提供 DNS 名称解析等功能。</li><li>故障恢复：k8s 可以监控应用程序的运行状态，并在容器出现故障时自动进行故障转移和恢复，保证应用程序的稳定性和可靠性。</li><li>滚动升级和回滚：k8s 可以帮助用户进行无缝的应用程序升级和回滚，保证应用程序在升级过程中的可用性。</li><li>多租户支持：k8s 可以支持多租户模式，允许不同的用户或团队使用同一个 k8s 集群，并为他们提供独立的环境和资源隔离。</li></ol><p>总之，k8s 可以帮助用户自动化、简化和优化应用程序的部署、管理和运维，提高应用程序的可用性、性能和可靠性。</p></blockquote><p><strong>基本概念</strong>：</p><blockquote><ol><li>Pod：最小的可部署单元，由一个或多个容器组成，并共享网络和存储资源。</li><li>ReplicaSet：用于创建和管理多个 Pod 副本的控制器。</li><li>Deployment：用于管理 ReplicaSet 和 Pod 的声明式配置对象。</li><li>Service：暴露应用程序的网络服务，并为 Pod 集群提供一个固定的 IP 地址和 DNS 名称。</li><li>Volume：用于在 Pod 中存储数据的抽象。</li><li>Namespace：用于在 Kubernetes 集群中创建虚拟集群的隔离机制。</li><li>ConfigMap：用于存储应用程序配置信息的对象。</li><li>Secret：用于存储敏感信息（如密码和密钥）的对象。</li><li>Node：Kubernetes 集群中的一个工作节点，可以是物理机器或虚拟机。</li><li>Cluster：由多个 Node 组成的 Kubernetes 集群，用于运行和管理容器化应用程序。</li></ol></blockquote><p><strong>常用的 Kubernetes 命令</strong>：</p><blockquote><ol><li>kubectl create：创建资源。</li><li>kubectl get：获取资源列表。</li><li>kubectl describe：获取资源的详细信息。</li><li>kubectl delete：删除资源。</li><li>kubectl apply：应用或更新资源配置。</li><li>kubectl scale：扩展或缩小 Pod 数量。</li><li>kubectl logs：获取 Pod 日志。</li><li>kubectl exec：在容器中执行命令。</li><li>kubectl port-forward：将 Kubernetes Pod 中的端口转发到本地机器。</li><li>kubectl rollout：管理滚动升级。</li><li>kubectl config：管理 Kubernetes 配置文件。</li><li>kubectl version：查看 Kubernetes 版本信息。</li></ol><p>这些命令是使用 Kubernetes 时最常用的命令。使用这些命令，可以管理 Kubernetes 资源，并进行常见操作，如创建和删除资源、获取资源的详细信息、扩展和缩小 Pod 数量、获取 Pod 日志、在容器中执行命令等。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;服务器运维关注的问题&quot;&gt;&lt;a href=&quot;#服务器运维关注的问题&quot; class=&quot;headerlink&quot; title=&quot;服务器运维关注的问题&quot;&gt;&lt;/a&gt;服务器运维关注的问题&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;以下是一些服务器运维需要关注的问题：&lt;/p&gt;
&lt;o</summary>
      
    
    
    
    
    <category term="默认标签" scheme="https://levyya.github.io/tags/%E9%BB%98%E8%AE%A4%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>对话系统</title>
    <link href="https://levyya.github.io/2023/03/19/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    <id>https://levyya.github.io/2023/03/19/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/</id>
    <published>2023-03-19T05:22:33.000Z</published>
    <updated>2023-03-19T12:17:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ChatGPT意见"><a href="#ChatGPT意见" class="headerlink" title="ChatGPT意见"></a>ChatGPT意见</h2><blockquote><p>构造一个对话系统通常需要以下步骤：</p><ol><li>数据收集：收集对话数据，可以是聊天记录、对话语料库或其他相关数据。数据应该涵盖尽可能多的话题和情景，以便系统能够更好地理解和回答用户的问题。</li><li>数据清洗和预处理：对收集到的数据进行清洗和预处理，包括去除重复数据、噪声数据和无效数据，并进行分词、词性标注、实体识别等自然语言处理预处理工作。</li><li>构建对话管理器：对话管理器是对话系统的核心组件，它决定了对话系统如何回答用户的问题。对话管理器可以是基于规则的系统，也可以是基于机器学习的系统，其中包括强化学习和监督学习等方法。基于规则的对话管理器需要手动编写规则和模板来回答用户的问题，而基于机器学习的对话管理器则需要训练模型来学习如何回答用户的问题。</li><li>构建自然语言理解模型：自然语言理解模型负责将用户的自然语言输入转换为计算机可以理解的语言。这可以通过基于规则的方法，如正则表达式和词汇表匹配，也可以通过基于机器学习的方法，如深度学习模型，例如 RNN 和 Transformer 来实现。</li><li>构建自然语言生成模型：自然语言生成模型负责将对话管理器生成的计算机语言转换为自然语言输出。自然语言生成模型可以基于模板，规则或者深度学习模型来实现。</li><li>用户界面设计：构建用户交互界面，提供用户友好的对话体验。可以是文本对话、语音对话或图形化界面，可以使用第三方的聊天框架或自行开发。</li><li>集成和部署：将对话管理器、自然语言理解模型、自然语言生成模型和用户界面集成到一个完整的对话系统中，并进行测试和部署。</li></ol><p>总之，构建对话系统是一个复杂的过程，需要多个组件和技术的集成。需要根据应用场景和用户需求来选择最合适的技术和算法。</p></blockquote><h2 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h2><blockquote><p>词性标注通常使用标准的标签集来表示不同的词性类别。不同的标签集在不同的任务和场景下有不同的使用，常见的标签集包括以下几种：</p><ol><li>布朗标签集（Brown Corpus tagset）：布朗标签集是最早的词性标注标签集之一，包括87个不同的标签，例如名词（NN）、动词（VB）、形容词（JJ）等。</li><li>通用标签集（Universal POS tagset）：通用标签集是一种精简的标签集，共包含17个不同的标签，例如名词（NOUN）、动词（VERB）、形容词（ADJ）等，适用于多种自然语言处理任务。</li><li>丹麦标签集（Danish tagset）：丹麦标签集是适用于丹麦语的词性标注标签集，包括56个不同的标签，例如名词（NN）、动词（VB）、形容词（JJ）等。</li></ol><p>以下是常见词性标注标签的解释和示例：</p><ul><li>NN：名词（Noun），如 “book”、”music”、”idea”</li><li>VB：动词（Verb），如 “run”、”eat”、”talk”</li><li>JJ：形容词（Adjective），如 “happy”、”big”、”interesting”</li><li>RB：副词（Adverb），如 “quickly”、”often”、”very”</li><li>IN：介词（Preposition），如 “in”、”on”、”at”</li><li>PRP：代词（Pronoun），如 “I”、”you”、”they”</li><li>DT：冠词（Determiner），如 “a”、”an”、”the”</li><li>CC：连词（Conjunction），如 “and”、”or”、”but”</li><li>CD：基数词（Cardinal Number），如 “one”、”two”、”three”</li><li>EX：存在句型（Existential there），如 “there is”、”there are”</li><li>FW：外来词（Foreign Word），如 “c’est”、”s’il vous plaît”</li><li>MD：情态动词（Modal Verb），如 “can”、”should”、”will”</li><li>POS：所有格标记（Possessive Ending），如 “‘s”</li><li>TO：不定式标志（to），如 “to go”、”to do”</li><li>UH：感叹词（Interjection），如 “oh”、”wow”、”ah”</li><li>WP：疑问代词（Wh-pronoun），如 “what”、”who”、”which”</li><li>WRB：疑问副词（Wh-adverb），如 “when”、”why”、”where”</li></ul><p>需要注意的是，不同的标签集可能存在一些差异，因此在使用时需要根据实际情况选择合适的标签集。</p></blockquote><p>数据集</p><blockquote><p>对话系统的训练数据集通常包括以下几种类型：</p><ol><li>人工构建的对话数据集：这种数据集是由人工创建的，通常需要专业的领域知识和技能。例如，有些公司会雇用人员编写特定主题的对话，或者请专业的演员录制对话，以构建训练数据集。</li><li>开放域对话数据集：这种数据集通常包括开放式的对话，可以包括各种主题和场景，例如Twitter、Reddit等社交媒体上的对话。其中比较著名的包括Cornell Movie Dialogs Corpus、Persona-Chat、DialoGPT等。</li><li>特定领域的对话数据集：这种数据集针对特定领域的对话，例如医疗保健、金融、法律等等。这种数据集的优点是能够训练出专门针对某一特定领域的对话系统。比较著名的包括Stanford’s Multi-Domain Dialogue Dataset、CoQA等。</li><li>技术支持和客服对话数据集：这种数据集是根据用户与技术支持和客服的对话记录构建的。例如Microsoft Research Asia的MSDialog、Alibaba’s E-commerce Dialogue Corpus等。</li><li>生成式对话数据集：这种数据集包括生成对话，即由对话系统生成的对话。这种数据集通常需要使用对话生成技术，例如Seq2Seq、GAN等生成模型来生成。</li></ol><p>以上数据集都可以通过各种方式获得，例如从公共数据集库中下载、从在线社区中抓取、从企业内部系统中收集等等。不同的数据集类型都有不同的优点和缺点，选择合适的数据集需要根据对话系统的目标任务、应用场景和需求来进行选择。</p></blockquote><p>基于GPT-2的对话系统模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2Tokenizer, GPT2LMHeadModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDialogModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name_or_path, num_layers=<span class="number">2</span>, hidden_size=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDialogModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)</span><br><span class="line">        self.transformer = GPT2LMHeadModel.from_pretrained(model_name_or_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将原始transformer解码器替换为多层解码器</span></span><br><span class="line">        decoder_layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            decoder_layer = nn.TransformerDecoderLayer(hidden_size, nhead=<span class="number">12</span>, dropout=<span class="number">0.1</span>)</span><br><span class="line">            decoder_layers.append(decoder_layer)</span><br><span class="line">        self.transformer.transformer.decoder.layers = decoder_layers</span><br><span class="line"></span><br><span class="line">        self.transformer.lm_head.weight = self.transformer.transformer.wte.weight</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask=<span class="literal">None</span>, past=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = self.transformer(input_ids, past_key_values=past, attention_mask=attention_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = self.transformer(input_ids, attention_mask=attention_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, input_str, max_length=<span class="number">50</span></span>):</span><br><span class="line">        input_ids = self.tokenizer.encode(input_str, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">        output = self.transformer.generate(input_ids, max_length=max_length)</span><br><span class="line">        output_str = self.tokenizer.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> output_str</span><br></pre></td></tr></table></figure><h2 id="基于强化学习的预训练语言模型"><a href="#基于强化学习的预训练语言模型" class="headerlink" title="基于强化学习的预训练语言模型"></a>基于强化学习的预训练语言模型</h2><p>近年来，基于强化学习的预训练语言模型不断涌现，以下是其中几个比较著名的模型：</p><ol><li>GPT-3: 是由OpenAI开发的一种基于Transformer架构的预训练语言模型，采用无监督学习方式进行预训练。GPT-3在各种NLP任务上取得了非常优异的表现，成为了目前最具代表性的预训练语言模型之一。</li><li>T5: 是由Google Brain团队开发的一种基于Transformer架构的预训练语言模型，采用无监督学习方式进行预训练，同时支持多种任务的迁移学习。T5在各种NLP任务上的表现也非常优秀。</li><li>XLNet: 是由Carnegie Mellon大学和Google Brain团队联合开发的一种基于Transformer架构的预训练语言模型，采用无监督学习方式进行预训练，通过使用一种新的自回归算法来优化模型的性能。XLNet在各种NLP任务上的表现也非常出色。</li><li>Reformer: 是由Google Research团队开发的一种基于Transformer架构的预训练语言模型，采用无监督学习方式进行预训练。Reformer通过引入一种新的稀疏注意力机制来优化模型的性能，具有更高的训练速度和更低的内存消耗。</li></ol><p>以上这些模型在自然语言处理领域具有非常重要的地位，它们的出现推动了自然语言处理技术的不断发展和进步。</p><h2 id="强化学习方法优化语言模型"><a href="#强化学习方法优化语言模型" class="headerlink" title="强化学习方法优化语言模型"></a>强化学习方法优化语言模型</h2><p>好的，以下是使用强化学习方法优化语言模型参数的示例代码，其中我们使用PyTorch实现。</p><p>首先，我们需要定义一个智能体（Agent），它通过与环境（即生成的文本）交互来学习生成高质量的文本。在本例中，我们使用基于策略梯度的Actor-Critic方法来训练智能体。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Categorical</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ActorCriticAgent</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, hidden_size, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(ActorCriticAgent, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.actor = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.critic = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, hidden=<span class="literal">None</span></span>):</span><br><span class="line">        embedding = self.embedding(inputs)</span><br><span class="line">        lstm_out, hidden = self.lstm(embedding, hidden)</span><br><span class="line">        logits = self.actor(lstm_out)</span><br><span class="line">        value = self.critic(lstm_out)</span><br><span class="line">        <span class="keyword">return</span> logits, value, hidden</span><br></pre></td></tr></table></figure><p>接下来，我们需要定义一个环境（Environment），即生成文本的任务。在本例中，我们使用一个简单的环境，它会随机生成一个固定长度的文本，然后计算文本的质量得分作为奖励（reward）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextGenerationEnvironment</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, max_len</span>):</span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        self.target = <span class="string">&#x27;&#x27;</span>.join(random.choice(string.ascii_lowercase) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_len))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.current_text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.current_text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, action</span>):</span><br><span class="line">        self.current_text += action</span><br><span class="line">        done = <span class="built_in">len</span>(self.current_text) &gt;= self.max_len</span><br><span class="line">        reward = self.get_reward()</span><br><span class="line">        <span class="keyword">return</span> self.current_text, reward, done</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_reward</span>(<span class="params">self</span>):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.current_text):</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(self.target) <span class="keyword">and</span> c == self.target[i]:</span><br><span class="line">                score += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> score / <span class="built_in">len</span>(self.target)</span><br></pre></td></tr></table></figure><p>最后，我们可以使用Actor-Critic算法来训练智能体，以优化语言模型的参数。以下是训练过程的示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">env = TextGenerationEnvironment(max_len=<span class="number">10</span>)</span><br><span class="line">agent = ActorCriticAgent(vocab_size=<span class="number">26</span>, hidden_size=<span class="number">64</span>, num_layers=<span class="number">2</span>)</span><br><span class="line">optimizer = optim.Adam(agent.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    log_probs = []</span><br><span class="line">    values = []</span><br><span class="line">    rewards = []</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        logits, value, hidden = agent(torch.LongTensor([<span class="built_in">ord</span>(c) - <span class="number">97</span> <span class="keyword">for</span> c <span class="keyword">in</span> state]))</span><br><span class="line">        dist = Categorical(logits=logits)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        log_prob = dist.log_prob(action)</span><br><span class="line">        state, reward, done = env.step(<span class="built_in">chr</span>(action.item() + <span class="number">97</span>))</span><br><span class="line">        log_probs.append(log_prob)</span><br><span class="line">        values.append(value)</span><br><span class="line">        rewards.append(reward)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    returns = [<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> rewards[::-<span class="number">1</span>]:</span><br><span class="line">        ...</span><br><span class="line">       </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;ChatGPT意见&quot;&gt;&lt;a href=&quot;#ChatGPT意见&quot; class=&quot;headerlink&quot; title=&quot;ChatGPT意见&quot;&gt;&lt;/a&gt;ChatGPT意见&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;构造一个对话系统通常需要以下步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;l</summary>
      
    
    
    
    
    <category term="NLP" scheme="https://levyya.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT--talk (1) QNLP</title>
    <link href="https://levyya.github.io/2023/03/17/ChatGPT-talk-1-QNLP/"/>
    <id>https://levyya.github.io/2023/03/17/ChatGPT-talk-1-QNLP/</id>
    <published>2023-03-17T07:50:55.000Z</published>
    <updated>2023-03-17T12:08:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一些Demo"><a href="#一些Demo" class="headerlink" title="一些Demo"></a>一些Demo</h2><blockquote><details><summary>点击查看回复</summary>    <div style="background-color:#ecf7fe">    <pre><code>System.out.println("This is a test code line.");    </code></pre>    </div></details></blockquote><p><strong>Q: 请你基于pytorch设计一个量子启发式语言模型QNN，这个模型需要能够处理文本分类任务</strong></p><blockquote><details><summary>点击查看回复</summary><div style="background-color:#ecf7fe"><p>好的，我将提供一个基本的框架，让您可以根据需要进行调整和完善。</p><p>首先，我们需要安装必要的库：PyTorch, PennyLane, numpy, sklearn, matplotlib等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install torch pennylane numpy sklearn matplotlib</span><br></pre></td></tr></table></figure><p>接下来，我们可以开始处理数据集。这里以IMDB数据集为例，使用PyTorch的torchtext库进行数据处理和加载。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">TEXT = torchtext.data.Field(tokenize=<span class="string">&#x27;spacy&#x27;</span>, lower=<span class="literal">True</span>)</span><br><span class="line">LABEL = torchtext.data.LabelField(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">25000</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">train_iterator, test_iterator = torchtext.data.BucketIterator.splits(</span><br><span class="line">(train_data, test_data), batch_size=BATCH_SIZE, device=device)</span><br></pre></td></tr></table></figure><p>接下来，我们可以搭建量子启发式语言模型QNN。这里使用PennyLane库来实现QNN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pennylane <span class="keyword">as</span> qml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定量子神经网络结构</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">qnn_layer</span>(<span class="params">inputs, weights</span>):</span><br><span class="line">    qml.templates.AngleEmbedding(inputs, wires=<span class="built_in">range</span>(n_qubits))</span><br><span class="line">    qml.templates.BasicEntanglerLayers(weights, wires=<span class="built_in">range</span>(n_qubits))</span><br><span class="line">    <span class="keyword">return</span> [qml.expval(qml.PauliZ(i)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_qubits)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QNNTextClassifier</span>(torch.nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_qubits, vocab_size, emb_dim, hidden_dim, n_classes</span>):</span><br><span class="line">  <span class="built_in">super</span>().__init__()</span><br><span class="line">  self.embedding = torch.nn.Embedding(vocab_size, emb_dim)</span><br><span class="line">  self.qnn_layer = qml.qnode(qnn_layer, device=qml.device(<span class="string">&quot;default.qubit&quot;</span>, wires=n_qubits))</span><br><span class="line">  self.linear1 = torch.nn.Linear(n_qubits, hidden_dim)</span><br><span class="line">  self.linear2 = torch.nn.Linear(hidden_dim, n_classes)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">  x = self.embedding(inputs)</span><br><span class="line">  x = torch.transpose(x, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">  qnn_inputs = torch.unbind(x, dim=<span class="number">0</span>)</span><br><span class="line">  qnn_weights = [qml.init.strong_ent_layers_normal(n_qubits, layer) <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]</span><br><span class="line">  qnn_results = [self.qnn_layer(qnn_inputs, qnn_weights)]</span><br><span class="line">  qnn_outputs = torch.stack(qnn_results, dim=<span class="number">0</span>)</span><br><span class="line">  qnn_outputs = torch.transpose(qnn_outputs, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">  x = self.linear1(qnn_outputs)</span><br><span class="line">  x = torch.relu(x)</span><br><span class="line">  x = self.linear2(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>接下来，我们可以进行模型训练和测试。这里使用交叉熵损失函数和Adam优化器进行训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置模型参数</span></span><br><span class="line">n_qubits = <span class="number">4</span></span><br><span class="line">n_layers = <span class="number">2</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(TEXT.vocab)</span><br><span class="line">emb_dim = <span class="number">100</span></span><br><span class="line">hidden_dim = <span class="number">32</span></span><br><span class="line">n_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型并设定优化器和损失函数</span></span><br><span class="line">model = QNNText</span><br></pre></td></tr></table></figure></div></details></blockquote><p><strong>Q: 请用python构造一个量子启发式复数值神经网络</strong></p><blockquote><details><summary>点击查看回复</summary>    <div style="background-color:#ecf7fe">量子启发式复数值神经网络是一种新兴的深度学习方法，用于解决复杂的分类、回归和优化问题。它利用量子计算的思想，采用复数值权重和门控机制来增强网络的表达能力。在Python中，可以使用TensorFlow Quantum库来构建量子启发式复数值神经网络。<p>下面是一个简单的代码示例，用于构建一个量子启发式复数值神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_quantum <span class="keyword">as</span> tfq</span><br><span class="line"></span><br><span class="line">qubits = tfq.util.qubit_count([input_shape])</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Input(shape=input_shape),</span><br><span class="line">    tfq.layers.PQC(</span><br><span class="line">        model_circuit=tfq.layers.ControlledPQC(</span><br><span class="line">            control_qids=[<span class="number">0</span>],</span><br><span class="line">            operations=[</span><br><span class="line">                tfq.layers.AddCircuit(</span><br><span class="line">                    cirq.Circuit(</span><br><span class="line">                        cirq.X(q) <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(qubits)</span><br><span class="line">                    )</span><br><span class="line">                ),</span><br><span class="line">                tfq.layers.ControlledPQC(</span><br><span class="line">                    control_qids=[<span class="number">1</span>],</span><br><span class="line">                    operations=[</span><br><span class="line">                        tfq.layers.AddCircuit(</span><br><span class="line">                            cirq.Circuit(</span><br><span class="line">                                cirq.Y(q) <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(qubits)</span><br><span class="line">                            )</span><br><span class="line">                        ),</span><br><span class="line">                        tfq.layers.Dense(</span><br><span class="line">                            <span class="number">2</span>,</span><br><span class="line">                            activation=<span class="string">&quot;softmax&quot;</span>,</span><br><span class="line">                            dtype=tf.complex64,</span><br><span class="line">                            name=<span class="string">&quot;output&quot;</span></span><br><span class="line">                        )</span><br><span class="line">                    ],</span><br><span class="line">                    differentiator=tfq.differentiators.ParameterShift()</span><br><span class="line">                )</span><br><span class="line">            ]</span><br><span class="line">        ),</span><br><span class="line">        initializer=tf.keras.initializers.RandomUniform(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">        repetitions=<span class="number">4</span></span><br><span class="line">    )</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">    loss=tf.keras.losses.CategoricalCrossentropy(),</span><br><span class="line">    optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">    metrics=[tf.keras.metrics.CategoricalAccuracy()]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>以上代码使用了TensorFlow Quantum库中的tfq.layers.PQC类来定义量子启发式复数值神经网络。这个网络包含两个tfq.layers.ControlledPQC层，其中第一个层实现X门操作，第二个层实现Y门和Dense操作。这个网络还包含一个参数repetitions，用于指定网络的重复次数。</p><p>需要注意的是，这个示例代码仅仅是一个简单的示例，实际使用时需要根据具体的问题进行适当的修改。同时，由于量子计算和复数运算都是比较复杂的数学理论，因此建议使用者具备相关的数学和物理基础知识。<br> </p></div><p></p></details></blockquote><p><strong>Q: 量子启发式神经网络和经典神经网络的区别以及它的优势</strong></p><blockquote><details><summary>点击查看回复</summary><div style="background-color:#ecf7fe">量子启发式神经网络（Quantum-inspired Neural Network，QINN）和经典神经网络（Classical Neural Network，CNN）都是一种基于神经网络的模型，但它们在计算原理和性能方面存在显著差异。<p>首先，量子启发式神经网络利用了量子计算的思想，通过模拟量子行为来完成计算，而经典神经网络则使用传统的计算方式。在实现上，量子启发式神经网络通常需要借助量子计算库或者量子计算器件来实现。</p><p>其次，量子启发式神经网络在处理某些问题时具有经典神经网络无法比拟的优势。例如，量子启发式神经网络在处理大规模数据时可以更加高效地进行计算，而且能够更好地处理非线性问题。此外，量子计算的一些特性，如量子并行性、量子纠缠等，也可以为量子启发式神经网络的计算提供额外的优势。</p><p>另外，量子启发式神经网络还可以借鉴量子计算的一些算法和技术来优化网络结构和参数调整等方面。例如，量子主成分分析（Quantum Principal Component Analysis，QPCA）可以用来优化神经网络的权重和偏置参数，量子变分算法（Variational Quantum Algorithm，VQA）可以用来训练神经网络的参数。</p><p>总的来说，量子启发式神经网络相较于经典神经网络具有更高效、更灵活和更精确的计算能力，这使得它可以在某些任务上取得比经典神经网络更好的表现。然而，目前量子启发式神经网络的应用还处于起步阶段，需要进一步探索和发展。</p></div></details></blockquote><p><strong>Q: 请你描述一下量子启发式自然语言处理的研究背景与意义</strong></p><blockquote><details><summary>点击查看回复</summary><div style="background-color:#ecf7fe">自然语言处理（NLP）是人工智能领域的一个重要分支，它致力于让计算机理解和处理人类语言。在NLP中，许多任务都涉及到对大规模文本数据的处理和分析，例如文本分类、情感分析、语言模型等。传统的NLP方法通常基于经典计算机，但随着量子计算的发展，越来越多的研究者开始探索如何将量子计算的优势应用于NLP领域。<p>量子启发式自然语言处理（Quantum-inspired Natural Language Processing，QINLP）是一种利用量子计算思想和技术进行自然语言处理的新兴研究领域。QINLP并非直接使用量子计算机进行计算，而是受到量子计算理论和算法的启发，设计出一些基于经典计算机的算法和模型来解决NLP中的问题。QINLP的研究意义在于，通过借鉴量子计算的思想，可以提高NLP任务的处理效率和精度，同时为NLP和量子计算之间的交叉学科提供了新的研究方向。</p><p>QINLP的研究内容主要包括：量子语言模型、量子词向量表示、基于量子计算的文本分类、基于量子计算的机器翻译、量子信息检索等。例如，一些研究者已经尝试使用量子主成分分析（Quantum Principal Component Analysis，QPCA）来生成词向量，或者使用量子变分算法（Variational Quantum Algorithm，VQA）来训练文本分类模型。</p><p>总之，QINLP作为一个新兴的研究领域，不仅有助于提高NLP任务的效率和精度，还为NLP和量子计算之间的交叉学科提供了新的研究方向和思路。</p></div></details></blockquote><p><strong>Q: can you build a quantum-inspired language model with Python code</strong></p><blockquote><details style="background-color:#ecf7fe"><summary>点击查看回复</summary><p>Yes, you can build a quantum-inspired language model using Python code. Here is a simple example of a quantum-inspired language model that uses the Quantum Language Model (QLM) framework developed by IBM:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> qiskit <span class="keyword">import</span> QuantumRegister, ClassicalRegister, QuantumCircuit, Aer</span><br><span class="line"><span class="keyword">from</span> qiskit.circuit.library <span class="keyword">import</span> ZZFeatureMap</span><br><span class="line"><span class="keyword">from</span> qiskit.aqua.components.optimizers <span class="keyword">import</span> L_BFGS_B</span><br><span class="line"><span class="keyword">from</span> qiskit.aqua.algorithms <span class="keyword">import</span> VQE</span><br><span class="line"><span class="keyword">from</span> qiskit.aqua <span class="keyword">import</span> QuantumInstance</span><br><span class="line"><span class="keyword">from</span> qiskit.providers.aer <span class="keyword">import</span> QasmSimulator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a simple corpus of text</span></span><br><span class="line">corpus = [<span class="string">&quot;The quick brown fox jumps over the lazy dog&quot;</span>,</span><br><span class="line">       <span class="string">&quot;This is a test sentence&quot;</span>,</span><br><span class="line">       <span class="string">&quot;Natural language processing is an interesting field&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a feature map using the ZZFeatureMap from Qiskit</span></span><br><span class="line">feature_map = ZZFeatureMap(feature_dimension=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a quantum circuit to encode the text into a quantum state</span></span><br><span class="line">qr = QuantumRegister(feature_map.num_qubits)</span><br><span class="line">cr = ClassicalRegister(feature_map.num_qubits)</span><br><span class="line">qc = QuantumCircuit(qr, cr)</span><br><span class="line">qc.append(feature_map, [qr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_map.num_qubits)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a quantum instance for running the VQE algorithm</span></span><br><span class="line">backend = Aer.get_backend(<span class="string">&#x27;qasm_simulator&#x27;</span>)</span><br><span class="line">quantum_instance = QuantumInstance(backend, shots=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a cost function that measures the similarity between two quantum states</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost_function</span>(<span class="params">params</span>):</span><br><span class="line"> qc_params = np.array(params)</span><br><span class="line"> qc_params = qc_params.reshape(feature_map.num_parameters, -<span class="number">1</span>)</span><br><span class="line"> qc_params = qc_params.T[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"> <span class="comment"># Create a new circuit and apply the parameterized gates to it</span></span><br><span class="line"> qc_new = QuantumCircuit(qr, cr)</span><br><span class="line"> qc_new.append(feature_map.bind_parameters(qc_params), [qr[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_map.num_qubits)])</span><br><span class="line"></span><br><span class="line"> <span class="comment"># Compute the fidelity between the original circuit and the new circuit</span></span><br><span class="line"> job = backend.run(qc.combine(qc_new.inverse()), shots=<span class="number">1024</span>)</span><br><span class="line"> result = job.result().get_counts()</span><br><span class="line"> p0 = result.get(<span class="string">&#x27;00&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> p1 = result.get(<span class="string">&#x27;01&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> p2 = result.get(<span class="string">&#x27;10&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> p3 = result.get(<span class="string">&#x27;11&#x27;</span>, <span class="number">0</span>) / <span class="number">1024</span></span><br><span class="line"> f = <span class="number">0.25</span> * (p0 + p3 - p1 - p2)</span><br><span class="line"> <span class="keyword">return</span> <span class="number">1</span> - f</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the VQE algorithm to optimize the circuit parameters</span></span><br><span class="line">optimizer = L_BFGS_B(maxfun=<span class="number">1000</span>)</span><br><span class="line">vqe = VQE(feature_map, optimizer)</span><br><span class="line">result = vqe.run(quantum_instance)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the optimized circuit parameters</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimized circuit parameters:&quot;</span>, result.optimal_point)</span><br></pre></td></tr></table></figure><p>This code creates a feature map using the ZZFeatureMap class from Qiskit, which maps the input text to a quantum state. It then defines a cost function that measures the similarity between two quantum states, and uses the Variational Quantum Eigensolver (VQE) algorithm to optimize the parameters of the quantum circuit. Finally, the code prints the optimized circuit parameters.</p><p>Note that this is just a simple example, and there are many ways to build a quantum-inspired language model using Python and other programming languages. The details of the implementation will depend on the specific framework and techniques used.</p></details></blockquote><p><strong>Q: what is the quantum-inspired language model</strong></p><blockquote><details><summary>点击查看回复</summary>    <div style="background-color:#ecf7fe">A quantum-inspired language model is a type of natural language processing (NLP) model that is inspired by the principles of quantum computing. Unlike a true quantum language model, which uses quantum bits (qubits) and quantum gates to process information, a quantum-inspired language model is a classical model that incorporates some of the mathematical concepts and techniques from quantum computing.<p>One example of a quantum-inspired language model is the Quantum Language Model (QLM) developed by IBM. The QLM uses a mathematical framework called quantum probability theory to represent and manipulate natural language data. This framework allows the QLM to model the complex relationships between words in a sentence and to capture the inherent uncertainty and ambiguity of natural language.</p><p>Another example of a quantum-inspired language model is the Variational Quantum Language Model (VQLM) developed by researchers at the University of Oxford. The VQLM uses a variational quantum circuit to generate word embeddings, which are then used to construct a language model. The VQLM is designed to work with small quantum devices and can be trained using a classical optimizer.</p><p>While quantum-inspired language models do not provide the same computational advantages as true quantum language models, they are still an active area of research and have shown promise in certain NLP tasks, such as language modeling and machine translation. As the field of quantum computing continues to evolve, it is likely that we will see more advanced quantum-inspired language models and other quantum-inspired approaches to natural language processing.</p> </div></details></blockquote><p><strong>Q: Tell me the latest researches in quantum natural language processing</strong></p><blockquote><details><summary>点击查看回复</summary>    <div style="background-color:#ecf7fe">Quantum Natural Language Processing (QNLP) is a rapidly growing field that explores the application of quantum computing to the analysis and processing of natural language data. While still in its early stages, QNLP research has produced some promising results, and there are several ongoing research projects in this area. Here are a few recent examples:<br><br>    1. Quantum Language Modeling: Researchers at the University of Southern California have developed a quantum language model that can predict the next word in a sentence using quantum computing techniques. The model was tested on several datasets, including the Penn Treebank dataset, and was found to outperform classical language models in terms of accuracy.<br><br>    2. Quantum Neural Networks for NLP: Researchers at the University of Toronto have developed a quantum neural network for natural language processing. The network uses quantum computing techniques to perform operations such as matrix multiplication and activation functions, and was tested on several NLP tasks, including sentiment analysis and named entity recognition. The researchers found that the quantum neural network outperformed classical neural networks on some of these tasks.<br><br>    3. Quantum Sentiment Analysis: Researchers at the University of Waterloo have developed a quantum algorithm for sentiment analysis. The algorithm uses a quantum feature mapping technique to convert sentences into quantum states, which are then classified using a quantum support vector machine. The researchers found that the quantum algorithm outperformed classical sentiment analysis methods on some datasets.<br><br>    4. Quantum Language Generation: Researchers at the University of Maryland have developed a quantum algorithm for generating coherent text. The algorithm uses a quantum language model to generate a distribution over possible sentences, which is then sampled to generate coherent text. The researchers found that the quantum algorithm generated text that was more coherent than classical language generation methods.<br><br>These are just a few examples of the latest research in quantum natural language processing. As quantum computing technology continues to advance, it is likely that we will see many more exciting developments in this field in the coming years. </div></details></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一些Demo&quot;&gt;&lt;a href=&quot;#一些Demo&quot; class=&quot;headerlink&quot; title=&quot;一些Demo&quot;&gt;&lt;/a&gt;一些Demo&lt;/h2&gt;&lt;blockquote&gt;
&lt;details&gt;
&lt;summary&gt;点击查看回复&lt;/summary&gt;
    &lt;div</summary>
      
    
    
    
    
    <category term="ChatGPT" scheme="https://levyya.github.io/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>AvWing record--20230316</title>
    <link href="https://levyya.github.io/2023/03/16/AvWing-record-20230316/"/>
    <id>https://levyya.github.io/2023/03/16/AvWing-record-20230316/</id>
    <published>2023-03-16T11:43:59.000Z</published>
    <updated>2023-03-19T11:59:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="T2-01背包问题"><a href="#T2-01背包问题" class="headerlink" title="T2 01背包问题"></a>T2 01背包问题</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n, v;</span><br><span class="line">cin &gt;&gt; n &gt;&gt; v;</span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">weight</span><span class="params">(n)</span></span>;</span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">value</span><span class="params">(n)</span></span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">cin &gt;&gt; weight[i] &gt;&gt; value[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//vector&lt;vector&lt;int&gt; &gt; dp(n + 1, vector&lt;int&gt; (v + 1, 0));</span></span><br><span class="line"><span class="comment">//for (int i = 0; i &lt; n; i++) &#123;</span></span><br><span class="line"><span class="comment">//for (int j = 0; j &lt;= v; j++) &#123;</span></span><br><span class="line"><span class="comment">//dp[i+1][j] = dp[i][j];</span></span><br><span class="line"><span class="comment">//if (j &lt; weight[i]) continue;</span></span><br><span class="line"><span class="comment">//dp[i+1][j] = max(dp[i][j], dp[i][j-weight[i]] + value[i]);</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//cout &lt;&lt; dp[n][v];</span></span><br><span class="line"><span class="comment">// optimize</span></span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(v + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = v; j &gt;= weight[i]; j--) &#123;</span><br><span class="line">dp[j] = <span class="built_in">max</span>(dp[j], dp[j-weight[i]] + value[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">cout &lt;&lt; dp[v];</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="T3-完全背包问题"><a href="#T3-完全背包问题" class="headerlink" title="T3 完全背包问题"></a>T3 完全背包问题</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Created by Levy on 2023/3/16.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line">    <span class="type">int</span> n, v;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; v;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">weight</span><span class="params">(n+<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">value</span><span class="params">(n+<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cin &gt;&gt; weight[i] &gt;&gt; value[i];</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//    vector&lt;vector&lt;int&gt; &gt; dp(n+1, vector&lt;int&gt; (v+1, 0));</span></span><br><span class="line"><span class="comment">//    for (int j = 0; j &lt;= v; j++) &#123;</span></span><br><span class="line"><span class="comment">//        for (int i = 0; i &lt; n; i++) &#123;</span></span><br><span class="line"><span class="comment">//            dp[i+1][j] = dp[i][j];</span></span><br><span class="line"><span class="comment">//            if (j &lt; weight[i]) continue;</span></span><br><span class="line"><span class="comment">//            dp[i+1][j] = max(dp[i][j], dp[i+1][j-weight[i]] + value[i]);</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"><span class="comment">//    cout &lt;&lt; dp[n][v];</span></span><br><span class="line">    <span class="comment">// optimize</span></span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(v+<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = weight[i]; j &lt;= v; j++) &#123;</span><br><span class="line">            dp[j] = <span class="built_in">max</span>(dp[j-weight[i]] + value[i], dp[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; dp[v];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="T4-多重背包问题-I"><a href="#T4-多重背包问题-I" class="headerlink" title="T4 多重背包问题 I"></a>T4 多重背包问题 I</h2><p>将相同的多个物品看作是一个物品，然后按照01背包计算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Created by Levy on 2023/3/19.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="comment">//#include &lt;cmath&gt;</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    <span class="type">int</span> v;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; v;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(v+<span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="type">int</span> weight;</span><br><span class="line">        <span class="type">int</span> value;</span><br><span class="line">        <span class="type">int</span> cnt;</span><br><span class="line">        cin &gt;&gt; weight &gt;&gt; value &gt;&gt; cnt;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; cnt; j++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = v; k &gt;= weight; k--) &#123;</span><br><span class="line">                <span class="comment">// 01背包</span></span><br><span class="line">                dp[k] = <span class="built_in">max</span>(dp[k], dp[k-weight] + value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; dp[v];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="T5-多重背包问题-II"><a href="#T5-多重背包问题-II" class="headerlink" title="T5 多重背包问题 II"></a>T5 多重背包问题 II</h2><p>题意和I一样，数据集范围变大，上一种解法复杂度为$O(n^3)$，这里需要至少$O(n^2lgn)$复杂度才能不超时。</p><p>利用二进制优化</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Created by Levy on 2023/3/19.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    <span class="type">int</span> v;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; v;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; weights;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; values;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">dp</span><span class="params">(v+<span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="type">int</span> weight;</span><br><span class="line">        <span class="type">int</span> value;</span><br><span class="line">        <span class="type">int</span> cnt;</span><br><span class="line">        cin &gt;&gt; weight &gt;&gt; value &gt;&gt; cnt;</span><br><span class="line">        <span class="type">int</span> j = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (; j &lt;= cnt; j &lt;&lt;= <span class="number">1</span>) &#123;</span><br><span class="line">            weights.<span class="built_in">emplace_back</span>(j * weight);</span><br><span class="line">            values.<span class="built_in">emplace_back</span>(j * value);</span><br><span class="line">            cnt -= j;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (cnt &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            weights.<span class="built_in">emplace_back</span>(cnt * weight);</span><br><span class="line">            values.<span class="built_in">emplace_back</span>(cnt * value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; weights.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = v; j &gt;= weights[i]; j--) &#123;</span><br><span class="line">            dp[j] = <span class="built_in">max</span>(dp[j], dp[j-weights[i]] + values[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; dp[v];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="T35-反转链表"><a href="#T35-反转链表" class="headerlink" title="T35 反转链表"></a>T35 反转链表</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"> * struct ListNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     ListNode *next;</span></span><br><span class="line"><span class="comment"> *     ListNode(int x) : val(x), next(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// if (head == NULL) &#123;</span></span><br><span class="line">        <span class="comment">//     return head;</span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line">        <span class="comment">// ListNode* preHead = new ListNode(0);</span></span><br><span class="line">        <span class="comment">// preHead-&gt;next = head;</span></span><br><span class="line">        <span class="comment">// head = head-&gt;next;</span></span><br><span class="line">        <span class="comment">// preHead-&gt;next-&gt;next = NULL;</span></span><br><span class="line">        <span class="comment">// while (head != NULL) &#123;</span></span><br><span class="line">        <span class="comment">//     ListNode* p = head-&gt;next;</span></span><br><span class="line">        <span class="comment">//     head-&gt;next = preHead-&gt;next;</span></span><br><span class="line">        <span class="comment">//     preHead-&gt;next = head;</span></span><br><span class="line">        <span class="comment">//     head = p;</span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line">        <span class="comment">// return preHead-&gt;next;</span></span><br><span class="line">        ListNode* prev = <span class="literal">nullptr</span>;</span><br><span class="line">        ListNode* cur = head;</span><br><span class="line">        <span class="keyword">while</span> (cur != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            ListNode* next = cur-&gt;next;</span><br><span class="line">            cur-&gt;next = prev;</span><br><span class="line">            prev = cur;</span><br><span class="line">            cur = next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> prev;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="T446-统计单词数"><a href="#T446-统计单词数" class="headerlink" title="T446 统计单词数"></a>T446 统计单词数</h2><p>注意完整单词，在<code>word</code>前后加上空格判断！</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Created by Levy on 2023/3/19.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    string word;</span><br><span class="line">    string sent;</span><br><span class="line">    <span class="built_in">getline</span>(cin, word);</span><br><span class="line">    <span class="built_in">getline</span>(cin, sent);</span><br><span class="line">    <span class="built_in">transform</span>(word.<span class="built_in">begin</span>(), word.<span class="built_in">end</span>(), word.<span class="built_in">begin</span>(), ::tolower);</span><br><span class="line">    <span class="built_in">transform</span>(sent.<span class="built_in">begin</span>(), sent.<span class="built_in">end</span>(), sent.<span class="built_in">begin</span>(), ::tolower);</span><br><span class="line">    word = <span class="string">&#x27; &#x27;</span> + word + <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">    sent = <span class="string">&#x27; &#x27;</span> + sent + <span class="string">&#x27; &#x27;</span>;</span><br><span class="line"><span class="comment">//    cout &lt;&lt; word &lt;&lt; endl &lt;&lt; sent;</span></span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> first_index = <span class="number">-1</span>;</span><br><span class="line">    <span class="type">int</span> pos = sent.<span class="built_in">find</span>(word);</span><br><span class="line">    <span class="keyword">while</span> (pos != <span class="number">-1</span>) &#123;</span><br><span class="line">        cnt++;</span><br><span class="line">        <span class="keyword">if</span> (first_index == <span class="number">-1</span>) &#123;</span><br><span class="line">            first_index = pos;</span><br><span class="line">        &#125;</span><br><span class="line">        pos = sent.<span class="built_in">find</span>(word, pos + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (cnt == <span class="number">0</span>) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="number">-1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cout &lt;&lt; cnt &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; first_index;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;T2-01背包问题&quot;&gt;&lt;a href=&quot;#T2-01背包问题&quot; class=&quot;headerlink&quot; title=&quot;T2 01背包问题&quot;&gt;&lt;/a&gt;T2 01背包问题&lt;/h2&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td </summary>
      
    
    
    
    
    <category term="默认标签" scheme="https://levyya.github.io/tags/%E9%BB%98%E8%AE%A4%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT</title>
    <link href="https://levyya.github.io/2023/02/22/ChatGPT/"/>
    <id>https://levyya.github.io/2023/02/22/ChatGPT/</id>
    <published>2023-02-22T08:24:01.000Z</published>
    <updated>2023-02-22T08:56:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf">InstructGPT论文地址</a></p><p><a href="https://www.bilibili.com/video/BV1hd4y187CR/?spm_id_from=333.999.0.0&vd_source=53b8cc6488ce514ed3a9458e8e4f939a">InstructGPT 论文精读 (李沐)</a></p></blockquote><h2 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h2><p>Open AI发布的一个引起广泛关注的产品。</p><p>这里主要讲Open AI不久前发布的一篇论文 (InstructGPT) 的一些内容，ChatGPT使用的技术也是基于这项工作以及更早的GPT系列模型。</p><p>InstructGPT是一个对话模型，找了一些外包来标注数据，使用微调和强化学习来优化模型输出，使用比GPT-3小100倍的模型，输出结果更符合人们的期望，在一些数据集上效果也不错。在这项工作中，研究者对模型生成的文本非常关注以下三点：</p><ul><li>有用性</li><li>真实性</li><li>无害性</li></ul><p>按paper原话说就是想和人期望的输出align（对齐）。</p><h2 id="InstructGPT使用的技术"><a href="#InstructGPT使用的技术" class="headerlink" title="InstructGPT使用的技术"></a>InstructGPT使用的技术</h2><p><strong>Prompt dataset</strong></p><p>提示学习，告诉模型我们想要什么。</p><img src="/2023/02/22/ChatGPT/image-20230222164446999.png" class alt="image-20230222164446999"><p>图：InstructGPT中使用的提示数据集种类</p><p><strong>SFT (Supervised Fine-Tuning)</strong></p><p>首先在GPT-3上用Prompt数据集微调</p><p><strong>RM (Reward modeling)</strong></p><p>奖励建模，labelers给模型的多个输出排序，再重新训练</p><img src="/2023/02/22/ChatGPT/image-20230222165406796.png" class alt="image-20230222165406796"><p>图：奖励模型的损失函数 (Pairwise 排序损失)</p><p><strong>PPO</strong></p><p>一种强化学习方法，也是Open AI提出来的。</p><p>强化学习，RM和PPO是同时进行的，用PPO优化RM。</p><img src="/2023/02/22/ChatGPT/image-20230222165505813.png" class alt="image-20230222165505813"><p>图：PPO的优化目标</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;参考&quot;&gt;&lt;a href=&quot;#参考&quot; class=&quot;headerlink&quot; title=&quot;参考&quot;&gt;&lt;/a&gt;参考&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://cdn.openai.com/papers/Training_language_</summary>
      
    
    
    
    
    <category term="ChatGPT" scheme="https://levyya.github.io/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>数据结构</title>
    <link href="https://levyya.github.io/2023/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>https://levyya.github.io/2023/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</id>
    <published>2023-02-20T13:07:08.000Z</published>
    <updated>2023-03-19T05:21:31.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://eajack.github.io/2020/07/03/%E3%80%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90-C%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E3%80%8BC++%E5%AE%9E%E7%8E%B0%EF%BC%88CPP%20%E9%87%8D%E6%9E%84%E7%89%88%E6%9C%AC%EF%BC%89/">学习参考</a></p></blockquote><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><p>b树，b+树</p><p>红黑树</p><p>场景</p><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><blockquote><p>计算机科学中的图论是研究图（Graph）及其在各领域中的应用的一个分支。以下是常见的图论算法：</p><ol><li>最短路径算法</li></ol><ul><li>Dijkstra算法：用于求带权重有向图的单源最短路径。</li><li>Bellman-Ford算法：用于求带权重有向图的单源最短路径，可以处理负权重边的情况。</li><li>Floyd-Warshall算法：用于求带权重的有向图或无向图中所有点对的最短路径。</li></ul><ol start="2"><li>最小生成树算法</li></ol><ul><li>Prim算法：用于求无向图的最小生成树。</li><li>Kruskal算法：用于求无向图的最小生成树。</li></ul><ol start="3"><li>拓扑排序算法</li></ol><ul><li>拓扑排序算法：用于有向无环图的顶点排序。</li></ul><ol start="4"><li>最大流算法</li></ol><ul><li>Ford-Fulkerson算法：用于求网络流中的最大流量。</li><li>Edmonds-Karp算法：是Ford-Fulkerson算法的一种实现，其时间复杂度为O(E^2 * V)。</li></ul><ol start="5"><li>最小割算法</li></ol><ul><li>Stoer-Wagner算法：用于求无向图的最小割，时间复杂度为O(V^3)。</li><li>Karger算法：用于求无向图的最小割，时间复杂度为O(V^4 * logV)。</li></ul><ol start="6"><li>图匹配算法</li></ol><ul><li>Hopcroft-Karp算法：用于求二分图的最大匹配，时间复杂度为O(E * sqrt(V))。</li></ul><ol start="7"><li>点双联通分量和边双联通分量算法</li></ol><ul><li>Tarjan算法：用于有向或无向图的点双联通分量和边双联通分量的计算。</li></ul><ol start="8"><li>强连通分量算法</li></ol><ul><li>Kosaraju算法：用于有向图的强连通分量的计算。</li><li>Tarjan算法：用于有向图的强连通分量的计算，时间复杂度为O(V+E)。</li></ul><ol start="9"><li>欧拉路径算法</li></ol><ul><li>Fleury算法：用于无向图的欧拉路径的计算。</li><li>Hierholzer算法：用于无向图或有向图的欧拉路径的计算，时间复杂度为O(E)。</li></ul><p>以上是图论中常见的一些算法，每种算法都有不同的应用场景和特点。</p></blockquote><h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Method 1</span></span><br><span class="line"><span class="comment">// Separate chainning method: </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HashMap</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">HashMap1</span>();</span><br><span class="line">    <span class="built_in">HashMap1</span>(<span class="type">int</span> size);</span><br><span class="line">    <span class="built_in">HashMap1</span>(<span class="type">int</span> size, <span class="type">const</span> <span class="type">int</span> keys[], <span class="type">int</span> N);</span><br><span class="line">    ~<span class="built_in">HashMap1</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 1. 下一个质数，用来更新size</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">nextPrime</span><span class="params">(<span class="type">int</span> N)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">hashFunction</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">erase</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;std::vector&lt;HashNode1&gt;&gt; hashMap1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参考<a href="https://www.cnblogs.com/kuillldan/p/6079992.html">博客</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Method2</span></span><br><span class="line"><span class="comment">// 开放定址法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashTable</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">KeyValue</span> &#123;</span><br><span class="line">        K key;</span><br><span class="line">        V value;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">private</span> <span class="title function_">KeyValue</span><span class="params">(K key, V value)</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.key = key;</span><br><span class="line">            <span class="built_in">this</span>.value = value;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// get and set</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> Object[] table;</span><br><span class="line">    <span class="type">private</span> <span class="variable">maxSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">currentAmmount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">HashTable</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.table = <span class="keyword">new</span> <span class="title class_">Object</span>[<span class="built_in">this</span>.maxSize];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">HashTable</span><span class="params">(<span class="type">int</span> maxSize)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> == maxSize || maxSize &lt; <span class="number">0</span> || maxSize &gt; <span class="number">100</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;Illegal maxSize!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">this</span>.maxSize = maxSize;</span><br><span class="line">        <span class="built_in">this</span>.table = <span class="keyword">new</span> <span class="title class_">Info</span>[maxSize];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(K key, V value)</span> &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span> <span class="variable">hashCode</span> <span class="operator">=</span> Math.abs(key.hashCode()) % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">this</span>.table[hashCode] != <span class="literal">null</span> &amp;&amp; (<span class="built_in">this</span>.currentAmount &lt; <span class="built_in">this</span>.maxSize)) &#123;</span><br><span class="line">            hashCode++;</span><br><span class="line">            hashCode = hashCode % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.currentAmount == <span class="built_in">this</span>.maxSize) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Hash table is out of memory!&quot;</span>);</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.table[hashCode] = <span class="keyword">new</span> <span class="title class_">KeyValue</span>(key, value);</span><br><span class="line">            <span class="built_in">this</span>.currentAmount++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">remove</span><span class="params">(K key)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">hashCode</span> <span class="operator">=</span> math.abs(key.hashCode()) % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">this</span>.table[hashCode] != <span class="literal">null</span> &amp;&amp; count &lt; <span class="built_in">this</span>.maxSize) &#123;</span><br><span class="line">            <span class="keyword">if</span> (((KeyValue) <span class="built_in">this</span>.table[hashCode]).getKey().equals(key)) &#123;</span><br><span class="line">                <span class="built_in">this</span>.table[hashCode] = <span class="literal">null</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            count++;</span><br><span class="line">            hashCode++;</span><br><span class="line">            hashCode = hashCode % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> V <span class="title function_">get</span><span class="params">(K key)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">hashCode</span> <span class="operator">=</span> Math.abs(key.hashCode()) % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">this</span>.table[hashCode] != <span class="literal">null</span> &amp;&amp; count &lt; <span class="built_in">this</span>.maxSize) &#123;</span><br><span class="line">            <span class="keyword">if</span> (key.equals(((KeyValue) <span class="built_in">this</span>.table[hashCode]).getKey())) &#123;</span><br><span class="line">                <span class="keyword">return</span> ((KeyValue) <span class="built_in">this</span>.table[hashCode]).getValue();</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            hashCode++;</span><br><span class="line">            count++;</span><br><span class="line">            hashCode = hashCode % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><p>插入排序</p><ul><li>直接插入</li><li>Shell排序 O(n^1.3) 不稳定</li></ul><p>选择排序</p><ul><li>直接选择 不稳定</li><li>堆排序 不稳定</li></ul><p>交换排序</p><ul><li>冒泡排序 稳定</li><li>快速排序 不稳定</li></ul><p>归并排序 稳定</p><p>基数排序 稳定</p><p>快速排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://eajack.github.io/2020/07/03/%E3%80%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%88%</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="数据结构" scheme="https://levyya.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>EndNote 使用</title>
    <link href="https://levyya.github.io/2023/02/04/EndNote-%E4%BD%BF%E7%94%A8/"/>
    <id>https://levyya.github.io/2023/02/04/EndNote-%E4%BD%BF%E7%94%A8/</id>
    <published>2023-02-04T03:17:57.000Z</published>
    <updated>2023-02-04T09:12:30.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Import-Option"><a href="#Import-Option" class="headerlink" title="Import Option"></a>Import Option</h1><p>Web of science：ISI-CE</p><p>pubmed、scopus、ScienceDirect ： Reference Manager(RIS)</p><p>知网、万方、维普：EndNote import</p><p>google scholar： EndNote generated XML</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Import-Option&quot;&gt;&lt;a href=&quot;#Import-Option&quot; class=&quot;headerlink&quot; title=&quot;Import Option&quot;&gt;&lt;/a&gt;Import Option&lt;/h1&gt;&lt;p&gt;Web of science：ISI-CE&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="EndNote" scheme="https://levyya.github.io/tags/EndNote/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--Quantum Natural Language Processing: Challenges and Opportunities</title>
    <link href="https://levyya.github.io/2023/01/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Natural-Language-Processing-Challenges-and-Opportunities/"/>
    <id>https://levyya.github.io/2023/01/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Natural-Language-Processing-Challenges-and-Opportunities/</id>
    <published>2023-01-31T08:05:16.000Z</published>
    <updated>2023-02-02T07:04:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href="https://www.mdpi.com/2076-3417/12/11/5651">click</a></p><p>作者：</p><blockquote><p>Raffaele Guarasci, Giuseppe De Pietroand Massimo Esposito *</p><p>Institute for High Performance Computing and Networking (ICAR), National Research Council (CNR),80131 Naples, Italy; <a href="mailto:&#x72;&#97;&#102;&#x66;&#97;&#101;&#x6c;&#x65;&#46;&#x67;&#117;&#x61;&#x72;&#97;&#115;&#x63;&#x69;&#64;&#x69;&#x63;&#97;&#x72;&#46;&#99;&#x6e;&#x72;&#x2e;&#x69;&#116;">&#x72;&#97;&#102;&#x66;&#97;&#101;&#x6c;&#x65;&#46;&#x67;&#117;&#x61;&#x72;&#97;&#115;&#x63;&#x69;&#64;&#x69;&#x63;&#97;&#x72;&#46;&#99;&#x6e;&#x72;&#x2e;&#x69;&#116;</a> (R.G.); <a href="mailto:&#103;&#x69;&#117;&#115;&#101;&#x70;&#112;&#101;&#46;&#100;&#101;&#x70;&#105;&#x65;&#x74;&#x72;&#x6f;&#x40;&#x69;&#99;&#x61;&#114;&#46;&#x63;&#x6e;&#114;&#x2e;&#x69;&#x74;">&#103;&#x69;&#117;&#115;&#101;&#x70;&#112;&#101;&#46;&#100;&#101;&#x70;&#105;&#x65;&#x74;&#x72;&#x6f;&#x40;&#x69;&#99;&#x61;&#114;&#46;&#x63;&#x6e;&#114;&#x2e;&#x69;&#x74;</a> (G.D.P.)*Correspondence: <a href="mailto:&#x6d;&#97;&#115;&#115;&#x69;&#x6d;&#x6f;&#46;&#101;&#115;&#x70;&#x6f;&#x73;&#x69;&#x74;&#111;&#x40;&#x69;&#x63;&#x61;&#x72;&#46;&#99;&#110;&#x72;&#46;&#105;&#116;">&#x6d;&#97;&#115;&#115;&#x69;&#x6d;&#x6f;&#46;&#101;&#115;&#x70;&#x6f;&#x73;&#x69;&#x74;&#111;&#x40;&#x69;&#x63;&#x61;&#x72;&#46;&#99;&#110;&#x72;&#46;&#105;&#116;</a></p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>近年来，自然语言处理(NLP)和量子计算(QNLP)的结合非常成功，导致了所谓的量子自然语言处理(QNLP)的几种方法的发展。这是一个混合领域，其中量子力学的潜力被开发并应用于语言处理的关键方面，涉及不同的NLP任务。迄今为止开发的方法从那些仅在理论层面上证明量子优势的方法到在量子硬件上实现算法的方法。本文旨在列出迄今为止开发的方法，按类型进行分类，即理论工作和在经典或量子硬件上实现的工作;按任务，即一般目的，如语法-语义表示或特定的NLP任务，如情绪分析或问题回答;以及评估阶段使用的资源，即使用基准数据集还是自定义数据集。讨论了QNLP在性能和方法方面的优势，并给出了一些关于使用QNLP方法取代最先进的基于深度学习的方法的可能考虑。</p><p>关键词:量子计算;自然语言处理</p></blockquote><p>一篇关于量子自然语言处理的综述</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>简介近年来，基于深度学习架构的神经语言模型的爆发使得所有NLP任务[1-4]都有了显著的改善，从机器翻译[5]，文本分类[6]，共指解析[7,8]或多语言语法分析[9-11]。特别是，基于transformer的模型(如BERT)已被证明优于上一代最先进的体系结构，如长短期记忆(LSTM)循环神经网络(RNN)。然而，与性能的提高相匹配的是模型的复杂性不断增加，这导致了一个悖论。模型需要大量的数据来进行有效的训练，在时间、资源和计算上都有巨大的成本。这是目前基于transformer的方法的主要缺点，例如这类神经网络的参数数量达到了数千亿数量级(数据参考OpenAI GPT模型)[12,13]。此外，它需要大量的资源用于训练阶段(例如，整个维基百科语料库的几种语言)。除了这些方面，还有一些固有的开放性问题，这些模型真正从语言中学习了什么[14,15]，它们如何对这些信息进行编码，以及学习到的信息有多少是真正可解释的。文献已经产生了一些研究，重点是神经语言模型是否能够编码某种语言信息，或者它们是否只是复制书面文本中观察到的模式。近年来备受关注的另一种方法是源于量子计算，特别是量子机器学习子领域。这个想法是利用从量子力学中借来的强大方面来克服当前方法的计算限制。经典统计学的主导范式可以用量子力学扩展，用复数矩阵表示对象。</p><p>在量子计算中，比特被量子比特所取代，量子比特能够利用量子系统的叠加[19]特性处理非二进制状态下的信息。与经典方法相比，量子算法可以使用量子比特的固有属性(称为超多项式加速)执行更小的计算复杂度[18,20,21]。最近的工作观察到了量子算法的发展，这些算法可以作为机器学习应用的基础。在某些情况下，量子特性被用于简单地提高机器学习方法的性能，而在其他情况下，机器学习的问题已经使用量子理论重新表述[22,23]。利用量子系统的机器学习算法处理经典数据已经产生了大量的研究。量子机器学习已被用于不同的目的:深入研究量子现象在学习系统中的使用，探索量子计算机在量子数据上学习的能力，以及在量子硬件上重新制定和实现机器学习算法的可能性。就像以前在经典机器学习的情况下发生的那样，量子机器学习算法的这种快速增长，无论是从硬件还是软件方面(就设备和算法而言)，都涉及到自然语言处理(NLP)领域。这就产生了所谓的量子nlp (QNLP)[24]，定义为在量子硬件上实现自然语言。它将组合语言结构(语法和语义)与量子系统实现的组合相匹配，以便对自然语言建模，并利用源自量子机器学习的算法执行简单的NLP任务。</p><p>QNLP的核心思想是，将语言意义和语法结构结合在一起的最有效方式是由范畴量子力学框架提供的。此外，它还承诺更多地了解建模中涉及的语言的含义。使用可视化表示语言的优点是可以表示一个词的意思和它与句子中其他词的关系，以及它的结构。这将从经典的基于依赖项的表示法向前迈进一步，这种表示法依赖于树结构，而与所涉及的语法元素的含义没有显式连接。事实上所有QNLP方法的共同理论框架是自然语言[25]的分类分布成分(DisCoCat)模型。DisCoCat允许将单词和短语的含义编码为量子态，并随后在专用硬件或模拟器中作为量子电路实现。目前，开发的方法主要可以通过混合方法进行，将经典操作与量子力学的操作相结合。这是因为，尽管基于量子的方法的优势在理论水平上得到了证明[25,26]，但硬件的可用性目前仅限于小型到中型规模的机器(特别是NISQ计算机:有噪声的中型规模)[27]。鉴于NLP社区对QNLP提供的可能性越来越感兴趣，本文旨在提供解决自然语言处理任务的量子方法的全面概述。在简要介绍了量子力学和NLP的概念和性质之后，描述了所提出的方法的理论背景。随后，列出了不同的工作，区分它们的类型，即尚未真正实现的完整理论工作，基于量子力学但在经典硬件上运行的工作，以及可以在目前可用的量子硬件上运行并已在真实数据上测试的工作。</p><p>请注意，尽管QNLP很有前途，可以解决许多与当前成熟的NLP机制相关的关键问题，但目前还不可能在经典方法和量子方法之间进行真正的比较。这是因为许多作品只有理论上已经证明了性能优势，而那些已经实现的性能优势只能在非常小的数据部分上运行。本文的结构如下:第二部分分别从量子计算和语言学的角度介绍了本文的理论背景。首先，在2.1节中详细介绍了QNLP中使用的基本量子力学概念、算法和硬件。第2.2节简要介绍了量子计算和NLP之间的交集。之后，在第3节中列举了迄今为止提出的QNLP方法，将其分为理论方法(第4节)、经典硬件方法(第5节)和在真实量子硬件上运行的方法(第6节)。第7节和第8节致力于回顾所列出的工作，并强调了QNLP方法的优点和缺点。</p></blockquote><p>* </p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><blockquote><p>尽管QNLP仍然是一个新的研究领域，到目前为止，它的应用主要集中在相对简单的任务上，但它已经在许多方面显示出了许多机会和优势。</p><p>首先，从理论的角度来看，基于量子的方法可以更好地处理自然语言的机制:这就是所谓的QNLP的“量子原生”观点，得到了许多研究的支持。因此，量子语言模型将更适合以一种更符合人类真实认知过程的方式来理解和描述自然语言现象。然而，尽管这一说法很吸引人，但目前没有具体的证据来支持它，除了在管理特定情况下，即使用有限和受控词汇的简单句子。这既不能反映人类学习语言的实际方式，也不能反映语言的产生。从应用的角度来看，随着量子计算机的功率和存储容量的增加，未来可能会出现一种解决方案，这可能是处理越来越大的向量空间的最现实和最合适的选择。从理论的角度来看，使用CFG和预组引入不同结构的不同类型的句子是昂贵的，因为在某些情况下，这意味着这些资源必须从头开始构建。此外，还有一些关于这些类型的语法是否能够近似所有语法的问题语言现象的种类。</p><p>关于所谓的“量子优势”，与最先进的基线相比，一些运行在经典硬件上的QNLP模型在各种任务中取得了类似甚至更好的性能。作为这些语句应用的一个例子，QNLP模型被应用于管理NLP的各个方面，这些方面对于处理经典概率模型一直是至关重要的，例如信息检索中的干扰现象，术语依赖或歧义消解。</p><p>然而，量子加速所提供的性能提升仅在理论上得到了证明，依赖于量子硬件，该硬件可以利用尚未实现且昂贵的QRAM。尽管已经找到了替代解决方案来弥补这一缺点，但NLP任务上的实现还不能与文献中众所周知的经典解决方案相比。事实上，这些方法所测试的数据非常少(在中等规模的实验中大约有100个自然语言句子)，这是由于目前NISQ计算机施加的限制，也是由于需要为所涉及的句子创建一个特别的语法，因此任务非常有限。</p><p>关于未来的工作，QNLP无疑可以解决依赖大型数据集和具有大量参数的复杂模型的理想结局，但仍有许多问题尚未解决。从理论的角度来看，在模型基础上构建cfg以及为不同语言创建引人注目的形式化描述的可能性方面仍然存在可伸缩性的问题。相反，从应用的角度来看，它可以考虑更重要的现实世界数据的使用，以及更复杂的QNLP任务的实现，如句子相似性和所有那些涉及含糊性的任务，这些任务可能从QNLP的量子原生方法中受益最大。然而，尽管QNLP方法可能需要数年时间才能大规模执行，就像目前基于深度学习的模型一样，QNLP可以提供独特的机会，使用量子特性以更类似于自然语言的工作方式来处理具有挑战性的语言现象。利用量子叠加来模拟语言中的不确定性和歧义或纠缠来有效地描述语法和语义的组成和分布已经成功地证明了这一点。</p></blockquote><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href=&quot;https://www.mdpi.com/2076-3417/12/11/5651&quot;&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Raffaele Guarasci, Giuseppe De Pietroand </summary>
      
    
    
    
    
    <category term="默认标签" scheme="https://levyya.github.io/tags/%E9%BB%98%E8%AE%A4%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--QNLP in Pratice</title>
    <link href="https://levyya.github.io/2023/01/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-QNLP-in-Pratice/"/>
    <id>https://levyya.github.io/2023/01/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-QNLP-in-Pratice/</id>
    <published>2023-01-17T15:07:03.000Z</published>
    <updated>2023-01-18T15:44:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href>click</a></p><p>作者：</p><blockquote><p>Robin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri Kartsaklis, Bob Coecke</p><p>剑桥量子计算，牛津大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>量子自然语言处理(QNLP)处理旨在在量子硬件上运行的NLP模型的设计和实现。在本文中，我们介绍了在噪声中等尺度量子(NISQ)计算机上对大小为&gt; &#x3D;100句的数据集进行的第一次NLP实验的结果。利用Coecke等人(2010)利用量子理论的意义组成模型的形式相似性，我们创建了与量子电路有自然映射的句子表示。我们使用这些表示来实现并成功训练两个NLP模型，在量子硬件上解决简单的句子分类任务。我们以NLP研究人员易于理解的方式详细描述了这些实验的主要原理、过程和挑战，从而为实际的量子自然语言处理铺平了道路。</p></blockquote><p>关键词：量子自然语言处理，100句数据集，文本分类</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>量子计算带来的计算速度比目前的标准高出几倍，正在迅速发展成为计算机科学中最受欢迎的前沿领域之一。虽然直到最近，量子计算的大部分工作都是纯理论的或与经典硬件上的模拟有关，但第一批可供研究人员使用的量子计算机(被称为噪声中尺度量子(NISQ)设备)的出现已经导致了一些有前途的实际结果和应用，涵盖了广泛的主题，如密码学(Pirandola等人，2020年)，化学(Cao等人。和生物医学(Cao et al.， 2018)。</p><p>一个显而易见的问题是，这是否是新的计算范式也可以用于自然语言处理。这种适用性可能会一直持续到杠杆化结束与语言相关的计算加速问题，以及如何研究量子系统，它们的数学描述和信息被“定量”编码的方式可能导致概念上和实践上的进步处理语言的意义计算加速。</p><p>受这些前景的启发，量子自然语言处理(QNLP)，一个仍处于起步阶段的研究领域，旨在开发明确设计为在量子硬件上执行的NLP模型。在这方面有一些令人印象深刻的理论工作，但所提出的实验是经典的模拟。一个值得注意的例外是两位作者(Meichanetzidis et al.， 2020)最近的工作，他们首次在量子硬件上进行了非常小规模的概念证明实验。</p><p>在这篇论文中，我们提出了两个完整的中等规模的实验，包括在量子硬件上运行的语言动机NLP任务。这些实验的目标不是要证明在NLP任务中与经典实现相比的某种形式的“量子优势”;我们认为，由于目前可用的量子计算机的能力有限，这是不可能的。在这项工作中，我们最感兴趣的是向NLP社区提供一个详细的说明，说明QNLP在实践中需要什么。我们展示了传统的建模和编码范式如何转变为量子友好的形式，并探索了当前NISQ计算机所带来的挑战和限制。</p><p>从NLP的角度来看，这两项任务都涉及某种形式的句子分类:每一种句子在数据集中，我们使用组合Coecke等人(2010)的模型-通常被称为分布构成范畴-计算状态向量，即转换为二进制标签。对模型进行训练一个标准的二元交叉熵目标，使用被称为Simultaneous PerarXiv的优化技术:2102.12846 v1 [cs。2021年2月25日扰动随机逼近(SPSA)。的附带了从量子运行中获得的结果通过各种经典模拟表明系统的预期长期行为没有硬件限制。</p><p>选择DISCOCAT的动机是，它产生的导数本质上形成了一个张量网络，这意味着它们已经非常接近量子计算机处理数据的方式。此外，该模型对语法和语义之间的相互作用进行了严格的处理，并使用了方便的图表语言。在第5节中，我们将看到生成的图表如何自然地转换为量子电路——量子计算机的基本计算单元以及不同语法结构的句子如何映射到不同的量子电路。我们还将解释张量收缩(复合函数)如何以量子门的形式表达。我们进一步讨论了噪声在NISQ器件上的作用，以及它如何影响我们对电路的设计选择。实验在IBM量子体验平台提供的IBM NISQ计算机上进行。</p><p>在我们的实验中，我们使用了两个不同的数据集。第一个句子(130个句子)是由一个简单的上下文无关语法自动生成的，其中一半句子与食物有关，一半与IT有关(一个二进制分类任务)。另一个(105个名词短语)是从RELPRON数据集中提取的(Rimell et al.， 2016)，模型的目标是预测一个名词短语是否包含基于主语的关系从句或基于对象的关系从句(同样是一个二元分类任务)。我们证明了模型平滑地收敛，并且在量子和模拟运行中都产生了良好的结果(给定数据集的大小)。综上所述，本文的贡献如下:首先，我们深入概述了在量子计算机上训练和运行NLP模型的过程、技术性和挑战;其次，我们提供了一个强有力的概念证明，量子NLP是在我们的范围内。</p><p>本文的结构如下讨论了最重要的相关工作实验量子计算与QNLP;部分3描述DISCOCAT;第4节规定量子计算入门;部分5给出了一般QNLP的高级概述管道;第6节解释了任务;第7节提供了实验所需的所有细节最后;第8节总结了我们的发现和指向未来的工作。</p></blockquote><p>量子计算，计算速度</p><p>量子自然语言处理，量子硬件，</p><p>DISCOCAT，接近量子计算机处理的数据</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Robin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri Kartsaklis, Bob Coec</summary>
      
    
    
    
    
    <category term="默认标签" scheme="https://levyya.github.io/tags/%E9%BB%98%E8%AE%A4%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>Jsonnet 语法</title>
    <link href="https://levyya.github.io/2023/01/11/Jsonnet-%E8%AF%AD%E6%B3%95/"/>
    <id>https://levyya.github.io/2023/01/11/Jsonnet-%E8%AF%AD%E6%B3%95/</id>
    <published>2023-01-11T09:28:00.000Z</published>
    <updated>2023-01-12T08:15:05.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="支持"><a href="#支持" class="headerlink" title="支持"></a>支持</h2><ul><li>字符串格式化</li><li>数组拼接，切片</li><li>python 表达式</li><li>条件表达式<ul><li><code>if condition then [] else []</code></li></ul></li><li>变量<ul><li>::  隐藏字段</li><li>local</li><li>self  当前对象</li><li>$  根对象</li></ul></li><li>函数，与python函数类似，可以有默认值</li><li>对象<ul><li>super</li><li>+ (覆盖组合)</li><li>+: (组合，不覆盖)</li></ul></li><li>import</li></ul><h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><ol><li>多行注释: &#x2F;* *&#x2F;</li><li>单行注释: &#x2F;&#x2F;</li></ol><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">local batch_size = 32;</span><br><span class="line">local cuda_device = 2;</span><br><span class="line">local num_epochs = 2;</span><br><span class="line">local seed = 42;</span><br><span class="line"></span><br><span class="line">local embedding_dim = 768;</span><br><span class="line">local hidden_dim = 128;</span><br><span class="line">local dropout = 0.2;</span><br><span class="line">local lr = 0.00001;</span><br><span class="line">local output_dim = 128;</span><br><span class="line">local model_name = &#x27;RoBERTa&#x27;;</span><br><span class="line">local ptm_name = </span><br><span class="line">  if model_name == &#x27;RoBERTa&#x27;</span><br><span class="line">  then &#x27;roberta-base&#x27;</span><br><span class="line">  else &#x27;bert-base-cased&#x27;;</span><br><span class="line">  </span><br><span class="line">local data_dir = &#x27;/workspace/Wei_lai/NLP/Mine_Project/NLP/Mine_Project/AllenNLP/Learning/Baseline_Allennlp/data/&#x27;;</span><br><span class="line">local get_train_path(task_name=&#x27;SST&#x27;) = </span><br><span class="line">  if task_name == &#x27;SST&#x27; </span><br><span class="line">  then &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-train&#x27;</span><br><span class="line">  else data_dir + task_name + &#x27;/&#x27; + task_name + &#x27;_train.txt&#x27;;</span><br><span class="line">local get_val_path(task_name=&#x27;SST&#x27;) = </span><br><span class="line">  if task_name == &#x27;SST&#x27; </span><br><span class="line">  then &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-dev&#x27;</span><br><span class="line">  else data_dir + task_name + &#x27;/&#x27; + task_name + &#x27;_test.txt&#x27;;</span><br><span class="line"></span><br><span class="line">local SST2_train_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-train&#x27;;</span><br><span class="line">local SST2_dev_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-dev&#x27;;</span><br><span class="line">local SST5_train_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Fine-Grained/sentiment-train&#x27;;</span><br><span class="line">local SST5_dev_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Fine-Grained/sentiment-dev&#x27;;</span><br><span class="line"></span><br><span class="line">// 如果数据集为CR, MPQA, MR, SUBJ, 使用get_train_path</span><br><span class="line">// local task_name = &#x27;SUBJ&#x27;;</span><br><span class="line">// local train_path = get_train_path(task_name);</span><br><span class="line">// local val_path = get_val_path(task_name);</span><br><span class="line"></span><br><span class="line">// 如果数据集为SST2，SST5，使用根变量引用</span><br><span class="line">local train_path = SST5_train_path;</span><br><span class="line">local val_path = SST5_dev_path;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  numpy_seed: seed,</span><br><span class="line">  pytorch_seed: seed,</span><br><span class="line">  random_seed: seed,</span><br><span class="line">  </span><br><span class="line">  dataset_reader: &#123;</span><br><span class="line">    type: &#x27;my_dataset_reader&#x27;,</span><br><span class="line">    tokenizer: &#123;</span><br><span class="line">      type: &#x27;pretrained_transformer&#x27;,</span><br><span class="line">      model_name: ptm_name</span><br><span class="line">    &#125;,</span><br><span class="line">    token_indexers: &#123;</span><br><span class="line">      tokens: &#123;</span><br><span class="line">        type: &#x27;pretrained_transformer&#x27;,</span><br><span class="line">        model_name: ptm_name</span><br><span class="line">      &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  datasets_for_vocab_creation: [&#x27;train&#x27;],</span><br><span class="line">  train_data_path: train_path,</span><br><span class="line">  validation_data_path: val_path,</span><br><span class="line">  </span><br><span class="line">  model: &#123;</span><br><span class="line">    type: &#x27;text_classifier&#x27;,</span><br><span class="line">    embedder: &#123;</span><br><span class="line">      token_embedders: &#123;</span><br><span class="line">        tokens: &#123;</span><br><span class="line">          type: &#x27;pretrained_transformer&#x27;,</span><br><span class="line">          model_name: ptm_name</span><br><span class="line">        &#125;,</span><br><span class="line">      &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    encoder: &#123;</span><br><span class="line">      type: &#x27;bert_pooler&#x27;,</span><br><span class="line">      pretrained_model: ptm_name</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  data_loader: &#123;</span><br><span class="line">    shuffle: true,</span><br><span class="line">    batch_size: batch_size,</span><br><span class="line">  &#125;,</span><br><span class="line">  trainer: &#123;</span><br><span class="line">    cuda_device: cuda_device,</span><br><span class="line">    num_epochs: num_epochs,</span><br><span class="line">    optimizer: &#123;</span><br><span class="line">      lr: lr,</span><br><span class="line">      type: &#x27;adamw&#x27;,</span><br><span class="line">    &#125;,</span><br><span class="line">    validation_metric: &#x27;+fscore&#x27;,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;支持&quot;&gt;&lt;a href=&quot;#支持&quot; class=&quot;headerlink&quot; title=&quot;支持&quot;&gt;&lt;/a&gt;支持&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;字符串格式化&lt;/li&gt;
&lt;li&gt;数组拼接，切片&lt;/li&gt;
&lt;li&gt;python 表达式&lt;/li&gt;
&lt;li&gt;条件表达式&lt;ul&gt;
&lt;</summary>
      
    
    
    
    
    <category term="Jsonnet" scheme="https://levyya.github.io/tags/Jsonnet/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--Quantum-Inspired Complex-valued Language Models...</title>
    <link href="https://levyya.github.io/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/"/>
    <id>https://levyya.github.io/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/</id>
    <published>2023-01-10T14:03:22.000Z</published>
    <updated>2023-01-11T09:31:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href>click</a></p><p>作者：</p><blockquote><p>Qin Zhao 1,† , Chenguang Hou 2,† and Ruifeng Xu 1,3,*</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>基于方面的情感分析(ABSA)是一种细粒度的情感分析方法，其目的是对面向方面的极性进行分类。当前模型的向量表示通常受限于实际值。基于量子理论的数学公式，量子语言模型越来越受到人们的关注。这样的模型中的单词可以投射为量子系统中的物理粒子，并且自然地由希尔伯特空间中具有丰富表示的复值向量表示，而不是实值向量。本文研究了ABSA模型的Hilbert空间表示，构造了三个强实值基线的复化。实验结果证明了复值嵌入的有效性和复值嵌入模型的卓越性能，说明复值嵌入可以携带超出真实嵌入的额外信息。特别是，复值RoBERTa模型在三个标准基准数据集上优于或接近先前的最先进技术。</p></blockquote><p>关键词：量子语言模型；复数值网络；方面级情感分析</p><p>后面重点关注复值RoBERTa的构建</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>基于方面的情感分析(Aspect-Based Sentiment Analysis, ABSA)是一种细粒度的情感分析任务，其目的是将句子的情感极性按一个或多个方面进行分类[1-5]。ABSA任务中涉及的基本情感要素有方面类别、方面项、意见项和情感极性。方面类别定义了方面术语应该属于哪个类别。例如，在餐厅领域中，“食物”和“服务”是方面类别。方面项是在给定文本中显示的意见目标。意见术语表达了一个人对方面术语的看法。情感极性描述了情感在一个方面类别或方面术语上的取向，通常是积极的、消极的或中性的。例如，在“the food was great and the service was severe slow”这句话中，第一个方面词是“food”，属于“food”范畴，意见词是“great”。因此，“食物”方面术语和“食物”方面类别的情感极性为正。第二个方面的术语是“服务”，属于“服务”范畴，意见术语是“慢”。因此，方面术语“服务”和方面类别“服务”的情感极性是消极的。ABSA通常由两个子任务组成:方面提取(AE)和方面级情感分类(ALSC)。本文只关注ALSC，即预测不同方面术语在上下文中的确切情感极性，而不是在句子层面或文档层面上对整体情感极性进行分类。也就是说，我们的任务是在给定的上下文中对方面术语的情感极性进行分类，比如上面例子中提到的“食物”和“服务”。这种面向方面的情感极性分类可以更好地研究评论中细粒度的情感倾向，从而为决策者提供更准确的建议。</p><p>此前，为了避免设计手工制作的特征，大量基于深度学习的神经网络模型被提出来解决ALSC任务，如基于rnn的模型[6-9]，基于cnn的模型[10,11]。为了更好地对方面级情感进行分类，在构建神经网络时也考虑了目标信息。注意机制在图像识别、机器翻译、句子摘要等方面已被证明是有效的[12-14]，在ALSC任务[9]中也引入了注意机制，考虑词与方面的关系，动态计算不同词的注意权重。最近，预训练的基于变压器的模型在各种NLP任务中占据主导地位，在ALSC任务中也受到了很多关注。基于BERT和roberta的模型在各种ABSA基准数据集上取得了突出的成功[15-19]。</p><p>然而，在大多数模型中使用的向量表示都被限制为真实值。复值向量是一个广泛应用于信号处理、量子物理、医学图像处理等各个领域的基本概念，它是由一对正交维上相互关联的实向量和虚向量组成。由于复值向量具有更丰富的表示能力，复化神经网络已被应用于信号处理[20,21]、计算机视觉[22]、自然语言处理[23]等众多领域。从量子理论的角度，将多维实值输入向量映射到频率空间或波向量空间，可以自然地表示为复值输入向量。</p><p>受量子理论启发的量子语言模型(QLMs)具有与复值向量兼容的特性，受到越来越多的关注。在量子语言模型中，每个单词都自然地表示为希尔伯特空间中量子系统中的一个观测状态，并由义素的叠加表示[24,25]。从这样的量子视角出发，将量子算符与神经网络计算进行类比，用量子物理学发展的方法来弥补NLP中神经网络可解释性的不足，可以更有原则地构建神经网络。基于此背景，Li et al.[25]构造了用于问答任务的复值网络，Zhao et al.[26]提出了基于量子期望值的语言模型。受量子语言模型的出色工作及其与复值向量的兼容性的激励，我们研究使用量子物理数学框架中的复值表示来解决ABSA任务。为此，通过引入语义希尔伯特空间构建复值神经网络，其中ABSA模型的一个词被视为物理状态，编码为复值向量。为了对结果模型的性能进行基准测试，每个模型都根据其相应的实值基线进行评估。在本研究中，我们构建了三个强实基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。从数学角度看，实向量空间V的复化是通过取V与复数的张量积来定义的。这里，实值基线的复化遵循类似的操作。值得注意的是，通过将复值向量的虚部设为零，复值模型只是缩小到它们的实值等价。</p><p>在三个基准数据集(即Twitter、Restaurant 14和Laptop 14)上的实验结果表明，我们的复值嵌入模型表现出色，并说明复值嵌入可以携带超出真实嵌入的额外信息。具体而言，LSTM模型的复值版本和基于注意的LSTM模型都优于原始模型。同时，复值RoBERTa模型在三个标准基准上优于或接近于之前的SOTA模型。结果表明，由量子物理粒子表示扩展而来的复杂化有可能被封装到ABSA的通用语言模型中。</p></blockquote><h3 id="基于方面的情感分析"><a href="#基于方面的情感分析" class="headerlink" title="基于方面的情感分析"></a>基于方面的情感分析</h3><p>（Aspect-Based Sentiment Analysis, ABSA）</p><p>基本情感要素：方面类别、方面项、意见项、情感极性</p><p>两个子任务：方面提取（AE）；方面级情感分类（ALSC）</p><h3 id="复数值向量"><a href="#复数值向量" class="headerlink" title="复数值向量"></a>复数值向量</h3><p>由一对<strong>正交</strong>维上<strong>相互关联</strong>的实向量和虚向量组成</p><p>优势：具有更丰富的表示能力</p><p>应用：信号处理、计算机视觉、自然语言处理</p><p>与量子理论的联系：将多维实数值输入向量映射到频率空间或波向量空间</p><h3 id="量子语言模型"><a href="#量子语言模型" class="headerlink" title="量子语言模型"></a>量子语言模型</h3><p>每个单词表示为希尔伯特空间中量子系统的一个观测状态，并由义素的叠加表示</p><p>优势：用量子物理学发展的方法，可以弥补NLP中神经网络解释性不足的问题</p><p>相关工作：Li 构造用于问答任务的复值网络；Zhao 量子期望值语言模型</p><h3 id="该文研究内容"><a href="#该文研究内容" class="headerlink" title="该文研究内容"></a>该文研究内容</h3><ol><li><p>构建了三个模型：复值LSTM；复值注意力LSTM；复值BERT&#x2F;RoBERTa</p></li><li><p>实验数据集：Twitter；Restaurant 14; Laptop 14</p></li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><blockquote><p>本节将介绍ALSC任务和QLMs的相关研究。</p><p>为了解决ALSC任务，循环神经网络和卷积神经网络是最常用的深度神经网络架构。结合目标词的信息，Tang et al.[8]首次提出了两种单向LSTMs (TD-LSTMs)，分别独立处理目标词的左右上下文。随后，在ALSC任务中引入了注意机制。MemNet由Tang et al.[27]设计，它使用多跳注意力来揭示每个上下文单词相对于方面目标的重要性。Wang等[9]设计了ATAE-LSTM，以方面词嵌入和上下文词嵌入的串联为输入，应用注意机制动态计算注意权重。Chen等人发布的内存循环注意(Recurrent Attention on memory, RAM)是一种双向LSTM，利用具有记忆层的多重注意机制来获取全局语义特征。</p><p>近年来，基于变压器的模型在各种NLP任务中占据主导地位，ALSC任务也引起了广泛的关注。Song et al.[15]提出的BERT- SPC是一种纯BERT文本对分类模型，并取得了优异的性能。通过预先训练的BERT提供输入词嵌入，他们同时设计了一个注意编码器网络(AEN)，该网络可以利用注意机制获得语义词-上下文交互。Yang等[16]提出了一种基于多头自注意的局部上下文焦点(LCF)机制。BERT- ADA表明，通过在任务相关语料库[17]上进行微调，可以进一步改进只适应特定任务的预训练BERT。</p><p>第一个QLM是由Sordoni, Nie和Bengio[29]在信息检索(IR)中提出的，将Hilbert空间简化为实空间。受到他们工作的启发，人们对量子语言模型进行了广泛的研究[24,30]。受限于真实空间，Zhang et al.[24]提出了一种端到端的基于神经网络的类量子语言模型(Quantum-like Language Model, NNQLM)来解决问答任务。在该模型中，每个单词都被视为系统中的一个纯量子态，问答句分别由其对应的密度矩阵表征。Zhang et al.[31]采用张量积来描述整个词序列之间的相互作用，建立了一个仅采用实值嵌入的量子多体波函数启发的语言模型。</p><p>值得注意的是，上述模型中使用的向量表示仅局限于实值神经网络。特别是对于qlm，他们将希尔伯特空间过度简化为真正的子空间。因此，后来又提出了复值量子语言模型，将量子系统的物理状态恰当地表示为复值函数。在问答任务中，复值量子语言模型已被成功应用，并证明了复杂形式的单词表示在额外的虚数部分的支持下，可以提供比真实单词更多的信息。在这些模型中，Li et al.[25]建立了复值匹配网络(complex-value Network for Matching, CNM)，其中每个词在极坐标系统中编码为一个复值向量，其长度和方向分别表示相对词权和叠加。Zhao等人[26]提出了一种基于量子期望值的语言模型。在此框架下，语言模型具有良好的可解释性和性能。李等人。研究了复值神经网络在视频情感分析任务中的应用，其模型与目前最先进的模型[32]取得了相当的效果。</p><p>然而，据笔者所知，复值框架还没有在ABSA中得到任何应用。因此，受量子语言模型令人兴奋的工作及其与复值向量的兼容性的启发，我们从量子物理的数学框架中研究了复值表示在ABSA任务中的应用。</p></blockquote><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>首先介绍方面级情感分类的一些工作：TD-LSTMs，RAM（Atthention），BERT-SPC，BERT-ADA；</p><p>然后介绍QLM相关工作：NNQLM，量子多体波函数语言模型，CNM，量子期望值语言模型。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>此处简单介绍了一下量子计算理论基础，翻译略。</p><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224306577.png" class alt="image-20230110224306577"><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><blockquote><p>复值语言模型受量子语言模型的启发，一个词被视为量子系统中的一个物理可观测物，它由一个复值向量表示。在此范围下，将三个均以复值嵌入为输入的QLMs进一步构造为三个强实值基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。将得到的三种复值模型与相应的实值模型进行比较，以衡量其性能。通过对比，我们希望看到嵌入的虚部可以携带超出实部的额外信息，进一步强调引入量子语言模型的重要性。首先，我们愿做一个澄清。本文基于三个典型的实基线，建立了三个复值语言模型。然而，除了被选中的，还有大量不同类型的神经网络用于语言任务。我们希望这三种类型的模型能够阐明复值结构对提高模型性能的影响，并显示进一步探索其他类型神经网络结构的可能性。</p><p>在本节中，我们首先介绍了实向量空间的复化，将单词编码为复值向量的过程，然后介绍了构造三个实基线的复化的方法。</p></blockquote><h3 id="复数值语言模型"><a href="#复数值语言模型" class="headerlink" title="复数值语言模型"></a>复数值语言模型</h3><h4 id="Complex-valued-LSTM"><a href="#Complex-valued-LSTM" class="headerlink" title="Complex-valued LSTM"></a>Complex-valued LSTM</h4><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224831764.png" class alt="image-20230110224831764"><h4 id="Complex-Valued-Attention-Based-LSTM"><a href="#Complex-Valued-Attention-Based-LSTM" class="headerlink" title="Complex-Valued Attention-Based LSTM"></a>Complex-Valued Attention-Based LSTM</h4><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224820862.png" class alt="image-20230110224820862"><h3 id="Complex-Valued-BERT-x2F-RoBERTa-Model"><a href="#Complex-Valued-BERT-x2F-RoBERTa-Model" class="headerlink" title="Complex-Valued BERT&#x2F;RoBERTa Model"></a>Complex-Valued BERT&#x2F;RoBERTa Model</h3><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224804949.png" class alt="image-20230110224804949"><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验数据集"><a href="#实验数据集" class="headerlink" title="实验数据集"></a>实验数据集</h3><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110225048766.png" class alt="image-20230110225048766"><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110225134938.png" class alt="image-20230110225134938"><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>这篇工作是利用复数值网络构建了三个模型：C-LSTM，C-ATAE-LSTM，C-BERT&#x2F;C-RoBERTa，实验通过方面级情感分析来验证模型结果。</p><p>与我提出来的ComplexQNN的区别：</p><ul><li>相同点：都利用了复数值网络去构建量子启发式模型</li><li>不同点：ComplexQNN尝试构建了量子系统模块，如词语以量子态表示，句子以密度矩阵表示，中间特征提取通过演化（维度不发生改变），最后结果通过测量得到预测结果</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Qin Zhao 1,† , Chenguang Hou 2,† and Ruifeng Xu 1,3,*&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="论文" scheme="https://levyya.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>hexo图片加载问题</title>
    <link href="https://levyya.github.io/2023/01/09/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E9%97%AE%E9%A2%98/"/>
    <id>https://levyya.github.io/2023/01/09/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E9%97%AE%E9%A2%98/</id>
    <published>2023-01-09T11:50:18.000Z</published>
    <updated>2023-01-10T12:35:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Typora设置"><a href="#Typora设置" class="headerlink" title="Typora设置"></a>Typora设置</h2><p>修改图像保存路径为相对路径（以文件名为路径）</p><img src="/2023/01/09/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E9%97%AE%E9%A2%98/image-20230109195354353.png" class alt="image-20230109195354353"><h2 id="Hexo设置"><a href="#Hexo设置" class="headerlink" title="Hexo设置"></a>Hexo设置</h2><p>修改hexo根路径下配置文件<code>_config.yml</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure><p>（创建博客时，自动创建对应存放图片的文件夹）</p><h2 id="安装两个插件"><a href="#安装两个插件" class="headerlink" title="安装两个插件"></a>安装两个插件</h2><ol><li><p>hexo-asset-image</p><ol><li>使用下面命令安装</li></ol><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>此时生成的html链接中，图片路径不对，需要修改该模块的index.js内容</p><ol start="2"><li>替换node_modules&#x2F;hexo-asset-image&#x2F;index.js</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> cheerio = <span class="built_in">require</span>(<span class="string">&#x27;cheerio&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getPosition</span>(<span class="params">str, m, i</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> str.<span class="title function_">split</span>(m, i).<span class="title function_">join</span>(m).<span class="property">length</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> version = <span class="title class_">String</span>(hexo.<span class="property">version</span>).<span class="title function_">split</span>(<span class="string">&#x27;.&#x27;</span>);</span><br><span class="line">hexo.<span class="property">extend</span>.<span class="property">filter</span>.<span class="title function_">register</span>(<span class="string">&#x27;after_post_render&#x27;</span>, <span class="keyword">function</span>(<span class="params">data</span>)&#123;</span><br><span class="line">  <span class="keyword">var</span> config = hexo.<span class="property">config</span>;</span><br><span class="line">  <span class="keyword">if</span>(config.<span class="property">post_asset_folder</span>)&#123;</span><br><span class="line">    <span class="keyword">var</span> link = data.<span class="property">permalink</span>;</span><br><span class="line"><span class="keyword">if</span>(version.<span class="property">length</span> &gt; <span class="number">0</span> &amp;&amp; <span class="title class_">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>)</span><br><span class="line">   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="comment">// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span></span><br><span class="line"><span class="keyword">var</span> endPos = link.<span class="title function_">lastIndexOf</span>(<span class="string">&#x27;/&#x27;</span>) + <span class="number">1</span>;</span><br><span class="line">    link = link.<span class="title function_">substring</span>(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> toprocess = [<span class="string">&#x27;excerpt&#x27;</span>, <span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;content&#x27;</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; toprocess.<span class="property">length</span>; i++)&#123;</span><br><span class="line">      <span class="keyword">var</span> key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      <span class="keyword">var</span> $ = cheerio.<span class="title function_">load</span>(data[key], &#123;</span><br><span class="line">        <span class="attr">ignoreWhitespace</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">xmlMode</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">lowerCaseTags</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">decodeEntities</span>: <span class="literal">false</span></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(<span class="string">&#x27;img&#x27;</span>).<span class="title function_">each</span>(<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line"><span class="keyword">if</span> ($(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>))&#123;</span><br><span class="line"><span class="comment">// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span></span><br><span class="line"><span class="keyword">var</span> src = $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>).<span class="title function_">replace</span>(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>);</span><br><span class="line"><span class="keyword">if</span>(!<span class="regexp">/http[s]*.*|\/\/.*/</span>.<span class="title function_">test</span>(src) &amp;&amp;</span><br><span class="line">   !<span class="regexp">/^\s*\//</span>.<span class="title function_">test</span>(src)) &#123;</span><br><span class="line">  <span class="comment">// For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span></span><br><span class="line">  <span class="comment">// In addition, to support multi-level local directory.</span></span><br><span class="line">  <span class="keyword">var</span> linkArray = link.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">var</span> srcArray = src.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span> &amp;&amp; elem != <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">if</span>(srcArray.<span class="property">length</span> &gt; <span class="number">1</span>)</span><br><span class="line">srcArray.<span class="title function_">shift</span>();</span><br><span class="line">  src = srcArray.<span class="title function_">join</span>(<span class="string">&#x27;/&#x27;</span>);</span><br><span class="line">  $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>, config.<span class="property">root</span> + link + src);</span><br><span class="line">  <span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;update link as:--&gt;&quot;</span>+config.<span class="property">root</span> + link + src);</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;no src attr, skipped...&quot;</span>);</span><br><span class="line"><span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>($(<span class="variable language_">this</span>));</span><br><span class="line">&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.<span class="title function_">html</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>此时可以使用语法<code>&#123;% asset_img img_name.jpg img_description %&#125;</code> 来正常使用图片，但是用markdown语法还是无法显示。</p></li><li><p>hexo-simple-image</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>安装完这个插件后，可以使用原生markdown图片语法（<code>![]()</code>）</p></li></ol><h2 id="参考致谢"><a href="#参考致谢" class="headerlink" title="参考致谢"></a>参考致谢</h2><ol><li><a href="https://cloud.tencent.com/developer/article/1600295?from=article.detail.1702112">https://cloud.tencent.com/developer/article/1600295?from=article.detail.1702112</a></li><li><a href="https://willern.gitee.io/2019/10/14/20191014/">https://willern.gitee.io/2019/10/14/20191014/</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Typora设置&quot;&gt;&lt;a href=&quot;#Typora设置&quot; class=&quot;headerlink&quot; title=&quot;Typora设置&quot;&gt;&lt;/a&gt;Typora设置&lt;/h2&gt;&lt;p&gt;修改图像保存路径为相对路径（以文件名为路径）&lt;/p&gt;
&lt;img src=&quot;/2023/01/</summary>
      
    
    
    
    <category term="Hexo" scheme="https://levyya.github.io/categories/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://levyya.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>github本地项目上传</title>
    <link href="https://levyya.github.io/2022/12/06/github%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0/"/>
    <id>https://levyya.github.io/2022/12/06/github%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0/</id>
    <published>2022-12-06T07:24:42.000Z</published>
    <updated>2023-03-15T05:34:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>查看系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/version</span><br><span class="line"></span><br><span class="line"># 查看内核</span><br><span class="line">uname -a</span><br><span class="line"></span><br><span class="line"># 查看系统版本信息的命令</span><br><span class="line">lsb_release -a</span><br></pre></td></tr></table></figure><p>连接github</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add [origin] git@github.com:xxxx</span><br><span class="line"></span><br><span class="line">查看所有远程仓库</span><br><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p>拉取文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get fetch [origin]</span><br><span class="line">git merge name/branch</span><br></pre></td></tr></table></figure><p>添加文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br></pre></td></tr></table></figure><p>第一次上传</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git push -u [origin] main</span><br><span class="line">git push --set-upstream name master</span><br></pre></td></tr></table></figure><p><strong>工作区</strong></p><p>慎用git clean -df！！</p><p><strong>暂存区</strong></p><p><strong>资源库</strong></p><p><strong>git仓库</strong></p><h2 id="git-常用命令"><a href="#git-常用命令" class="headerlink" title="git 常用命令"></a>git 常用命令</h2><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;message&quot;</span><br><span class="line">git push</span><br><span class="line">git fetch/clone</span><br><span class="line">git pull</span><br><span class="line"></span><br><span class="line">git branch -a</span><br><span class="line">git checkout</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;查看系统&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="github" scheme="https://levyya.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>ComplexQNN</title>
    <link href="https://levyya.github.io/2022/12/05/ComplexQNN/"/>
    <id>https://levyya.github.io/2022/12/05/ComplexQNN/</id>
    <published>2022-12-05T09:24:03.000Z</published>
    <updated>2023-02-22T06:30:09.000Z</updated>
    
    <content type="html"><![CDATA[<div style="font-family:verdana;font-size: 22px"> ComplexQNN: A Complex-valued Quantum-inspired Language Model </div><h2 id="中文版"><a href="#中文版" class="headerlink" title="中文版"></a>中文版</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>问题：现有的量子启发式模型都是基于振幅相位嵌入，将词语映射到希尔伯特空间中。而量子计算理论中，量子态对应的向量都是复数值，所以需要使用欧拉公式进行转换。</p><p>想法：复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等。实际上，复数值神经网络在表示能力上要优于实数值神经网络，适合对复杂的自然语言进行建模。另一方面，量子启发式模型是定义在希尔伯特空间下的，它同样是一个复数空间，因此可以很自然地基于复数值神经网络构建量子启发式模型。于是我们提出了一种新的模型ComplexQNN，它可以在实部和虚部嵌入异构的语义信息，从而提升模型的表达能力。</p><p>实验：我们在六个情感分类数据集上进行了实验。相比于一些经典的模型如ELMo，BERT和RoBERTa，ComplexQNN能够取得竞争性的结果。</p><hr><h3 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h3><blockquote><p>引用：”NLP Meets Quantum Physis”</p><p>量子启发式神经网络模型和现有的神经网络模型之间的主要区别在于，前者使用描述语言特征的量子理论的数学框架，然后将量子力学概念来描述的这些特征作为神经网络的输入。</p><p>使用量子力学概念来描述特征具有更好的可解释性，因为他们具有更透明的物理解释，也更有利于后续的神经网络提取有用的信息。</p><p>适用性，借鉴量子力学的数学框架，而处理数据时不受量子计算操作的限制。</p></blockquote><p>情感分析方法经历了早期的机器学习算法到目前流行的深度学习算法，早期的机器学习方法通过特征选择算法进行文本的特征向量学习，如词袋模型（Bag of Words, BOW)，词频-逆文档频率（Term Frequency-Inverse Document Frequency，TF-IDF），这些方法考虑了词语的出现频率，但是丢失了词语的顺序和上下文信息。目前的深度学习方法基于连续词袋模型（CBOW）和Skip-gram，通过词嵌入层（Word Embedding）将离散的词语映射到高维向量空间，再连接其它网络模块进行特征学习，能够保留文本的上下文信息。按处理的文本序列长度分类，可分为句子级情感分类和篇章级。</p><p>量子启发式模型是结合了量子计算理论和深度学习理论构造的一种新的神经网络模型。它是在NISQ时代量子硬件发展受限提出的一种启发式方法，借助量子计算理论的思想，而采用经典的计算方式实现。量子启发式语言模型是对自然语言进行建模，它受到自然语言和量子系统微粒的相似性启发，如语言的一词多义现象和微粒的叠加态，语言随时间、空间变化而变化和微粒随时间改变不断演化，语言在特定场景下语义被确定和微粒被测量后坍塌到某种基态等。量子启发式语言模型相比于经典的神经网络模型，量子启发式语言模型更加符合自然语言的特性，具有更好的可解释性和指数级加速的潜能，而且量子启发式语言模型是定义在希尔伯特空间下的，它具有更强的表示能力。</p><p>近年来，自然语言处理（Natural Language Processing, NLP）受益于深度学习的发展，在包括情感分析、问题回答、机器翻译、文本生成等诸多领域取得重大进展。循环神经网络（RNN）以及长短期记忆（LSTM）和循环门控单元（GRU），可以学习上下文之间的关联信息，是NLP中常用的神经网络模块，不过循环神经网络存在梯度爆炸和梯度消失的问题，难以构建深度神经网络。Transformer基于注意力机制（Self-Attention）和残差结构，能够关注到数据中的重点信息并且记忆前面学习的知识，非常适合构建深度神经网络。自然语言处理目前使用的主要技术是基于Transformer构建的预训练语言模型，常见预训练语言模型有BERT、RoBERTa、GPT等。这些预训练模型可用作文本编码器，再通过微调策略，可以轻易适应不同的下游任务。目前，NLP研究方向开始朝向更大的数据集，更大的模型方面发展，像GPT-3，训练的数据集规模达到750GB，参数量更是高达1750亿。虽然计算能力在逐年提升，但面对如此庞大的数据集以及参数量超千亿的模型，研究者仍然会感到束手束脚。</p><p>量子计算是一种全新的计算理论，已经有工作证明在一些任务上，量子计算机具有指数级计算复杂度加速优势，比如著名的Shor算法，能够以多项式复杂度完成整数的质因子分解，这威胁到了经典的基于RSA加密算法的通信过程。当前也有一些工作基于量子计算理论对自然语言建模，这主要有两方面原因，一是人类语言和量子系统具有很多的相似性，比如语言的歧义性和量子叠加态，语言的进化性和量子态的演化；二是量子计算具有潜在的指数级加速优势，这对目前需要花费大量资源训练网络的预训练方法来说非常有吸引力。目前已经有一些关于量子自然语言处理的工作，如剑桥的Bob Coecke 提出DisCoCat以及lambeq，将自然语言编码成字符串后，再编码为量子线路，通过参数化量子线路学习，可以实现文本分类任务。现阶段，量子机器学习的实验进展因为量子计算设备中量子比特与纠错能力的限制还停留在初步阶段。因此，运行在真实的量子计算机上，能够处理的文本规模还很小，只能处理包含10几个词的100条句子规模的数据集任务。另一方面，还有研究者基于量子计算理论数学框架构建量子启发式模型，这种算法模型不需要运行在真实的量子计算机上，只是借鉴了量子计算理论中的相关概念来帮助自然语言建模，因此不会受到硬件发展的束缚。</p><p>量子启发式模型是基于量子计算理论的数学框架对自然语言进行建模，将自然语言类比于量子系统，同时使用经典神经网络模型去模拟这一过程。然而，现有的量子语言模型如NNQLM，ICWE等，是通过振幅相位编码模拟量子态的构建，再利用欧拉公式得到量子态的复数值表示。实际上，直接使用复数值神经网络进行构建量子启发式模型也是可行的。有许多研究者在研究复数值神经网络，Chiheb Trabelsi et al. 提出了深度复数值卷积网络，并在图像分类、音乐转录和语音频谱预测任务取得了不错的性能。</p><p>正是考虑到量子计算理论是定义在希尔伯特空间上，量子态对应的向量表示中每一个维度都是复数值，而且很少有工作基于复数值神经网络来构造。因此，为了更加合理地构建量子启发式模型，以及丰富模型的表达能力，我们基于复数值神经网络，提出了复数值量子启发式模型（ComplexQNN），省略了振幅相位的模拟过程，简化了量子启发式模型的开发。我们基于提出的模型进行了广泛实验，使用多个情感分类数据集对模型进行测评，并和经典的模型进行比较，证明了我们模型的有效性。</p><p>综上所述，我们总结了这项工作的核心内容：完全基于复数神经网络构建了一个新的量子启发式模型ComplexQNN，它更符合量子计算理论。情感分类任务上的实验结果验证了ComplexQNN的有效性，在6个情感分类数据集上的实验结果证明，ComplexQNN具有良好的性能，与经典的TextCNN、BiLSTM、ELMo、BERT和RoBERTa相比都有竞争性的优势。</p><p>本文的其余部分组织如下。第2节概述了背景和相关工作。启发式程序在第3节中介绍，并在第4节中评估和讨论。第五节对全文进行了总结。</p><hr><h3 id="第二章-相关工作"><a href="#第二章-相关工作" class="headerlink" title="第二章 相关工作"></a><strong>第二章 相关工作</strong></h3><p>这一章，我们会介绍与复数值量子启发式神经网络有关的知识，包括量子计算,复数值神经网络,和量子启发式神经网络的研究进展。</p><p>量子计算是基于量子力学而非经典物理学的思想的一种新型计算方式。1984年，David Deutsch首次提出通用量子计算机的概念，量子计算机从速度上对经典计算机有本质的超越。传统计算机可以模拟量子计算机，但是效率不是很高。1994年，Peter Shor提出量子计算机可以解决寻找整数的素因子问题和解决所谓离散对数问题。1995年，Lov Grover证明量子计算机上可以加速在没有结构的搜索空间上进行搜索的问题。</p><p>我们将介绍量子计算中的一些基本概念，如量子态、量子系统、量子态演化和量子测量。</p><p><strong>量子机器学习</strong></p><p>将数据编码与纠缠量子线路相结合，通过测量来推理数据实例的类标签。对主体线路进行传统的特征描述和存储，并对线路中的参数进行混合量子或传统训练。</p><p>量子线路由单量子比特旋转门和双量子比特受控旋转门组成，其中旋转门中的旋转角度是可学习的参数。旋转门和受控旋转门对于量子计算来说是通用的，任何酉矩阵都可以分解成由一组特定量子门构成的线路。</p><p><strong>量子启发式模型</strong></p><p>Sordoni等人于2013年提出了量子语言模型，这是量子概率理论在自然语言处理的第一个实际应用。QLM将量子理论和自然语言处理语言模型相关研究结合了起来，在理论上具有重大的意义。然而，QLM在许多方面具有局限性。比如，term的表示采用的是one-hot形式的向量。相比于现在常用的分布式词向量(distributed representation), one-hot向量没有能力考虑文本的全局语义信息，会占用更多的存储空间，同时也由于自身独特的表示形式也会造成大量存储资源的浪费。其次，QLM难以将通过迭代计算得到的密度矩阵嵌入到一个端到端的可以通过反向传播算法进行训练的神经网络当中，因而很难得到实际的应用。2018年，Peng Zhang等人在QLM的基础之上，利用类量子力学理论建立深度学习网络，提出端到端的类量子神经网络模型(End-to-End Quantum-like Language Models, NNQLM)来完成NLP中的问答匹配任务，是最早的将类量子力学理论和深度学习技术结合，并实现端到端的神经网络模型在NLP中的一次应用。然而，由于模型采用的是实数词向量，因而未能真正意义上地模拟量子微粒量子态，也没能够充分利用量子力学密度矩阵的概率属性。因此NNQLM的研究和实现目的只是将量子力学和NLP的相关理论进行推广。</p><p>同样是2018年，Li等人采用复数词向量模拟量子态，提出量子启发式复数词嵌入，并结合量子力学理论基础，使用密度矩阵的形式表示文本语句，使用投影测量来对密度矩阵形式的文本语句进行观察测量，使用测量得到的概率值来推断文本语句的极性，进而完成NLP中的文本分类任务。使用复数词向量表示的密度矩阵完全契合了量子力学理论，基于量子力学基础提出的网络模型提高了网络在自然语言处理任务的可解释性。同时，相比于一些经典的机器学习模型(比如Unigram-TFIDF)，基于复数基向量模拟量子态设计实现的模型在五个英文二分类的数据集上具有更加优异的表现性能。因此，将量子力学理论和深度神经网络相结合的模型是时下的研究热点之一。2019年，Benyou Wang等人又在使用复数词向量模拟量子态的基础之上，实现端到端的神经网络模型来完成问答匹配任务，Benyou Wang等人分别使用复数形式的密度矩阵表示问句和答句，并分别对密度矩阵形式的问句和答句进行投影测量，最后基于投影测量得到的概率值来计算问句和答句的相似性，从而筛选出问题的正确答案。</p><p>2020年，Jiang和Zhang提出了一种量子干涉启发的神经网络匹配模型（QINM）用于处理信息提取任务，可以把干涉现象嵌入到信息提取过程中，实验结果显示要优于先前提出的量子启发式信息提取模型和一些神经网络信息提取模型。</p><p>2021年，Peng Zhang等人提出了TextTN，一种基于量子理论构建的文本张量网络，用于处理文本分类任务。TextTN可分为两个子模型，首先用将词生成张量网络（word-GTN）将词语编码为向量，随后用句子判别张量网络（sentence-DTN）对句子进行分类。Yazhou Zhang等人提出一种复数值模糊神经网络用于对话讽刺识别，将量子理论成功与模糊逻辑理论结合在一起。Jinjing Shi et al. 提出两种端到端的量子启发式深度神经网络ICWE-QNN 和 CICWE-QNN用于文本分类。模型采用了GRU，CNN和注意力机制改进了量子启发式模型，可以解决CE-Mix模型中忽略文本内部语言特性的问题。</p><blockquote><p>在量子复杂词嵌入的激励下，提出了可解释复值词嵌入(ICWE)，设计了两个端到端的量子深度神经网络(ICWE- qnn和CICWE-QNN代表基于ICWE的卷积复值神经网络)用于二进制文本分类。它们在NLP应用中具有被证明的可行性和有效性，可以解决CE-Mix[1]模型中由于忽略文本的重要语言特征而导致的文本信息丢失的问题，因为我们的模型采用深度学习算法进行语言特征提取，其中门控循环单元(GRU)提取句子的序列信息，注意机制使模型关注句子中重要的词，卷积层捕捉投影矩阵的局部特征。ICWE-QNN模型避免了词性符号的随机组合，CICWE-QNN充分考虑了投影矩阵的文本特征。在5个基准分类数据集上的实验表明，所提模型比CaptionRep Bow、DictRep Bow和para - phrase等传统模型有更高的准确率，并且在f1评分上也有很好的表现。特别是对于SST、SuBJ、CR和MPQA四组数据集，CICWE-QNN模型的精度均高于量子激励模型CE-Mix。设计量子深度神经网络以提高文本分类性能是一种有意义和有效的探索。</p></blockquote><p>量子启发式算法只是使用量子理论的数学框架，所以不用运行在真实的量子计算机上，在实用性上要比量子算法好。近年来不断涌现的量子启发式模型应用在NLP表明，这个研究方向是可行的。相较于神经网络模型，量子启发式算法的优势在于能赋予模型物理意义，使模型具有更好的可解释性。此外，量子启发式算法还能嵌入如量子干涉、量子纠缠等量子特性到模型中，进而增强模型的学习能力。</p><p>综上，将结合量子力学理论的神经网络应用到NLP领域是可行的。同时，现在的量子启发式模型很少基于复数值神经网络来构造，没有充分利用量子计算理论。这也是我们做这项工作的原因。</p><p><strong>1. 量子启发式模型</strong></p><p><strong>(1)量子态构建</strong></p><p>自然语言中的单词对应量子系统中的单个微粒，如同微粒存在上旋和下旋状态，以狄拉克符号可表示为$|\psi\rangle &#x3D; \alpha|\uparrow\rangle + \beta |\downarrow\rangle$,单词存在不同的词义或情感意义，如”long”可以表示一段遥远的时间记为$|S_1\rangle$,也可以表示相对较大的空间属性记为$|S_2\rangle$，单词”long”可用量子态表示为$|long\rangle &#x3D; \alpha |S_1\rangle + \beta |S_2\rangle$，其中$\alpha, \beta$都是复数，它们的模长$|\alpha|^2, |\beta|^2$表示对应量子基态的概率，且满足$|\alpha|^2+|\beta|^2&#x3D;1$。</p><p>对于任意一个单词$t$，其量子态可以表示为<br>$$<br>|t\rangle &#x3D; \sum w_i |e_i\rangle &#x3D; \sum \alpha_i e^{i\beta_i} |e_i\rangle<br>$$<br>上式中$w_i$为语义权重系数，$|e_i\rangle$为该单词的第$i$个语义，$\alpha_i, \beta_i$为$w_i$的极坐标表示，$\alpha_i$为振幅，$\beta_i$为相位。这样的量子态表示中，振幅表示的为语义信息，相位表示的是隐含的特征信息，如情感信息或者多义词信息。基态$|e_i\rangle$为基本的语义。最终，单词表示为多种基态词义构成的叠加态，可以表示词语的不确定性。</p><p><strong>(2)特征嵌入</strong></p><p>量子启发式模型相比于经典的NLP模型，一个显著区别就是引入了复数词向量。在实验中，复数词向量是通过构建振幅嵌入层和相位嵌入层来实现的，利用欧拉公式$re^{i\theta} &#x3D; r(cos\theta + i sin\theta)$，可以分别得到复数的实部和虚部系数。在先前的工作中，振幅表示语义信息，相位表示隐含特征，振幅往往采用Glove词向量，相位则是按Xavier正态分布随机初始化。这里限制了复数词向量的表示能力，且随机初始化的结果不适用于不同的任务。</p><p>我们改进相位的初始化方式，基于不同的下游任务，为相位嵌入层赋予不同的初始化参数。例如，处理情感分析任务中，相位嵌入层的初始化参数为包含情感信息的相位特征；处理词义消歧任务中，相位嵌入层的初始化参数为包含一次多义信息的相位特征。这种针对任务嵌入的特征能够显著提高模型的效果，在第4节实验结果能够看到这一点。要获得这样的特征参数，可以考虑两种方式：（a）选择公开的大规模语料库预训练词向量，如word2vec，Glove等，然后映射到$[0, 2\pi]$上；（b）训练一个朴素的量子启发式模型，抽取复数词向量中相位嵌入层的参数。在我们的实验中，我们采用的是第二种方式，因为第一种预训练词向量往往是固定词嵌入维度的，而第二种方式可以根据需求自定义词嵌入的维度大小。</p><p><strong>(3)量子态演化</strong></p><p>量子理论中，表示量子系统的量子态是随时间不断变化的，数学上表示为$|\psi\rangle&#x3D;U|\psi\rangle$，其中$U$是一个酉矩阵$U^\dagger U&#x3D;I$。这一特点也同样存在于自然语言中，某些词语会因为一些社会历史原因改变其原本的含义，如”silly”起源于古英语”saelig”，意为“快乐的、无忧无虑的”，而当今的意义已经变成“愚蠢的”。 这种语义的变化可以通过大型语料库训练学习，也可以根据文本上下文信息推测得到。在神经网络模型中，循环神经网络如长短期记忆（LSTM）、门控单元（GRU），能够学习时序数据中较长的内容，非常适合处理文本这样的前后相关联的数据。可使用循环神经网络模型如GRU来模拟量子态的演化，一个词语的量子态会因周围词语变化而改变，这种变化过程可用GRU模拟。GRU将先前词语的状态和当前词语的状态作为输入，中间经过选择性记忆门操作，输出开始至当前词语的状态向量。</p><p><strong>(4)量子态测量</strong></p><p>前面我们得到了一个句子中每个词语对应的量子态，记为$S&#x3D;{|t_i\rangle}$，下面我们需要得到整个句子的表示。量子系统中，多个微粒能够构成一个混合系统，通常表示为密度矩阵$\rho&#x3D;\sum w_i |\phi_i\rangle\langle \phi_i|$。同样的，我们将一个句子表示为密度矩阵$\rho&#x3D;\sum w_i |t_i\rangle \langle t_i|$，其中$w_i$表示的是第$i$个单词的权重系数，默认为$w_i&#x3D;\frac{1}{l}$（$l$为句子长度）。这一权重系数也可以通过注意力机制，学习到适合任务的权重。</p><p>得到表示句子的密度矩阵之后，我们将对句子进行测量操作。首先，我们选择一组满足完备性（$\sum M_m^\dagger M_m &#x3D; I$）的测量基$M_m &#x3D; |\lambda_m\rangle \langle \lambda_m|$。之后，我们利用该组测量基对密度矩阵进行测量，测量结果为$p(m)&#x3D;\langle \lambda_m| \rho |\lambda_m\rangle &#x3D; tr(\rho |\lambda_m\rangle \langle \lambda_m|)$。</p><p><strong>(5)分类器</strong></p><p>根据上一个过程得到的测量特征，我们使用一个全连接层将结果映射到一个结果集合大小的向量空间，并使用softmax激活函数将结果表示为预测概率。</p><p><strong>2. 复数值神经网络</strong></p><p>现有的深度学习技术绝大多数是基于实数值运算和表示的，实际上，复数可能具有更丰富的表示能力，有工作证明复数值神经网络具有一些独特的优势：有可能实现更容易地优化，更好的泛化特征，更快的学习，以及允许噪声鲁棒的记忆机制。2018年，Chiheb Trabelsi等人提出深度复数值神经网络，提出了复数值批量归一化和复数值权重初始化等训练复数值神经网络的关键模块，还提出了复数值卷积神经网络架构，并通过图像分类、音乐转录以及语音频谱预测实验进行了模型效果验证。下面简单介绍复数值神经网络的原理，包括复数值表示、复数值激活函数以及复数值卷积神经网络三部分内容：</p><p><strong>复数值线性层</strong></p><p>线性层也叫全连接层，每个神经元都和上一层所有神经元相连，是神经网络中最常见的网络结构。其计算公式描述为<br>$$<br>f&#x3D;WX+b<br>$$<br>其中$W$表示网络中的权重矩阵，$b$表示网络层中的偏置。C. Trabelsi et al.基于PyTorch库实现了复数值线性层。复数值线性层使用两个实数值线性层分别计算实部和虚部，在输出时基于复数计算原理得到新的实部和虚部。具体计算公式如下所示：<br>$$<br>f_r(X) &#x3D; W_r X + b_r\<br>f_i(X) &#x3D; W_i X + b_i\<br>f_c&#x3D; f_r(X_r) - f_i(X_i) + \bold{i} [f_r(X_i) + f_i(X_r)] \<br>   &#x3D; W_rX_r - W_iX_i + b_r - b_i + \bold{i}[W_rX_i + W_iX_r + b_r + b_i]<br>$$<br>复数值线性层是复数值神经网络的基本模块，后面复数值卷积神经网络和复数值循环神经网络都依赖这个模块。</p><p><strong>复数值激活函数</strong></p><p>We call Complex ReLU (CReLU) the complex activation that applies separate ReLUs on both of the real and the imaginary part of a neuron, i.e:<br>$$<br>CReLU(z) &#x3D; ReLU(R(z)) + i ReLU(I(z))<br>$$</p><p><strong>复数值批正则化</strong><br>$$<br>BN(\hat{x})&#x3D; \gamma \hat{x} + \beta<br>$$</p><p>深度网络通常依赖于批处理归一化(Ioffe和Szegedy, 201s)来加速学习。在某些情况下，批归一化是优化模型的必要条件。批处理归一化的标准公式只适用于实际值。在本节中，我们提出了一个可用于复值的批量归一化公式。要将一组复数标准化为标准正态复数分布，仅仅平移和缩放它们，使它们的均值为o，方差为l是不够的。这种归一化不能确保实分量和虚分量的方差相等，得到的分布也不能保证是圆形的;它将是椭圆形的，可能具有高离心率。相反，我们选择将这个问题视为二维矢量的美白问题，这意味着将数据按两个主成分方差的平方根缩放。这可以通过将以o为中心的数据(æ - E])乘以2x2协方差矩阵V的平方根倒数来实现</p><p><strong>复数值循环神经网络</strong></p><p>循环神经网络不同于线性层，每层的输出都依赖于前面的输出，能够学习前面数据中的信息。RNN常用来处理序列数据，在自然语言处理中有广泛的应用。LSTM和GRU是两种常用的循环神经网络，我们以复数值LSTM为例介绍复数值循环神经网络。如图1所示，LSTM中含有多个门：遗忘门、输入门和输出门，可以选择性地让信息通过。LSTM的计算过程如下所示：<br>$$<br>f_t &#x3D; \sigma(W_{xf} X_t + W_{hf} H_{t-1} + b_f), \<br>    i_t &#x3D; \sigma(W_{xi} X_t + W_{hi} H_{t-1} + b_i), \<br>    o_t &#x3D; \sigma(W_{xo} X_t + W_{ho} H_{t-1} + b_o), \<br>    \widetilde{c_t} &#x3D; \tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c), \<br>    c_t &#x3D; f_t \cdot c_{t-1} + i_t \cdot \widetilde{c_t}, \<br>    h_t &#x3D; o_t \cdot \tanh(c_t)<br>$$<br>其中$\bigodot$表示向量点乘，$f_t$表示遗忘门，用于丢弃过去不重要的信息，$i_t$表示输入门，$o_t$表示输出门，$\widetilde{c_t}$表示候选记忆单元，$c_t$表示输出的细胞状态，$H_t$表示输出的隐状态。复数值LSTM需要将遗忘门、输入门和输出门使用复数值线性层实现。Sigmoid激活函数以及tanh函数以及向量乘法也要使用复数值计算。这些基于复数值线性层都是比较容易实现的。</p><p><strong>复数值卷积神经网路</strong></p><p>卷积神经网络常用在计算机视觉领域，如LeNet，AlexNet，VGG，ResNet，Yolo等，也有用于自然语言处理的TextCNN。卷积神经网络通常包括卷积层、池化层和全连接层。其中卷积层是计算机神经网络的核心，一个卷积层通常包括多个尺寸一致的卷积核，卷积核的个数决定输出的大小。类似于经典的CNN，复数值卷积神经网络也包含复数值卷积层。复数值卷积层包含实部卷积与虚部卷积，计算过程如图1所示。$M_R$和$M_I$分别为实部特征图和虚部特征图，$K_R$和$K_I$分别为实部卷积核和虚部卷积核。复数值卷积层的输出为$M_RK_R-M_IK_I+\bold{i}(M_RK_I+M_IK_R)$。</p><p><strong>3. 我们的想法</strong></p><p>量子启发式模型现有方法都是基于振幅相位嵌入，将词语映射到希尔伯特空间中，而量子计算理论中，量子态对应的向量都是复数值的，所以需要使用欧拉公式进行转换。复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等。实际上，复数值神经网络在表示能力上要优于实数值神经网络，适合对复杂的自然语言进行建模。另一方面，量子启发式模型是定义在希尔伯特空间下的，它同样是一个复数空间，因此可以很自然地基于复数值神经网络构建量子启发式模型。于是我们提出了一种新的模型ComplexQNN，它可以在实部和虚部嵌入异构的语义信息，从而提升模型的表达能力。我们在情感分类数据集上进行了实验，并和基线模型对比，实验结果现实ComplexQNN能够很好地完成任务，并且相比于XX模型有xx%的精度提升。</p><hr><h3 id="第三章-正文"><a href="#第三章-正文" class="headerlink" title="第三章 正文"></a>第三章 正文</h3><p><strong>数据集</strong></p><p>情感二分类数据集</p><p>CR, MPQA, MR, SST, SUBJ</p><p>情感多分类数据集</p><p>SST-5</p><p>【方面级情感分类】</p><p><strong>特征提取以及数据表示</strong></p><p>文本分词</p><p>TF-IDF</p><p>word embedding</p><p>GloVe</p><p>Contextual embedding</p><blockquote><p>《Quantum-Inspired Complex-Valued Language Models for Aspect-Based Sentiment Classification》</p><ol start="4"><li><p>Complex-Valued Language Models</p><p>受量子语言模型的启发，一个词被视为量子系统中的一个物理可观测物，它由一个复值向量表示。在此范围下，将三个均以复值嵌入为输入的QLMs进一步构造为三个强实值基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。将得到的三种复值模型与相应的实值模型进行比较，以衡量其性能。通过对比，我们希望看到嵌入的虚部可以携带超出实部的额外信息，进一步强调引入量子语言模型的重要性。首先，我们愿做一个澄清。本文基于三个典型的实基线，建立了三个复值语言模型。然而，除了被选中的，还有大量不同类型的神经网络用于语言任务。我们希望这三种类型的模型能够阐明复值结构对提高模型性能的影响，并显示进一步探索其他类型神经网络结构的可能性。在本节中，我们首先介绍了实向量空间的复化，将单词编码为复值向量的过程，然后介绍了构造三个实基线的复化的方法。</p></li></ol></blockquote><p><strong>ComplexQNN</strong></p><p><strong>理论部分</strong></p><p>我们提出的复数值量子启发式模型同样也是基于量子计算的数学理论，因此模型中的模块都是定义在希尔伯特空间下。量子启发式模型是对自然语言以量子信息的方式进行建模的，因此首先需要将词语表示为量子态。在单个原子模型中，电子可以处于基态或者是激发态，或是介于两者之间的叠加态。类似的，自然语言因一词多义现象，同样可以表示为叠加态。比如词语$w$，具有$n$种不同的语义 ($n&#x3D;2^m, m\ge0$)，记为$e_i$，那么该词的量子态表示为<br>$$<br>|w\rangle &#x3D; \sum_{i&#x3D;1}^n \alpha|e_i\rangle<br>$$<br>其中$\alpha$是$n$维复向量，而$|\alpha_i|^2$表示词语$w$表示词义$e_i$的概率。以n&#x3D;4为例，<br>$$<br>|w\rangle &#x3D; \sum_{i&#x3D;1}^4 \alpha |e_i\rangle \<br>&#x3D; \alpha_{00}|e_{0}\rangle + \alpha_{01}|e_{1}\rangle + \alpha_{10}|e_{2}\rangle + \alpha_{11}|e_{3}\rangle\<br>&#x3D; [\alpha_{00}, \alpha_{01}, \alpha_{10}, \alpha_{11}] \left[\begin{array}{c} |00\rangle \ |01\rangle \|10\rangle \|11\rangle \\end{array}\right] \<br>&#x3D; \left[\begin{array}{c} \alpha_{00}\ \alpha_{01}\ \alpha_{10}\ \alpha_{11} \end{array}\right]<br>$$<br>从公式可以看到，词语$w$的量子态被映射到$n$维复向量空间中。</p><p>一句话通常是多个词语组成的，正如一个量子系统由多个微观粒子组成。一个量子系统在量子计算中通常用密度矩阵来表示。假设句子中含有$m$个词语，那么一个句子$S$的密度矩阵表示为<br>$$<br>\rho &#x3D; |S\rangle\langle S| &#x3D; \sum_{i&#x3D;1}^m\beta|w_i\rangle (\sum_{i&#x3D;1}^m\beta|w_i\rangle)^\dagger<br>$$<br>其中$\beta$是$n$维复向量，$|\beta_i|^2$表示词语$w_i$在句子$S$中的权重。类似于注意力机制，不同的权重有利于神经网络关注句子中重要的词语。在情感分类中，一些形容词如“棒”、“糟糕”、“优秀”等，对最终的预测结果有很大的影响，可以分配较大的权重。</p><p>句子被表示为密度矩阵之后，我们希望进一步学习句子内部词语的联系。对应于量子系统，这一操作叫做演化，即量子态随时间或是其它外界干扰而发生改变，通过公式表示为<br>$$<br>\rho^\prime&#x3D;U\rho<br>$$<br>其中，$U$是一个$n\times n$的复值矩阵，$\rho\prime$是经过演化后的系统状态。过去的量子启发式语言模型通常是抽取密度矩阵$\rho$中的实部和虚部，使用循环神经网络或者卷积神经网络分别进行训练，最后整合输出的特征。我们认为这一操作会割裂量子系统中的信息，会导致学习到的特征不完整，不能正确地模拟量子系统状态的改变。因此，在构造量子启发式模型时，我们通过复数值神经网络来模拟量子态的改变，整个演化过程将基于复数值进行计算，输出的结果也将保持复数状态。</p><p>最后，量子计算中的测量可以得到量子系统坍塌到一组基态下的概率值，这一过程应用到自然语言处理中的文本分类任务上。假设有一组测量算子${M_i},i\in{1,…,k}$，表示$k$种分类标签，那么句子对应第$i$个标签的测量概率为<br>$$<br>p_i &#x3D; \rho^\dagger M_i^\dagger M_i \rho<br>$$<br>。我们将模型应用到情感分析任务中，进行句子情感极性的验证，任务包含二分类和多分类，在第四章可以看到实验的细节。</p><p><strong>模型细节</strong></p><p>复数值量子启发式神经网络（ComplexQNN）是基于复数值神经网络和量子计算理论构建的语言模型，用于自然语言处理的下游任务，下面将从模型整体架构、模型具体模块的构造、模型适用的场景三个方面介绍该模型。</p><p>首先是ComplexQNN的模型整体架构，图片1描述了模型的整体架构，它包含三个模块：复数词嵌入、量子编码器、分类器。此外，我们还可以从 <strong>图片1</strong> 中了解到ComplexQNN处理文本的整个流程：</p><p>0）预处理过程：文本首先通过预处理过程（大小写转换、分词、词索引映射、填充和截断）得到令牌序列（token），另一方面，为了屏蔽掉填充序列带来了额外令牌序列，还需要构建由0和1组成的掩码序列（mask），令牌序列和掩码序列即为ComplexQNN的输入数据；</p><p>ComplexQNN的体系结构如图\ref{fig_ComplexQNN}所示。可以看到，它由四个模块组成:复杂嵌入，投影，演化和分类器。ComplexQNN的输入数据首先需要经过大小写转换、分词、词索引映射、填充和截断等预处理得到。此外，为了屏蔽填充序列带来的额外token序列，还需要构造由0和1组成的掩码序列。综上所述，token序列和mask序列是ComplexQNN的输入数据。下面介绍ComplexQNN的四个基本模块:</p><p>1）复数词嵌入：复数词嵌入层的作用是将词语对应的令牌号（这是一个整数对应词语在词表中的位置）映射到n维复数向量空间中，其对应于量子计算中的量子态构建过程，每一个词语从离散空间映射到高维希尔伯特空间下，对应一个复数值列向量。</p><p>2）投影：投影是把句子中多个离散的词语映射到$n\times n$的复数值空间中。上一步骤中，复数词嵌入已经把词语映射到$n$维复向量空间中。通过公式1，可以计算出句子的密度矩阵表示，其中词语的权重$\beta$可以通过注意力机制训练得到，默认所有的词语取相同的权重。</p><p>3）演化：演化过程是模拟量子系统的变化。在量子计算理论中，量子态和密度矩阵的变化是利用量子门实现的。量子门对应一个酉矩阵，矩阵的维度对应其操作的量子比特数（$n&#x3D;2^m$）。在ComplexQNN中，我们要通过复数值线性层、复数值循环神经网络以及复数值卷积神经网络来模拟量子系统的变化。我们在设计演化模块时，令输入和输出的维度都是$n\times n$。因此，在学习句子内部的特征后，不会改变原来量子系统的维度。</p><p>4）分类器：分类器的作用是利用之前模块学习到的高维特征预测分类结果。我们可以基于量子计算理论通过测量直接输出预测结果，如公式1所示。这里需要构造一组线性无关的测量基，基态的个数取决于要进行分类的数量。最终模型预测的结果取概率值最大对应测量基的标签。</p><p>以上，我们介绍了ComplexQNN的四个必要模块，并展示了文本序列从输入到预测输出的过程。接下来，将介绍模块的具体设计。</p><hr><p>ComplexQNN需要包含复数词嵌入、投影、演化以及分类器，其中复数词嵌入和演化是模型构建的核心。在我们的实现中，我们使用Allennlp库设计了三个模块：ComplexEmbedder，QuantumEncoder和Classifier。量子编码层中包含了投影和演化操作。</p><p>ComplexEmbedder是ComplexQNN的第一个模块，它的输入是被预处理后的文本Token序列，这是一个整数向量。复数词嵌入由实部嵌入层和虚部嵌入层组成。文本Token序列分别经过这两个嵌入层，最后通过公式<br>$$<br>[w_1, …, w_i, …, w_n] &#x3D; [r_1, …, r_i, …, r_n] + \bold{i} \times [i_1, …, i_i, …, i_n]<br>$$<br>得到每一个Token的复数词向量表示。实部嵌入层和虚部嵌入层可以方便地使用经典的词嵌入。</p><p>经典的词嵌入层有多种不同的嵌入方式。按照分词的级别，可分为字符级、wordpiece级以及词语级。按训练方式可分为非上下文词嵌入（静态词嵌入）和上下文词嵌入（动态词嵌入）。Word2vec 和 GloVe 是两种经典的非上下文词嵌入方法。上下文词嵌入是基于预训练模型思路，通过微调得到数据集相关的上下文词向量。</p><p>我们考虑了不同的方案来构造复数词嵌入层：（1）使用不同的预训练模型作为实部和虚部的嵌入层，如实部使用BERT，虚部使用RoBERTa，这样就可以将不同的信息编码到量子态中。（2）实部和虚部编码不同种类的文本信息，如实部编码wordpiece级别的词向量，虚部使用NLTK库编码词语极性等语义信息。（3）实部编码正向语序特征，虚部编码反向语序特征。</p><p>上面第一种方法同时使用了不同的预训练模型提取特征，可以取得最好的实验结果，但是训练需要占用很大的显存空间。第二种和第三种方法训练速度很快，但是效果不如预训练模型。综合考虑实验效果以及训练所需要的资源，最终ComplexEmbedder的实现采用实部使用RoBERTa，虚部采用自训练的词嵌入层。总的来说，我们希望实部和虚部嵌入不同类型的文本特征，充分利用复数值神经网络的异构特性，进而提升模型的语义表达能力。</p><p>ComplexQNN的第二个模块是量子编码层，负责投影和演化。前面我们提到过，进行编码前，需要使用投影先计算得到句子的密度矩阵表示，公式展示了计算密度矩阵的过程。演化是模拟量子门操作。这需要满足一些条件，即输入输出维度不变且维度是$2^n$。我们通过以下基本模块来构建我们的编码层：复数值全连接层、复数值循环神经网络和复数值卷积神经网络。我们构建了三种用于编码的中间模块层：复数值深度神经网络编码层、复数值循环神经网络编码层（基于ComplexLSTM）和复数值卷积神经网络编码层。</p><p>如图1所示，我们构造了ComplexTextCNN。该模块的输入为投影后的表示句子的密度矩阵$\rho$，我们使用了卷积核大小为$[3,4,5]$，每种卷积核数量为$2$，步长为$1$。然后通过最大池化提取特征，并将不同维度学习到的特征拼接起来。到目前为止，ComplexTextCNN和经典TextCNN最大的区别就是，所有的计算操作都是在复数值网络中进行计算的。最后，为了得到学习特征后的新的句子密度矩阵表示，我们使用复数值全连接网络层，把向量维度恢复成输入时的维度。通过外积操作，得到和输入一样维度的矩阵形式。</p><p>第三个模块是分类器，它利用量子计算理论的测量方法，预测模型的输出。具体是通过公式1来实现，我们可以设计不同数量的测量基态来进行文本多分类。后面实验中涉及了多分类，需要根据分类结果的种类数确定基向量的个数。最终的分类结果是取预测概率最大的那个作为最终的预测结果。此外，因为测量后的输出是实数值，我们可以取出每一个标签的概率值组成一组预测向量，然后和其它模型的预测结果拼接在一起，实现模型融合。</p><p>ComplexQNN 用于情感分类</p><p>情感分类是自然语言处理中的一项常见任务，旨在预测句子对应的情感极性。本文开展这项任务来验证ComplexQNN的实验性能。同时，对一些经典的网络模型进行比较。图\ref{fig_ComplexQNNforSA}是ComplexQNN用于情感分类任务的流程图，并对数据维度进行了标注。首先对文本进行预处理，将大小写归一化，分词并去除停用词;其次，通过复杂的词嵌入层模拟单词的量子态;然后，量子编码器层用于投影和演化。最后，通过分类器的模拟测量操作输出预测结果。</p><p><strong>评估指标</strong></p><p>在我们的情感分析实验中，五个数据集都是有两种分类结果。因此，我们使用准确率、精度、召回率和F1值作为评估指标去分析实验结果。</p><p>$$<br>Accuracy &#x3D; \frac{TP + TN}{TP + TN + FP + FN}<br>\<br>Precision &#x3D; \frac{TP}{TP + FP}<br>\<br>Recall &#x3D; \frac{TP}{TP + FN}<br>\<br>F1-score &#x3D; 2 \times \frac{Precision \times Recall}{Precision + Recall}<br>$$</p><p>对于多分类任务<br>$$<br>micro-F1 &#x3D; 2 \times \frac{Recall_m \times Precision_m}{Recall_m + Precision_m}<br>\<br>(3分类为例)<br>macro-F1 &#x3D; \frac{F1-score_1 + F1-score_2 + F1-score_3}{3}<br>$$</p><p>macro-F1</p><p><strong>损失函数</strong></p><p><strong>参数设置</strong></p><hr><h3 id="第四章-实验"><a href="#第四章-实验" class="headerlink" title="第四章 实验"></a>第四章 实验</h3><p>在这一章，我们将进行情感分类实验来验证我们提出的方法。我们会介绍用于对比的模型，展示我们的实验结果和进行结果分析，并相比于经典神经网络对ComplexQNN的优势进行讨论。</p><p>我们使用以下5个模型作为我们实验的对比模型：</p><p>情感分析是自然语言处理中的常见任务，任务要求预测给定的句子的情感极性。实验使用了五个常见的数据级如<strong>表（）</strong>所示。为了衡量ComplexQNN模型的性能，我们使用TextCNN、TextGRU、BERT、RoBERTa作为对比模型。具体地实验结果如<strong>表（）</strong>所示。</p><p>对比模型</p><p><strong>GRU</strong></p><p><strong>TextCNN</strong></p><p><strong>ELMo</strong>（Embeddings from Language Models）由双向LSTM作为基本组件构成，以语言模型（Language Model）为训练目标，通过大语料进行预训练，得到通用的语义表示，再迁移到下游的NLP任务中，可以显著提升下游任务的模型性能。ELMo提供了词级别的语义表示，在很多下游任务中表现优异。</p><p><strong>BERT</strong>（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型（pre-trained language model, PLM）。像ELMo和GPT都是自回归模型（Auto Regressive，AR），只能考虑单侧信息，也就是根据上文预测下一个单词，或者根据下文预测前一个单词。而BERT是利用上下文信息，从含噪音的数据中重建原始数据，属于自编码模型（Auto Encoding）。预训练过程中使用了两个任务：掩码语言模型（Masked Language Model）和下一句预测（Next Sentence Prediction）。BERT的输出是句子中每个Token对应的768维的向量，以及一个特殊Token（[CLS]）。</p><p><strong>RoBERTa</strong>（Robustly Optimized BERT Pretraining Approach）是BERT的精细调优版本，使用了更大的模型参数、更大的批处理大小、更多的预训练数据，同时改进了训练方法，去掉了下一句预测任务，使用动态掩码和BPE编码（Byte-Pair Encoding），在实验上取得了比BERT更好的效果。</p><p>实验结果分析：可以看到ComplexQNN相比于流行的预训练模型并不落于下风，在XX数据集上都有更好的预测精度。另外，我们使用了不同的预训练模型以及相同模型的不同尺寸构建ComplexQNN（GloVe, BERT, RoBERTa；BERT-tiny, BERT-base, BERT-large），实验结果如<strong>图（）</strong>所示。</p><blockquote><p>As shown in Tab , Model is superior to most quantum-inspired models and LSTM&#x2F;CNN-based models and achieves three best performances out of the four metrics on Wiki-QA and TREC-QA. However, it under-performed slightly worse than XX on XX metric about XX. The possible reason is that although Model has surpassed most models, models have larger parameter scales to ensure the effectiveness of learning.</p></blockquote><p>消融实验：</p><p>为了验证量子启发式模型以及复数值神经网络的有效性，我们构建了消融实验。我们构建了四个模型：</p><p>（1）经典卷积神经网络（TextCNN）；</p><p>（2）量子启发式模型（QNN）；</p><p>（3）复数值神经网络模型（ComplexNN）；</p><p>（4）复数值量子启发式模型（ComplexQNN）。</p><p><strong>表（）</strong>显示，复数值神经网络和量子启发式模型通常具有和TextCNN近似的实验结果；ComplexQNN具有比以上三个模型更好的性能。</p><table><thead><tr><th>model</th><th>SST-2</th><th>SST-5</th></tr></thead><tbody><tr><td>TextCNN</td><td>80.9</td><td>78.8</td></tr><tr><td>QNN</td><td>78.8</td><td>75.4</td></tr><tr><td>ComplexTextCNN</td><td>75.3</td><td>71.7</td></tr><tr><td>ComplexQNN</td><td>88.4</td><td>83.1</td></tr></tbody></table><p>实验结果（2022.11.28-…）</p><table><thead><tr><th align="left">model</th><th align="right">CR</th><th align="right">MPQA</th><th align="right">MR</th><th align="right">SST</th><th align="right">SUBJ</th></tr></thead><tbody><tr><td align="left">TextCNN</td><td align="right">acc: <strong>78.8</strong><br> prec: 70.2 <br>recall: 72.3<br> f1: 71.2<br>loss: 0.622</td><td align="right">acc: <strong>74.4</strong><br> prec: 72.4<br>recall: 78.8<br> f1: 75.5<br> loss: 0.638</td><td align="right">acc: <strong>75.0</strong><br> prec: 73.2<br> recall: 78.8<br> f1: 75.9<br> loss: 0.527</td><td align="right">acc: <strong>81.4</strong><br> prec: 81.5<br> recall: 80.3<br> f1: 80.9<br> loss: 0.493</td><td align="right">acc: <strong>90.3</strong><br> prec: 92.0<br> recall: 88.3<br> f1: 90.1<br> loss: 0.379</td></tr><tr><td align="left">ComplexTextCNN</td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td align="left">GRU</td><td align="right">acc: <strong>80.1</strong><br> prec: 75.7 <br>recall: 66.7<br> f1: 71.0<br>loss: 0.637</td><td align="right">acc: <strong>84.3</strong><br> prec: 78.3<br>recall: 69.9<br> f1: 73.3<br> loss: 0.424</td><td align="right">acc: <strong>76.0</strong><br> prec: 77.0<br>recall: 74.1<br> f1: 75.5<br> loss: 0.537</td><td align="right">acc: <strong>82.5</strong><br> prec: 81.6<br> recall: 83.1<br> f1: 82.3<br> loss: 0.512</td><td align="right">acc: <strong>91.7</strong><br> prec: 90.7<br> recall: 92.3<br> f1: 91.8<br> loss: 0.381</td></tr><tr><td align="left">ComplexGRU</td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td align="left">ComplexQNN</td><td align="right">acc: <strong>78.2</strong><br> prec: 66.3 <br>recall: 81.0<br> f1: 72.9<br>loss: 0.913</td><td align="right">acc: <strong>84.4</strong><br> prec: 76.8<br>recall: 71.7<br> f1: 74.2<br> loss: 0.580</td><td align="right">acc: <strong>73.6</strong><br> prec: 70.2<br>recall: 82.0<br> f1: 75.6<br> loss: 0.832</td><td align="right">acc: <strong>80.4</strong><br> prec: 80.4<br> recall: 79.4<br> f1: 79.9<br> loss: 0.623</td><td align="right">acc: <strong>90.1</strong><br> prec: 89.6<br> recall: 90.8<br> f1: 90.2<br> loss: 0.406</td></tr><tr><td align="left">ELMo</td><td align="right">acc: <strong>85.4</strong><br> prec: 80.0<br> recall: 79.8<br> f1: 79.9<br> loss: 0.381</td><td align="right">acc: <strong>90.5</strong><br> prec: 88.2<br> recall: 80.4<br> f1: 84.1<br> loss: 0.250</td><td align="right">acc: <strong>81.0</strong><br> prec: 77.0<br> recall: 88.3<br> f1: 82.3<br> loss: 0.415</td><td align="right">acc: <strong>88.8</strong><br> prec: 89.3<br> recall: 87.6<br> f1: 88.4<br> loss: 0.296</td><td align="right">acc: <strong>94.9</strong><br> prec: 95.6<br> recall: 94.1<br> f1: 94.9<br> loss: 0.140</td></tr><tr><td align="left">BERT</td><td align="right">acc: <strong>88.8</strong><br> prec: 81.8<br> recall: 88.0<br> f1: 85.2<br> loss: 0.289</td><td align="right">acc: <strong>89.5</strong><br> prec: 87.3<br> recall: 77.9<br> f1: 82.3<br> loss: 0.362</td><td align="right">acc: <strong>84.9</strong><br> prec: 86.0<br> recall: 83.3<br> f1: 84.6<br> loss: 0.360</td><td align="right">acc: <strong>88.9</strong><br> prec: 93.0<br> recall: 83.6<br> f1: 88.0<br> loss: 0.298</td><td align="right">acc: <strong>95.2</strong><br> prec: 95.5<br> recall: 94.9<br> f1: 95.2<br> loss: 0.142</td></tr><tr><td align="left">RoBERTa</td><td align="right">acc: <strong>90.4</strong><br> prec: 92.7<br> recall: 79.8<br> f1: 85.8<br> loss: 0.265</td><td align="right">acc: <strong>90.9</strong><br> prec: 85.0<br> recall: 86.1<br> f1: 85.6<br> loss: 0.257</td><td align="right">acc: <strong>89.8</strong><br> prec: 89.1<br> recall: 90.7<br> f1: 89.9<br> loss: 0.304</td><td align="right">acc: <strong>89.8</strong><br> prec: 88.8<br> recall: 90.7<br> f1: 89.7<br> loss: 0.238</td><td align="right">acc: <strong>96.7</strong><br> prec: 97.6<br> recall: 95.8<br> f1: 96.7<br> loss: 0.098</td></tr><tr><td align="left">Complex-QDNN(with <code>bert-tiny</code>)</td><td align="right">acc: <strong>83.7</strong><br> prec: 81.0<br> recall: 71.8<br> f1: 76.2<br> loss: 0.607</td><td align="right"></td><td align="right">acc: <strong>77.4</strong><br> prec: 79.9<br> recall: 73.3<br> f1: 76.4<br> loss: 0.849</td><td align="right">acc: <strong>77.2</strong><br> prec: 71.6<br> recall: 88.6<br> f1: 79.2<br> loss: 0.613</td><td align="right"></td></tr></tbody></table><p><strong>消融实验</strong></p><p>消融实验包括ComplexNN vs NN, ComplexNN vs ComplexQNN</p><p>通过复数值神经网络对比TextCNN、BiLSTM，验证复数值神经网络的有效性；</p><p>通过复数值神经网络对比复数值量子启发式神经网络，验证复数值量子启发式神经网络的有效性。</p><hr><p><strong>讨论</strong></p><p>% 关于ComplexQNN和RoBERTa的对比<br>% 对比主要从当前优势以及未来可能性</p><p>ChatGPT的出现给人们带来极大的震撼，同时它让很多人意识到，基于经典神经网络的自然语言处理技术已经达到了很高的水平。RoBERTa是一个很优秀的模型，在很多的数据集上它的表现都已经达到了人类水平。  因此我们并不意外我们提出的模型ComplexQNN在多个数据集上的结果与RoBERTa接近。下面我们讨论相比于流行的经典模型如RoBERTa，ComplexQNN具有的一些优势，我们总结出以下三点。</p><p>首先来看一下ComplexQNN和RoBERTa的区别。RoBERTa是在BERT模型的基础上进行改进，是多层Transformer结构，能够在实数值空间中进行特征提取。ComplexQNN是定义在希尔伯特空间中，词向量会被表示成复值，在表示空间上要大于实数值空间。实部可以表示上下文的语义信息，虚部可以用来表示语义之外的信息，比如句子中词语的位置信息、词语的情感信息和词语的歧义信息。 与实数空间相比，复数值空间给予深度学习算法更多的表示可能性，有利于拓展模型发展的边界。</p><p>其次经典模型大多是黑盒模型。现有的自然语言模型，将文本映射到向量后，再通过多层神经网络结构，中间向量的含义只能用文本的低维特征和高维特征描述。而基于量子计算的量子启发式模型，是把自然语言看作是一个量子系统：词语被表示为量子态，句子被表示为密度矩阵，句子中词语的相互作用被表示为量子态演化，句子对应的标签表示为量子态测量后坍塌到基态。这给模型带来了物理意义，有利于人们对模型的理解。自然语言的一些特性可以用量子现象解释，如词语的一词多义现象可以很好地用量子纠缠现象表示，这在一定程度上增加了模型的可解释性。</p><p>最后是计算复杂度的问题。从目前来看，要实现量子启发式复数值网络需要两倍的资源(实部和虚部)，但这是因为在经典计算机中模拟量子操作的原因。因为n个量子比特需要用$2^n$个经典比特来模拟(考虑复数值时，需要$2^{n+1}$)，$n$比特量子门需要$2^n\times 2^n$个经典比特来模拟。ComplexQNN设计的神经网络层是基于复数值神经网络，很容易迁移到未来的量子计算机中。而当我们的算法运行在真实的量子计算机中时，花费的存储和计算资源将会指数级减少。现有的量子计算机已经超过100多个量子比特，能处理小规模的自然语言处理数据集上的分类任务，我们期待未来在真实的量子计算机中实现我们提出的算法。</p><p>总言之，相比于经典神经网络模型如RoBERTa，ComplexQNN有更强的表示能力，更好的可解释性，复杂度指数级降低的可能性。</p><p>% [实验结果只是ComplexQNN的一部分，未来和可能性才是我们算法的优势。]</p><h3 id="第五章-结论"><a href="#第五章-结论" class="headerlink" title="第五章 结论"></a>第五章 结论</h3><p>本文中，我们基于复数值神经网络提出了一种新的量子启发式模型ComplexQNN，并在情感分类数据集上验证了模型的有效性。同时，通过消融实验，我们证明复数值量子启发式模型在语言建模上，具有比以往的量子启发式模型和复数值神经网络更加杰出的表现。</p><p>未来的研究方向可以考虑两个方向：第一是编码更深层的语义，比如开发出适合于数据集规模更大的复数值Tranformer网络模块，并应用在更加复杂的场景下，比如机器翻译，推荐系统等；第二是使用量子线路模型构建网络模块，虽然这在NISQ时代能处理的数据集是受限的。</p><hr><p>基金</p><p>This research was funded by the National Natural Science Foundation of China (Grant Nos.61872390, 61972418, 62272483) and the Special Foundation for Distinguished Young Scientists of Changsha (Grant Nos.kq1905058).</p><p>National Natural Science Foundation of China Nos.61872390, Nos.61972418, Nos.62272483</p><p>Special Foundation for Distinguished Young Scientists of Changsha Nos.kq1905058</p><p>This research was funded by the National Natural Science Foundation of China (Grant Nos.61872390, 61972418, 62272483) and the Special Foundation for Distinguished Young Scientists of Changsha (Grant Nos.kq1905058)</p><p>This work was supported by the National Natural Science Foundation of China(Grant Nos.61972418,61872390,61801522), the Natural Science Foundation of Hunan Province(Grant Nos. 2020JJ4750,2019JJ40352), the Special Foundation for Distinguished Young Scientists of Changsha(Grant Nos.kq1905058) and CCF-Baidu Open Fund (NO.2021031).</p><blockquote><p>Conceptualization, Wei Lai, Jinjing Shi and Yan Chang; Methodology, Wei Lai, Jinjing Shi and Yan Chang; Software, Wei Lai and Yan Chang; Validation, Wei Lai, Jinjing Shi and Yan Chang; Formal analysis, Wei Lai, Jinjing Shi and Yan Chang; Investigation, Wei Lai, Jinjing Shi and Yan Chang; Resources, Wei Lai and Jinjing Shi; Data curation, Wei Lai, Jinjing Shi and Yan Chang; Writing – original draft, Wei Lai; Writing – review &amp; editing, Wei Lai and Jinjing Shi; Visualization, Wei Lai and Yan Chang; Supervision, Jinjing Shi and Yan Chang; Project administration, Wei Lai and Yan Chang; Funding acquisition, Jinjing Shi and Yan Chang.</p></blockquote><hr><p>Cover Letter</p><blockquote><p>本文在量子理论的激励下，提出了一种量子预训练特征嵌入方法(QPFE)，设计了两种高效的量子深度神经网络(QPFE- ernie)用于情感分类和词义消歧。在情感分类和词义消歧方面，本文提出的模型对模糊词的叠加状态进行建模，并合并从ERNIE中学习到的语义特征，在情感分类和词义消歧方面都优于之前的量子模型和经典方法BiLSTM和TextCNN。我们相信，本文的上述介绍将使MDPI公理的普通读者感兴趣。</p></blockquote><p>本文基于复数值神经网络构造了一种全新的，更加符合量子计算理论的量子启发式模型，即ComplexQNN。ComplexQNN没有使用振幅相位编码，而是完全基于复数值神经网络构建复数值嵌入层，量子编码层以及测量层。本文在情感分类任务上进行了实验，包含文本二分类和多分类，并且对比于经典的TextCNN，BiLSTM-CRF，以及ELMo、BERT、RoBERTa都有竞争性的优势。</p><hr><p>一些细节：</p><ol><li>量子启发式语言模型（预训练+微调）：基于复数值神经网络 &amp; 预训练语言模型</li><li>方法：Complex encode<ol><li>ComplexPyTorch</li><li>ComplexLinear</li><li>ComplexConv2d</li><li>ComplexBatchNorm</li></ol></li><li>评估：<ol><li>sentiment classification</li><li>GLUE + CLUE</li></ol></li></ol><p>模型构建</p><p>embedder: [batch_sz, seq_len] -&gt; [batch_sz, seq_len, embedding_dim]</p><p>encoder: [batch_sz, seq_len, embedding_dim]</p><ul><li>complexcnn: </li><li>qnn</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处可以考虑给tokens随机加上一个虚部，或者虚部使用另外一种embedding!!!</span></span><br><span class="line"><span class="comment"># TODO</span></span><br><span class="line"><span class="comment"># embedding_type: random, GloVe or Word2vec, contextual embedding</span></span><br><span class="line"><span class="comment"># 思路：a + 1j * b</span></span><br><span class="line"><span class="comment"># tokens = ptm_tokens + 1j * ptm_tokens</span></span><br><span class="line"><span class="comment"># 实现：通过real_embedder将tokens -&gt; real_part, 通过imag_encoder将tokens -&gt; imag_part</span></span><br><span class="line"><span class="comment"># final_encoder = real_part + 1j * imag_part</span></span><br><span class="line"><span class="comment"># 注：这是一种简单的实现方式，另外还可以构造complex-valued embedding，但前者更符合模型构建的意义</span></span><br><span class="line"><span class="comment"># 意义：复数词向量空间/希尔伯特空间能够表示更加复杂的意义（相比于经典实数空间），提高模型的异构性，尝试使用实部、虚部分别编码不同的信息，一方面不同信息可以相互纠缠，另一方面模型表达能力得到提升</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># encoder: complex_tokens -&gt; vector</span></span><br><span class="line"><span class="comment"># Process:</span></span><br><span class="line"><span class="comment"># token: [batch_size, seq_len]</span></span><br><span class="line"><span class="comment"># complex_embedder: ptm_tokens + 1j * ptm_tokens</span></span><br><span class="line"><span class="comment"># token: [batch_size, seq_len, emb_dim]</span></span><br><span class="line"><span class="comment"># 分两种方向：其一量子启发式模型，其二复数值神经网络</span></span><br><span class="line"><span class="comment"># 其一：量子启发式模型</span></span><br><span class="line"><span class="comment"># 前面token对应词语的量子态表示，通过外积、求和（加权），可以得到句子的密度矩阵表示</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, emb_dim, emb_dim]</span></span><br><span class="line"><span class="comment"># 演化：学习文本内部特征，不改变维度 [emb_dim, emb_dim]；gru, cnn, fc, transformer</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, emb_dim, emb_dim]</span></span><br><span class="line"><span class="comment"># 测量：tr(PV), 其中P是测量算子对应的密度矩阵</span></span><br><span class="line"><span class="comment"># prediction: [batch_size, num_labels]</span></span><br><span class="line"><span class="comment"># 其二：复数值神经网络</span></span><br><span class="line"><span class="comment"># encoder: [seq2seq], seq2vec; 具体地，complexcnn, complexgru, complexdnn</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, hidden_size]</span></span><br><span class="line"><span class="comment"># classifer: nn.Linear()</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, num_labels]</span></span><br></pre></td></tr></table></figure><p><strong>Ｑ　＆　Ａ</strong></p><p>Q: datasets processing</p><p>A: Allennlp pipeline.</p><p>Q: CR, MPQA, MR, SUBJ</p><p>A: 先读取所有数据，然后打乱，最后按7:3分割训练集和测试集</p><p>Q: version of BERT, RoBERTa</p><p>A: RoBERTa: <code>roberta-base</code>, BERT: <code>bert-base-cased</code></p><p>Q: RoBERTa 训练出现精度无法提升的问题</p><p>A: 学习率不能调的太高，适合lr&#x3D;1e-5</p><p>Q: ComplexNN 效果不好，解决方法</p><p>A: 解决中…思路：ComplexEmbedding, ComplexCNN, ComplexGRU, ComplexTransformer</p><p><strong>TODO</strong></p><p><strong>表：情感分类数据集描述，对比实验表（总），消融实验表</strong></p><p><strong>图：对比实验结果图</strong></p><p><strong>ComplexQNN</strong></p><p>量子启发式模型：Embedder, Encoder, Classifier</p><ul><li><input checked disabled type="checkbox"> ComplexQNN整体模型<ul><li><input disabled type="checkbox"> 画一张整体模型图</li><li><input checked disabled type="checkbox"> 预处理过程：分词器、实例化、批处理</li><li><input disabled type="checkbox"> Embedder:  BagOfWord, SingleId</li><li><input disabled type="checkbox"> Encoder: </li><li><input disabled type="checkbox"> Classifer: nn.Linear</li></ul></li><li><input checked disabled type="checkbox"> ComplexQNN的每一个模块介绍<ul><li><input disabled type="checkbox"> ComplexEmbedder: real_embedder &amp; imag_embedder</li><li><input disabled type="checkbox"> QuantumEncoder: Projection &amp; Evolution</li><li><input disabled type="checkbox"> Measure: tr(PV)</li></ul></li><li><input checked disabled type="checkbox"> 训练细节<ul><li><input checked disabled type="checkbox"> 硬件信息：2080ti GPU 12G</li><li><input checked disabled type="checkbox"> 环境：py3.8, torch&#x3D;1.12.1, allennlp&#x3D;2.10.1</li><li><input checked disabled type="checkbox"> epoch: [3 - 30]</li><li><input checked disabled type="checkbox"> batch: [16, 32, 64]</li><li><input checked disabled type="checkbox"> optimizer：Adam, Adamw</li><li><input checked disabled type="checkbox"> loss: cross_entropy</li><li><input checked disabled type="checkbox"> metric: accuracy, precision,</li></ul></li></ul><h2 id="English-Version"><a href="#English-Version" class="headerlink" title="English Version"></a>English Version</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h3><p>(最多200words：背景，方法，结果，结论)</p><p>Existing methods of quantum heuristic model are all based on amplitude phase embedding to map words into Hilbert space, while in quantum computing theory, the vectors corresponding to quantum states are complex numerical values, so Euler’s formula is needed for transformation. Complex numerical neural networks have been studied, but their practical applications are few, let alone in the downstream tasks of natural language such as sentiment analysis and language modeling. In fact, the representation capability of complex numerical neural networks is better than that of real numerical neural networks, which is suitable for modeling complex natural languages. On the other hand, quantum heuristic models are defined under Hilbert space, which is also a complex space, so it is natural to construct quantum heuristic models based on complex numerical neural networks. Therefore, we propose a new model ComplexQNN, which can embed heterogeneous semantic information in the real and imaginary parts to improve the representation ability of the model. We conduct experiments on emotion classification data sets, which are compared to the baseline model, and the experimental results show that ComplexQNN is very good at accomplishing the task, and has xx% accuracy improvement compared to xx model.</p><p>key words:（三到十个）</p><p>sentiment analysis; machine learning; natural language processing</p><p>complex neural networks, sentiment analysis, language model, quantum theory.</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>A quick background, question thrown out: NLP has benefited from the development of deep learning and has made significant progress in many areas. The main technology currently in use is a pre-training language model built on transformer that ADAPTS to different downstream tasks through fine-tuning strategies. The research direction began to develop towards larger data sets and larger models. Although the computing power was improved year by year, it still felt constrained. Are there any new computational models that can be modeled more reasonably, save resources and learn more knowledge at the same time?</p><p>Existing solutions and shortcomings: Quantum computing is a brand new computing theory. It has been proved that in some tasks, quantum computers have exponential computational complexity acceleration advantages. There is also currently some work on speech quantum computing theory for modeling natural language, such as the lambeq work at Cambridge. Limited by the NISQ era, the size of the text processed is small, and can only handle data set tasks of 100 sentence size containing 10 or more words.</p><p>Based on what revelation, I came up with my plan, what work I did: trying to benefit from the theory of quantum computing and another way of thinking about it, building quantum heuristic models. The quantum heuristic model is based on the mathematical framework of quantum computing theory to model the natural language, comparing the natural language to the quantum system, and using the classical neural network model to simulate the process. At present, there are some quantum heuristic models such as NNQLM, ICWE, etc., which simulate the construction of quantum states by amplitude phase coding. In fact, quantum states are defined in Hilbert Spaces, where each dimension is a complex value. Therefore, in order to construct a more reasonable quantum heuristic model and enrich the expression ability of the model, we propose complex numerical quantum Heuristic model (ComplexQNN). We have carried out extensive experiments on the proposed model, using multiple sentiment classification data sets to evaluate the model, and compared it with the classical model, which proves the effectiveness of our model.</p><p>My method can be summarized into three points: In summary, we summarize the contents of this work, including the following: Based on complex numerical neural network, we construct a new quantum heuristic model which is more consistent with the theory of quantum computing, namely ComplexQNN. The above model is a theoretical framework. In the actual modeling, the model structure can be adjusted adaptively according to the real scene, which will be elaborated in the third chapter. The experimental results in emotion classification prove that our model has a good effect.</p><ul><li><input disabled type="checkbox"> 加内容</li></ul><hr><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>**Preliminary about Quantum Computation **</p><p>Quantum theory. Quantum state, quantum evolution and quantum measurement.</p><p><strong>Preliminary</strong><br>In quantum physics \cite{dirac1928quantum}, unlike classical physics, the quantum is the smallest unit that cannot be divided, which is the first property of quantum. For instance, the energy of an electron in an atom only can be one of several values (known as energy levels). The second property of quantum is transition that a quantum system can change its state. For example, electrons in an atom can jump to different energy levels when they gain or lose energy. The third property is superposition. A particle can be in a superposition state of upspin and downspin. Apart that, quantum states change over time, which calls quantum evolution. A superposition can be measured by a measure operator, after which the superposition state collapses and loses superposition property.</p><p>With the mathematical framework of quantum theory, quantum states can be represented by vectors with Dirac notation \cite{dirac1939new}, marking quantum states with $|$ and $\rangle$. Vectors describing quantum states are called as state vectors, which can be divided into ket and bra. Ket is column vector and bra is row vector. For a quantum state, its ket and bra are transpose conjugate of each other. Any quantum state in a quantum system can be represented by a linear combination of one basis. The quantum state evolution of a closed quantum system can be described by unitary transformation. Related details about quantum theory are described in the rest of this section.</p><p><strong>Quantum state</strong><br>A quantum state \cite{bennett2000quantum} defined in a Hilbert Space is noted as a ket ($|\psi\rangle$) and its transpose can be noted as a bra ($\langle \psi|$). A quantum bit (qubit) different from classical bit can be in superposition state of $|0\rangle$ and $|1\rangle$. The superposition state can be noted as $|\phi\rangle &#x3D; \alpha |0\rangle + \beta |1\rangle$, where both $\alpha$ and $\beta$ are complex numbers, as well as $\alpha, \beta$ satisfying $|\alpha |^2 + |\beta |^2 &#x3D; 1$. In addition, $|\alpha|^2$ ($|\beta|^2$) represents the probability of state $|0\rangle$ ($|1\rangle$).</p><p>In quantum mechanism, the density matrix of single particle represented as a pure state $\rho$ can be described as $\rho &#x3D; |\psi\rangle \langle \psi|$, where $\rho$ satisfies $\rho &#x3D; \rho^2$. For multiple particles, the total system is a mixed state that can not be represented by a superposition state, which only can be described by a density matrix $\rho &#x3D; \sum_i w_i |\psi_i \rangle \langle \psi_i |$. Similarly, a word consists of different basic senses, which can be viewed as a superposition state. For example, $|apple\rangle &#x3D; \alpha |fruit\rangle + \beta |company\rangle$. A sentence can be viewed as a mixed system for its several words in superposition.</p><p><strong>Evolution</strong><br>The quantum system always changes its state over time, which is called as evolution \cite{bennett2000quantum}. The evolution process can be mathematically described as<br>$$<br>    |\psi_2 \rangle &#x3D; U | \psi_1 \rangle,<br>$$<br>where $U$ is a unitary matrix satisfying $U^\dagger U&#x3D;I$.</p><p>A word or phrase changes its meaning based on historical reasons, which is similar with quantum state evolution. For instance, “silly” stems from Old English “saelig” representing happy and careless, while it represents fool nowadays. We can learn that it changes from context information or big corpus, which can be accomplished by RNN like long and short-term memory (LSTM) \cite{hochreiter1997lstm} and gated recurrent unit (GRU) \cite{chung2014empirical} as well as pretraining model such as BERT and ERNIE.</p><p><strong>Quantum Measurement</strong></p><p>Unlike evolution, measurement \cite{peres2002quantum} representing a non-unitary operation is an irreversible process. Quantum measurement are described with a group of measure operators ${M_m}$, satisfying $\sum_m M^\dagger_m M_m &#x3D; I$. Suppose quantum system state is $|\psi \rangle$ before measure operation, and the probability of measured result $m$ is shown as follows,<br>$$<br>p(m) &#x3D; \langle \psi | M^\dagger_m M_m | \psi \rangle.<br>$$</p><p>After measurement, the system state collapses into<br>$$<br>|\psi ^\prime \rangle &#x3D; \frac{M_m | \psi \rangle} {\sqrt{ \langle \psi | M^\dagger_m M_m | \psi \rangle }},<br>$$</p><p>and the sum of all the measured probabilities is 1:<br>$$<br>\sum_m p(m) &#x3D; \sum_m \langle \psi | M_m^\dagger M_m | \psi \rangle &#x3D; 1.<br>$$</p><p><strong>Complex Neural Network</strong></p><blockquote><p>% Complex nn basis: representation of complex numbers, complex convolution, complex differentiability, complex-valued activations, complex batch normalization, complex weight initialization, complex convolutional residual network.</p></blockquote><p><strong>representation of complex numbers</strong><br>We start by outlining the way in which complex numbers are represented in our framework. A complex number $z &#x3D; a + ib$ has a real component $a$ and an imaginary component $b$. We represent the real part $a$ and the imaginary part $b$ of a complex number as logically distinct real valued entities and simulate complex arithmetic using real-valued arithmetic internally. Consider a typical real-valued $2D$ convolution layer that has $N$ feature maps such that $N$ is divisible by 2; to represent these as complex numbers, we allocate the first $N &#x2F; 2$ feature maps to represent the real components and the remaining $N &#x2F; 2$ to represent the imaginary ones. Thus, for a four dimensional weight tensor $W$ that links $N_{in}$ input feature maps to $N_{out}$ feature maps and whose kernel size is $m \times m$ we would have  a weight tensor of size $(N_{out} \times N_{in} \times m \times m) &#x2F; 2$ complex weights.</p><p><strong>complex convolution</strong></p><p><strong>convolutional LSTM</strong><br>A Convolutional LSTM is similar to a fully connected LSTM. The only difference is that, instead of using matrix multiplications to perform computation, we use convolutional operations. The computation in a real-valued Convolutional LSTM is defined as follows:<br>$$<br>    i_t &#x3D; \sigma(W_{xi} * x_t + W_{hi} * W_{t-1} + b_i) \<br>    f_t &#x3D; \sigma(W_{xf} * x_t + W_{hf} * h_{t-1} + b_f) \<br>    c_t &#x3D; f_t \circ c_{t-1} + i_t \circ \tanh(W_{xc} * x_t + W_{hc} * h_{t-1} + b_c) \<br>    o_t &#x3D; \sigma(W_{xo} * x_t + W_{ho} * h_{t-1} + b_o) \<br>    t_t &#x3D; o_t \circ \tanh(c_t)<br>$$</p><p>Where $\sigma$ denotes the sigmoidal activation function, $\circ$ the elementwise multiplication and $*$ the real-valued convolution. $i_t, f_t, o_t$ represent the vector notation of the input, forget and output gates respectively. $c_t$ and $h_t$ represent the vector notation of the cell and hidden states respectively. the gates and states in a ConvLSTM are tensors whose last two dimensions are spatial dimensions. For each of the gates, $W_{xgate}$  and $W_{hgate}$ are respectively the input and hidden kernels.</p><p>For the Complex Convolutional LSTM, we just replace the real-valued convolutional operation by its complex countpart. We maintain the real-valued elementwise multiplication. The sigmoid and tanh are both performed separately on the real and the imaginary parts.</p><ul><li><p>ComplexGRU</p></li><li><p>complex differentiability</p></li><li><p>complex-valued activations</p></li><li><p>complex batch normalization</p></li><li><p>complex weight initialization</p></li><li><p>complex convolutional residual network</p></li></ul><p>Quantum-inspired Model</p><ul><li><input disabled type="checkbox"> 需要重写</li><li><input disabled type="checkbox"> </li></ul><hr><h3 id="ComplexQNN"><a href="#ComplexQNN" class="headerlink" title="ComplexQNN"></a>ComplexQNN</h3><p>In quantum language models, the Hilbert space is the mathematical foundation of physical events studied. Based on this background, our proposed models are constructed.</p><p>Since a quantum state is usually complex-valued, we therefore introduce the Semantic Hilbert Space $\mathbb{H}^n$ on a complex vector space $\mathbb{C}^n$, spanned by a set of orthogonal bases ${|e_j\rangle}_{j&#x3D;1}^n$. </p><p>Specifically, $ |e_j\rangle$ represents a sememe which is the minimum semantic unit of word meanings in language universals, and is an one-hot vector with only the j-th element in $|e_j\rangle$ being one while all the other elements being zero. A word $w$ is viewed as a physical state in such semantic Hilbert space, and hence can be represented as a superposition of sememes, written as follows:<br>$$<br>|w\rangle &#x3D; \sum_{j&#x3D;1}^n (w_{rj} + i w_{mj}) |e_j \rangle &#x3D; |w_r \rangle + i |w_i\rangle,<br>$$<br>where $|w_r\rangle &#x3D; \sum_{j&#x3D;1}^n w_{rj} |e_j\rangle$ and $|w_i\rangle &#x3D; \sum_{j&#x3D;1}^n w_{mj} |e_j\rangle$ are the real part and imaginary part of the state $|w\rangle$, respectively. And ${ w_{rj} }<em>{j&#x3D;1}^n$, ${w</em>{mj}}_{j&#x3D;1}^n$ are the real part and imaginary part of probability amplitudes along sememes respectively.</p><p>Complex numerical quantum Heuristic neural network (ComplexQNN) is a language model based on complex numerical neural network and quantum computing theory, which is used for downstream tasks of natural language processing. This model is introduced from three aspects: the overall architecture of the model, the construction of specific modules of the model, and the scenarios applicable to the model.</p><p>The first is the overall architecture of the ComplexQNN model, which is depicted in Figure 1 and consists of three modules: complex word embedding, quantum encoder, and classifier. In addition, you can see the ComplexQNN text processing process from Figure 1)Text preprocessing: The text first obtains token sequence through preprocessing (case conversion, word segmentation, word index mapping, filling and truncation). On the other hand, in order to mask the additional token sequence brought by filling sequence, a mask sequence composed of 0 and 1 is also needed to be constructed. The token sequence and mask sequence are the input data of ComplexQNN; 2) Complex word embedding: The function of complex word embedding layer is to map the token number (integer) corresponding to the word to the n-dimensional complex vector space, which corresponds to the construction process of quantum state in the quantum adjustment. Each word mapped from the discrete space to the high-dimensional Hilbert space corresponds to a column vector (namely the right vector). 3) Quantum encoder: the quantum state representation of words is obtained through the embedding layer, but the whole sentence is still composed of a single discrete word. Before the quantum state evolution encoding information, it is necessary to obtain the quantum state representation of sentences. Specifically, the density matrix representation of sentences is obtained through the formula (). The evolution of quantum systems will be simulated below. The evolution of quantum states and density matrices in quantum computing theory is realized by using quantum gates, which correspond to a unitary square matrix with dimensions corresponding to the number of qubits it operates $m&#x3D;2^n$. In the complex numerical neural network, we want to simulate this process through the complex numerical linear layer and the complex numerical cyclic neural network (Note: In a strict sense, linear layer and cyclic neural network guarantee the invariability of dimension by specifying the dimension of input and output, and cannot guarantee that the operation is reversible. In order to satisfy the unitary operation, it needs to use quantum computer to build the network layer, which is why our model is called quantum heuristic model). 4) Classifier: The function of the classifier is to use the encoded information to output the prediction result. There are two ways to construct the classifier. One is to output the prediction result directly through measurement based on the quantum computing theory; the other is to use XXX based on the previous quantum state and linear output is used to predict the results. The difference between the two ideas lies in that the first one complies with the characteristics of quantum computing theory, while the second one regards the representation of quantum states as the characteristics of coding and makes classification prediction through the classical neural network idea.</p><p>The next step is the detailed construction of each of the ComplexQNN modules, the first of which is ComplexEmbedder. As we all know, the classic word embedding layer can be divided into character level, wordpiece level and word level according to the level of word segmentation, and the common implementation is divided into context and non-context word embedding. word2vec and GloVe are two very classic non-context word vectors. Context word embedding is based on the idea of pre-training model. The data set - related contextual word vectors are obtained by fine tuning. We built a plural embedding into our model, Formula $[w_i w_1,…,…, w_n] &#x3D; [r_i r_1,…,…, r_n] + 1 j \ times [i_i i_1,…,…, i_n] $ shows the word embedded process, it consists of two parts, Real part word embedding and imaginary part word embedding, input token sequence, will output the corresponding complex word vector of the sequence. Specifically, we consider different schemes to construct complex word embedding layer. (1) Different pre-training models are used as embedding layers for the real and imaginary parts, BERT for the real part and RoBERTa for the imaginary part. In this way, different information can be encoded into the quantum state and further processed based on the output of complex word embedding in the following modules.</p><p>The second module is the quantum coding layer. The quantum coding layer needs to meet some conditions. The dimension is $2^n$and the input and output dimensions are unchanged. We mentioned above, before coding, need the density matrix of sentence, according to formula ($M &#x3D; \sum_i ^ n | w_i \rangle \langle w_i | $) shows the calculation process of density matrix. We construct our coding layer through the following basic modules: complex numerical fully connected layer, complex numerical cyclic neural network, complex numerical batch regularization. We construct two intermediate module layers for coding: deep coding, recurrent neural network coding (based on ComplexGRU), Graph and graph () are the two intermediate module layers we construct.</p><p>The third module is the classifier. As mentioned above, we have two ways to construct a classifier. Let’s first describe the first way. The first is a measurement method of using the theory of quantum computing, concrete is through formula ($p &#x3D; tr (PV) $), which the probability of p is predicted, and the tr is matrix trace, $P$ is the projection operator $|p\rangle$corresponding density matrix, the density matrix of V is a sentence said, is a random initialization of the nonzero vector projection operator, By formula (), we can perform text dichotomies. Obviously, a group of projection vector bases is needed for multiclassification. The following experiments involve multiclassification, so the number of basis vectors can be determined according to the number of categories of classification results. The final classification result is to take the one with the highest prediction probability as the final prediction result. Another classification idea is to build a cyclic neural network or use a convolutional neural network for pooling operation according to the classical deep learning method. Finally, the results are mapped to the vector dimension of the number of classification categories through the linear layer, and the subscript corresponding to the maximum value of the result is taken as the prediction result.</p><ul><li><input disabled type="checkbox"> 框架图</li><li><input disabled type="checkbox"> 模块细节图</li></ul><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>ComplexTextCNN （这一块可以考虑放到后面写）</p><ul><li><input disabled type="checkbox"> 情感分类</li><li><input disabled type="checkbox"> 实验结果分析</li><li><input disabled type="checkbox"> 消融实验</li></ul><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this paper, we propose a new quantum-inspired model ComplexQNN based on complex numerical neural networks, and verify the effectiveness of the model on sentiment classification datasets. At the same time, through the ablation experiment, we prove that the complex numerical quantum-inspired model has more outstanding performance in language modeling than the previous quantum-inspired model and complex numerical neural network.</p><p>The future research direction can be considered in two directions: the first is to encode deeper semantics, such as developing complex numerical Tranformer network module suitable for larger data set and applying it in more complex scenarios, such as machine translation and recommendation system; The second is to build network modules using quantum wiring models, although this is limited in the data set that can be processed in the NISQ era.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;div style=&quot;font-family:verdana;font-size: 22px&quot;&gt; ComplexQNN: A Complex-valued Quantum-inspired Language Model &lt;/div&gt;

&lt;h2 id=&quot;中文版&quot;&gt;&lt;a href=</summary>
      
    
    
    
    <category term="论文" scheme="https://levyya.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
    <category term="论文" scheme="https://levyya.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Allennlp学习</title>
    <link href="https://levyya.github.io/2022/12/05/Allennlp%E5%AD%A6%E4%B9%A0/"/>
    <id>https://levyya.github.io/2022/12/05/Allennlp%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-12-05T09:20:28.000Z</published>
    <updated>2023-01-10T12:35:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="AllenNLP"><a href="#AllenNLP" class="headerlink" title="AllenNLP"></a><a href="https://guide.allennlp.org/common-architectures#2">AllenNLP</a></h3><p>AllenNLP是艾伦人工智能研究院开发的开源NLP平台。软件设计优秀，面对对象思想，值得阅读源码</p><p>入门学习：</p><ol><li><a href="https://www.jianshu.com/p/17abfefc1b5b">AllenNLP使用教程</a></li><li><a href="https://zhuanlan.zhihu.com/p/102324519">学习AllenNLP专栏目录</a></li></ol><p>文本处理过程学习：</p><ol><li><p>language to features</p></li><li><p>Tokenizers and TextFields</p></li><li><p>TokenIndexers</p></li><li><p>TextFieldEmbedders</p></li><li><p>Coordinating the three parts</p></li><li><p>pretrained contextualizers and embeddings</p></li><li><p>word-level modeling with a wordpiece transformer</p></li><li><p>padding and masking</p></li><li><p>Interacting with TextField outputs in your model code</p></li></ol><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><p><code>Field</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter, defaultdict</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> TextField, LabelField, SequenceLabelField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token</span><br><span class="line"><span class="keyword">from</span> allennlp.data.vocabulary <span class="keyword">import</span> Vocabulary</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DatasetReader.register(<span class="params"><span class="string">&#x27;classification-tsv&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationTsvReader</span>(<span class="title class_ inherited__">DatasetReader</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.tokenizer = SpacyTokenizer()</span><br><span class="line">        self.token_indexers = &#123;<span class="string">&#x27;tokens&#x27;</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">text_to_instance</span>(<span class="params">self, text: <span class="built_in">str</span>, label: <span class="built_in">str</span> = <span class="literal">None</span></span>) -&gt; Instance:</span><br><span class="line">        tokens = self.tokenizer.tokenize(text)</span><br><span class="line">        text_field = TextField(tokens, self.token_indexers)</span><br><span class="line">        fields = &#123;<span class="string">&#x27;text&#x27;</span>: text_field&#125;</span><br><span class="line">        <span class="keyword">if</span> label:</span><br><span class="line">            fields[<span class="string">&#x27;label&#x27;</span>] = LabelField(label)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_read</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>) -&gt; Iterable[Instance]:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, label = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">                text_field = TextField(self.tokenizer.tokenize(text),</span><br><span class="line">                                       self.token_indexers)</span><br><span class="line">                label_field = LabelField(label)</span><br><span class="line">                fields = &#123;<span class="string">&#x27;text&#x27;</span>: text_field, <span class="string">&#x27;label&#x27;</span>: label_field&#125;</span><br><span class="line">                <span class="keyword">yield</span> Instance(fields)</span><br></pre></td></tr></table></figure><h4 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Model.register(<span class="params"><span class="string">&#x27;simple_classifier&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleClassifier</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                text: <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor],</span></span><br><span class="line"><span class="params">                label: torch.Tensor = <span class="literal">None</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits)</span><br><span class="line">        output = &#123;<span class="string">&#x27;probs&#x27;</span>: probs&#125;</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.accuracy(logits, label)</span><br><span class="line">            <span class="comment"># Shape: (1,)</span></span><br><span class="line">            output[<span class="string">&#x27;loss&#x27;</span>] = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>注：Model 是 torch.nn.Module 的一个子类，forward() 输出是字典</p><p>预测过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Predictor.register(<span class="params"><span class="string">&quot;sentence_classifier&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentenceClassifierPredictor</span>(<span class="title class_ inherited__">Predictor</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, sentence: <span class="built_in">str</span></span>) -&gt; JsonDict:</span><br><span class="line">        <span class="comment"># This method is implemented in the base class.</span></span><br><span class="line">        <span class="keyword">return</span> self.predict_json(&#123;<span class="string">&quot;sentence&quot;</span>: sentence&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_json_to_instance</span>(<span class="params">self, json_dict: JsonDict</span>) -&gt; Instance:</span><br><span class="line">        sentence = json_dict[<span class="string">&quot;sentence&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> self._dataset_reader.text_to_instance(sentence)</span><br></pre></td></tr></table></figure><p>损失函数</p><p>查看模型预测结果 (评估标准) </p><p>保存和读取模型</p><p>正则化</p><hr><h4 id="公共结构"><a href="#公共结构" class="headerlink" title="公共结构"></a>公共结构</h4><p>Summarizing sequences</p><p><code>seq2vec</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.modules.seq2vec_encoders <span class="keyword">import</span> (</span><br><span class="line">Seq2VecEncoder,</span><br><span class="line">    CnnEncoder,</span><br><span class="line">    LstmSeq2VecEncoder,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Contextualizing sequences</p><p><code>seq2seq</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.modules.seq2seq_encoders <span class="keyword">import</span> (</span><br><span class="line">Seq2SeqEncoder,</span><br><span class="line">    PassThroughEncoder,</span><br><span class="line">    LstmSeq2SeqEncoder,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>思考：为什么使用 LstmSeq2VecEncoder 而不是 torch.nn.LSTM ?</p><blockquote><p>The reasons you might want to use a <code>Seq2SeqEncoder</code> instead are three-fold: </p><p>first, it encourages you to think at a higher level about what basic operations your model is doing (am I contextualizing, summarizing, or both?). </p><p>Second, using <a href="https://guide.allennlp.org/using-config-files#1">dependency injection</a> allows you to do controlled experiments easier, if you think you might one day want to try a different contextualizer in your model. Using an abstraction that encapsulates the options you want to experiment with is a powerful way to get very easy, controlled experiments. </p><p>Lastly, having a collection of models available that are written using higher-level abstractions lets component designers easily test their developments on a wide range of models.</p></blockquote><h4 id="文本表示"><a href="#文本表示" class="headerlink" title="文本表示"></a>文本表示</h4><p>将词语转成向量的几种方法 （非上下文、上下文Contextual）</p><ul><li>GloVe or word2vec embeddings</li><li>Character CNNs</li><li>POS tag embeddings</li><li>Combination of GloVe and character CNNs</li><li>wordpieces and BERT</li></ul><p><strong>Three steps</strong> (tokenize, index, embedding) in converting language to features</p><p>Text -&gt; Tokens -&gt; Ids -&gt; Vectors</p><p>具体地</p><ol><li>Tokenizer (Text -&gt; Tokens)</li><li>TextField, TokenIndexer and Vocabulary (Tokens -&gt; Ids)</li><li>TextFieldEmbedder (Ids -&gt; Vectors)</li></ol><p>Tokenizers</p><ul><li>Characters (“AllenNLP is great” → <code>[&quot;A&quot;, &quot;l&quot;, &quot;l&quot;, &quot;e&quot;, &quot;n&quot;, &quot;N&quot;, &quot;L&quot;, &quot;P&quot;, &quot; &quot;, &quot;i&quot;, &quot;s&quot;, &quot; &quot;, &quot;g&quot;, &quot;r&quot;, &quot;e&quot;, &quot;a&quot;, &quot;t&quot;]</code>)</li><li>Wordpieces (“AllenNLP is great” → <code>[&quot;Allen&quot;, &quot;##NL&quot;, &quot;##P&quot;, &quot;is&quot;, &quot;great&quot;]</code>)</li><li>Words (“AllenNLP is great” → <code>[&quot;AllenNLP&quot;, &quot;is&quot;, &quot;great&quot;]</code>)</li></ul><p>note: Wordpieces are similar to words, but further split words into <code>subword units</code>.</p><p>常用的分词器：<code>SpacyTokenizer</code>, <code>PretrainedTransformerTokenizer</code>, <code>CharacterTokenizer</code></p><ul><li>character</li><li>wordpiece</li><li>word</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = ...</span><br><span class="line">sentence = <span class="string">&quot;We are learning about TextFields&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(sentence)</span><br><span class="line">token_indexers = &#123;...&#125;</span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">...</span><br><span class="line">instance = Instance(&#123;<span class="string">&quot;sentence&quot;</span>: text_field, ...&#125;)</span><br></pre></td></tr></table></figure><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a><strong>模型训练</strong></h4><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allennlp train \</span><br><span class="line">    my_text_classifier.jsonnet \</span><br><span class="line">    --serialization-<span class="built_in">dir</span> model-bert \</span><br><span class="line">    --include-package my_text_classifier</span><br></pre></td></tr></table></figure><p>train后面的参数指定了用哪个配置文件，-s后面的目录指定了训练日志、字典、模型等的存放位置，<code>--include-package</code>后面的参数指定了我们前面编写的python代码在哪里</p><p>train 参数</p><ul><li>-f  强制重写输出目录</li><li>–dry-run 加载数据不训练</li></ul><h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python -m allennlp.service.server_simple \</span><br><span class="line">    --archive-<span class="built_in">path</span> /<span class="built_in">path</span>/<span class="keyword">for</span>/model/and/log/model.tar.gz \</span><br><span class="line">    --predictor text_classifier \</span><br><span class="line">    --include-package AllenFrame.classification_code \</span><br><span class="line">    --<span class="built_in">title</span> &quot;classification&quot; \</span><br><span class="line">    --field-name sentence</span><br></pre></td></tr></table></figure><p>conda update -n base conda</p><h4 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h4><ul><li><p>多GPU</p></li><li><p><a href="https://medium.com/optuna/hyperparameter-optimization-for-allennlp-using-optuna-54b4bfecd78b">使用Optuna进行超参数优化</a></p></li></ul><p><a href="https://blog.allenai.org/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad">使用少量内存训练大批次</a></p><ol><li>梯度累计 num_gradient_accumulation_steps</li><li>梯度检查点 gradient_checkpointing&#x3D;True</li><li>自动混合精度 use_amp</li></ol><h4 id="Jsonnet"><a href="#Jsonnet" class="headerlink" title="Jsonnet"></a>Jsonnet</h4><p><a href="https://jsonnet.org/learning/tutorial.html">tutorial</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;AllenNLP&quot;&gt;&lt;a href=&quot;#AllenNLP&quot; class=&quot;headerlink&quot; title=&quot;AllenNLP&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://guide.allennlp.org/common-architectures#2&quot;&gt;Al</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="NLP" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/NLP/"/>
    
    
    <category term="NLP" scheme="https://levyya.github.io/tags/NLP/"/>
    
  </entry>
  
</feed>
