<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Levy&#39;s blog</title>
  
  
  <link href="https://levyya.github.io/atom.xml" rel="self"/>
  
  <link href="https://levyya.github.io/"/>
  <updated>2023-02-21T12:16:57.446Z</updated>
  <id>https://levyya.github.io/</id>
  
  <author>
    <name>Levy</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>歌词--生命因你而火热</title>
    <link href="https://levyya.github.io/2023/02/21/%E6%AD%8C%E8%AF%8D-%E7%94%9F%E5%91%BD%E5%9B%A0%E4%BD%A0%E8%80%8C%E7%81%AB%E7%83%AD/"/>
    <id>https://levyya.github.io/2023/02/21/%E6%AD%8C%E8%AF%8D-%E7%94%9F%E5%91%BD%E5%9B%A0%E4%BD%A0%E8%80%8C%E7%81%AB%E7%83%AD/</id>
    <published>2023-02-21T12:15:20.000Z</published>
    <updated>2023-02-21T12:16:57.446Z</updated>
    
    <content type="html"><![CDATA[<p>新裤子</p><blockquote><p>勇敢的你</p><p>站在这里</p><p>脸庞清瘦却骄傲</p><p>在这远方</p><p>没人陪伴</p><p>只有幻想和烦恼</p><p>无聊的 渺小的 反对不公平的世界</p><p>没能继续的革命</p><p>不欢而散的告别</p><p>我倒下后</p><p>不敢回头</p><p>不能再见的朋友</p><p>有人堕落</p><p>有人疯了</p><p>有人随着风去了</p><p>我难过</p><p>我不得不去工作</p><p>在大楼的一个角落</p><p>格子间的女孩</p><p>时间久了也很美</p><p>我会和她结婚</p><p>带我去小城过年</p><p>忘了吧那摇滚乐</p><p>奔腾不复的时代</p><p>我倒下后</p><p>不敢回头</p><p>被社会伤害的人们</p><p>有人堕落</p><p>有人疯了</p><p>有人随着风去了</p><p>那些昙花一现的灿烂</p><p>是爆炸的烟火</p><p>那一团耀眼的火焰</p><p>在燃烧着你和我</p><p>那刻骨铭心的恋爱</p><p>总带给我伤害</p><p>那平淡如水的生活</p><p>因为你而火热</p><p>勇敢的你</p><p>站在这里</p><p>脸庞清瘦却骄傲</p><p>在这远方</p><p>没人陪伴</p><p>只有幻想和烦恼</p><p>我倒下后</p><p>不敢回头</p><p>不能再见的朋友</p><p>有人堕落</p><p>有人疯了</p><p>有人随着风去了</p><p>那些昙花一现的灿烂</p><p>是爆炸的烟火</p><p>那一团耀眼的火焰</p><p>在燃烧着你和我</p><p>那刻骨铭心的恋爱</p><p>总带给我伤害</p><p>那平淡如水的生活</p><p>因为你而火热</p><p>那些昙花一现的灿烂</p><p>是爆炸的烟火</p><p>那一团耀眼的火焰</p><p>在燃烧着你和我</p><p>那刻骨铭心的恋爱</p><p>总带给我伤害</p><p>那平淡如水的生活</p><p>因为你而火热</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;新裤子&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;勇敢的你&lt;/p&gt;
&lt;p&gt;站在这里&lt;/p&gt;
&lt;p&gt;脸庞清瘦却骄傲&lt;/p&gt;
&lt;p&gt;在这远方&lt;/p&gt;
&lt;p&gt;没人陪伴&lt;/p&gt;
&lt;p&gt;只有幻想和烦恼&lt;/p&gt;
&lt;p&gt;无聊的 渺小的 反对不公平的世界&lt;/p&gt;
&lt;p&gt;没能继续的革命&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="歌词" scheme="https://levyya.github.io/tags/%E6%AD%8C%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>数据结构</title>
    <link href="https://levyya.github.io/2023/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>https://levyya.github.io/2023/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</id>
    <published>2023-02-20T13:07:08.000Z</published>
    <updated>2023-02-20T12:17:25.692Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://eajack.github.io/2020/07/03/%E3%80%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90-C%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E3%80%8BC++%E5%AE%9E%E7%8E%B0%EF%BC%88CPP%20%E9%87%8D%E6%9E%84%E7%89%88%E6%9C%AC%EF%BC%89/">学习参考</a></p></blockquote><h2 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h2><h2 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h2><h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><p>b树，b+树</p><p>红黑树</p><p>场景</p><h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Method 1</span></span><br><span class="line"><span class="comment">// Separate chainning method: </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HashMap</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">HashMap1</span>();</span><br><span class="line">    <span class="built_in">HashMap1</span>(<span class="type">int</span> size);</span><br><span class="line">    <span class="built_in">HashMap1</span>(<span class="type">int</span> size, <span class="type">const</span> <span class="type">int</span> keys[], <span class="type">int</span> N);</span><br><span class="line">    ~<span class="built_in">HashMap1</span>();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 1. 下一个质数，用来更新size</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">nextPrime</span><span class="params">(<span class="type">int</span> N)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">hashFunction</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insert</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">erase</span><span class="params">(<span class="type">int</span> key)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">size</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    std::vector&lt;std::vector&lt;HashNode1&gt;&gt; hashMap1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参考<a href="https://www.cnblogs.com/kuillldan/p/6079992.html">博客</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Method2</span></span><br><span class="line"><span class="comment">// 开放定址法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashTable</span>&lt;K, V&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">class</span> <span class="title class_">KeyValue</span> &#123;</span><br><span class="line">        K key;</span><br><span class="line">        V value;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">private</span> <span class="title function_">KeyValue</span><span class="params">(K key, V value)</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.key = key;</span><br><span class="line">            <span class="built_in">this</span>.value = value;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// get and set</span></span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> Object[] table;</span><br><span class="line">    <span class="type">private</span> <span class="variable">maxSize</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">currentAmmount</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">HashTable</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.table = <span class="keyword">new</span> <span class="title class_">Object</span>[<span class="built_in">this</span>.maxSize];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">HashTable</span><span class="params">(<span class="type">int</span> maxSize)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> == maxSize || maxSize &lt; <span class="number">0</span> || maxSize &gt; <span class="number">100</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;Illegal maxSize!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">this</span>.maxSize = maxSize;</span><br><span class="line">        <span class="built_in">this</span>.table = <span class="keyword">new</span> <span class="title class_">Info</span>[maxSize];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">add</span><span class="params">(K key, V value)</span> &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span> <span class="variable">hashCode</span> <span class="operator">=</span> Math.abs(key.hashCode()) % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">this</span>.table[hashCode] != <span class="literal">null</span> &amp;&amp; (<span class="built_in">this</span>.currentAmount &lt; <span class="built_in">this</span>.maxSize)) &#123;</span><br><span class="line">            hashCode++;</span><br><span class="line">            hashCode = hashCode % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.currentAmount == <span class="built_in">this</span>.maxSize) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Hash table is out of memory!&quot;</span>);</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="built_in">this</span>.table[hashCode] = <span class="keyword">new</span> <span class="title class_">KeyValue</span>(key, value);</span><br><span class="line">            <span class="built_in">this</span>.currentAmount++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">remove</span><span class="params">(K key)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">hashCode</span> <span class="operator">=</span> math.abs(key.hashCode()) % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">this</span>.table[hashCode] != <span class="literal">null</span> &amp;&amp; count &lt; <span class="built_in">this</span>.maxSize) &#123;</span><br><span class="line">            <span class="keyword">if</span> (((KeyValue) <span class="built_in">this</span>.table[hashCode]).getKey().equals(key)) &#123;</span><br><span class="line">                <span class="built_in">this</span>.table[hashCode] = <span class="literal">null</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            count++;</span><br><span class="line">            hashCode++;</span><br><span class="line">            hashCode = hashCode % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> V <span class="title function_">get</span><span class="params">(K key)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">hashCode</span> <span class="operator">=</span> Math.abs(key.hashCode()) % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">this</span>.table[hashCode] != <span class="literal">null</span> &amp;&amp; count &lt; <span class="built_in">this</span>.maxSize) &#123;</span><br><span class="line">            <span class="keyword">if</span> (key.equals(((KeyValue) <span class="built_in">this</span>.table[hashCode]).getKey())) &#123;</span><br><span class="line">                <span class="keyword">return</span> ((KeyValue) <span class="built_in">this</span>.table[hashCode]).getValue();</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            hashCode++;</span><br><span class="line">            count++;</span><br><span class="line">            hashCode = hashCode % <span class="built_in">this</span>.maxSize;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><p>插入排序</p><ul><li>直接插入</li><li>Shell排序 O(n^1.3) 不稳定</li></ul><p>选择排序</p><ul><li>直接选择 不稳定</li><li>堆排序 不稳定</li></ul><p>交换排序</p><ul><li>冒泡排序 稳定</li><li>快速排序 不稳定</li></ul><p>归并排序 稳定</p><p>基数排序 稳定</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://eajack.github.io/2020/07/03/%E3%80%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%88%</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="数据结构" scheme="https://levyya.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>歌词--火车</title>
    <link href="https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E7%81%AB%E8%BD%A6/"/>
    <id>https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E7%81%AB%E8%BD%A6/</id>
    <published>2023-02-20T11:24:04.000Z</published>
    <updated>2023-02-20T11:24:45.109Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我那些残梦 灵异九霄</p><p>徒忙漫奋斗 满目沧愁</p><p>在滑翔之后 完美坠落</p><p>在四维宇宙 眩目遨游</p><p>我那些烂曲 流窜九州</p><p>云游魂飞奏 音愤符吼</p><p>在宿命身后 不停挥手</p><p>视死如归仇 毫无保留</p><p>黑色的不是夜晚 是漫长的孤单</p><p>看脚下一片黑暗 望头顶星光璀璨</p><p>叹世万物皆可盼 唯真爱最短暂</p><p>失去的永不复返 世守恒而今倍还</p><p>摇旗呐喊的热情 携光阴渐远去</p><p>人世间悲喜烂剧 昼夜轮播不停</p><p>纷飞的滥情男女 情仇爱恨别离</p><p>一代人终将老去 但总有人正年轻</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;我那些残梦 灵异九霄&lt;/p&gt;
&lt;p&gt;徒忙漫奋斗 满目沧愁&lt;/p&gt;
&lt;p&gt;在滑翔之后 完美坠落&lt;/p&gt;
&lt;p&gt;在四维宇宙 眩目遨游&lt;/p&gt;
&lt;p&gt;我那些烂曲 流窜九州&lt;/p&gt;
&lt;p&gt;云游魂飞奏 音愤符吼&lt;/p&gt;
&lt;p&gt;在宿命身后 不停挥手&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
    <category term="歌词" scheme="https://levyya.github.io/tags/%E6%AD%8C%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>歌词--花火 (新裤子)</title>
    <link href="https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E8%8A%B1%E7%81%AB-%E6%96%B0%E8%A3%A4%E5%AD%90/"/>
    <id>https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E8%8A%B1%E7%81%AB-%E6%96%B0%E8%A3%A4%E5%AD%90/</id>
    <published>2023-02-20T11:16:16.000Z</published>
    <updated>2023-02-20T11:23:29.578Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这是一场没有结局的表演</p><p>包含所有荒谬和疯狂</p><p>像个孩子一样满怀悲伤</p><p>静悄悄地睡在大地上</p><p>现在我 有些倦了</p><p>倦得像一朵被风折断的野花</p><p>所以我 开始变了</p><p>变得像一团滚动炽热的花火</p><p>看着眼前欢笑骄傲的人群</p><p>心中泛起汹涌浪花</p><p>跳着放荡的舞蹈穿行在旷野</p><p>感到狂野 破碎辉煌</p><p>现在我 有些醉了</p><p>醉得像一只找不到方向的野鸽</p><p>所以我 开始变了</p><p>变得像一团暴烈炽热的花火</p><p>现在我 有些醉了</p><p>醉得像一只找不到方向的野鸽</p><p>所以我 开始变了</p><p>变得像一团暴烈炽热的花火</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;这是一场没有结局的表演&lt;/p&gt;
&lt;p&gt;包含所有荒谬和疯狂&lt;/p&gt;
&lt;p&gt;像个孩子一样满怀悲伤&lt;/p&gt;
&lt;p&gt;静悄悄地睡在大地上&lt;/p&gt;
&lt;p&gt;现在我 有些倦了&lt;/p&gt;
&lt;p&gt;倦得像一朵被风折断的野花&lt;/p&gt;
&lt;p&gt;所以我 开始变了&lt;/p&gt;
&lt;p&gt;变</summary>
      
    
    
    
    
    <category term="歌词" scheme="https://levyya.github.io/tags/%E6%AD%8C%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>歌词--没有理想的人不伤心</title>
    <link href="https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E6%B2%A1%E6%9C%89%E7%90%86%E6%83%B3%E7%9A%84%E4%BA%BA%E4%B8%8D%E4%BC%A4%E5%BF%83/"/>
    <id>https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E6%B2%A1%E6%9C%89%E7%90%86%E6%83%B3%E7%9A%84%E4%BA%BA%E4%B8%8D%E4%BC%A4%E5%BF%83/</id>
    <published>2023-02-20T11:15:55.000Z</published>
    <updated>2023-02-20T11:18:54.513Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我最爱去的唱片店</p><p>昨天是她的最后一天</p><p>曾经让我陶醉的碎片</p><p>全都散落在街边</p><p>我最爱去的书店</p><p>她也没撑过这个夏天</p><p>回忆文字流淌着怀念</p><p>可是已没什么好怀念</p><p>可是你曾经的那些梦</p><p>都已变得模糊看不见</p><p>那些为了理想的战斗</p><p>也不过为了钱</p><p>可是我最恨的那个人</p><p>他始终没死在我面前</p><p>还没年轻就变得苍老</p><p>这一生无解</p><p>没有我的空间</p><p>没有我的空间</p><p>没有我的空间</p><p>没有我的空间</p><p>你曾热爱的那个人</p><p>这一生也不会再见面</p><p>你等在这文化的废墟上</p><p>已没人觉得你狂野</p><p>那些让人敬仰的神殿</p><p>只在无知的人心中灵验</p><p>我住在属于我的猪圈</p><p>这一夜无眠</p><p>我不要在失败孤独中死去</p><p>我不要一直活在地下里</p><p>物质的骗局</p><p>匆匆的蚂蚁</p><p>没有文化的人不伤心</p><p>我不要在失败孤独中死去</p><p>我不要一直活在地下里</p><p>物质的骗局</p><p>匆匆的蚂蚁</p><p>没有文化的人不伤心</p><p>他不伤心</p><p>我最爱去的唱片店</p><p>昨天是她的最后一天</p><p>曾经让我陶醉的碎片</p><p>全都散落在街边</p><p>我最爱去的书店</p><p>她也没撑过这个夏天</p><p>回忆文字流淌着怀念</p><p>已不能怀念</p><p>我不要在失败孤独中死去</p><p>我不要一直活在地下里</p><p>物质的骗局</p><p>匆匆的蚂蚁</p><p>没有文化的人不伤心</p><p>我不要在失败孤独中死去</p><p>我不要一直活在地下里</p><p>物质的骗局</p><p>匆匆的蚂蚁</p><p>没有文化的人不伤心</p><p>他不伤心</p><p>我不要在失败孤独中死去</p><p>我不要一直活在地下里</p><p>物质的骗局</p><p>匆匆的蚂蚁</p><p>没有文化的人不伤心</p><p>我不要在失败孤独中死去</p><p>我不要一直活在地下里</p><p>物质的骗局</p><p>匆匆的蚂蚁</p><p>没有文化的人不伤心</p><p>他不会伤心</p><p>他不会伤心</p><p>他不会伤心</p><p>他不会伤心</p><p>他也会伤心</p><p>他也会伤心</p><p>他也会伤心</p><p>伤心</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;我最爱去的唱片店&lt;/p&gt;
&lt;p&gt;昨天是她的最后一天&lt;/p&gt;
&lt;p&gt;曾经让我陶醉的碎片&lt;/p&gt;
&lt;p&gt;全都散落在街边&lt;/p&gt;
&lt;p&gt;我最爱去的书店&lt;/p&gt;
&lt;p&gt;她也没撑过这个夏天&lt;/p&gt;
&lt;p&gt;回忆文字流淌着怀念&lt;/p&gt;
&lt;p&gt;可是已没什么好怀念&lt;</summary>
      
    
    
    
    
    <category term="歌词" scheme="https://levyya.github.io/tags/%E6%AD%8C%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>歌词--白日梦蓝</title>
    <link href="https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E7%99%BD%E6%97%A5%E6%A2%A6%E8%93%9D/"/>
    <id>https://levyya.github.io/2023/02/20/%E6%AD%8C%E8%AF%8D-%E7%99%BD%E6%97%A5%E6%A2%A6%E8%93%9D/</id>
    <published>2023-02-20T11:15:32.000Z</published>
    <updated>2023-02-20T11:22:42.078Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>青春是青涩的年代</p><p>我明白 明天不会有色彩</p><p>社会是伤害的比赛</p><p>当我醒来时才明白</p><p>请你不要离开</p><p>这里胜似花开</p><p>没有人能够掩盖</p><p>梦境中的色彩</p><p>请你不要离开</p><p>这里胜似花开</p><p>没有人会去涂改</p><p>梦境中的色彩</p><p>头上蓝色时光流淌</p><p>空荡的世界沮丧</p><p>请你不要离开</p><p>这里胜似花开</p><p>没有人能够掩盖</p><p>梦境中的色彩</p><p>请你不要离开</p><p>这里胜似花开</p><p>没有人会去涂改</p><p>梦境中的色彩</p><p>青春是青涩的年代</p><p>我明白 明天不会有色彩</p><p>社会是伤害的比赛</p><p>当我醒来时才明白</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;青春是青涩的年代&lt;/p&gt;
&lt;p&gt;我明白 明天不会有色彩&lt;/p&gt;
&lt;p&gt;社会是伤害的比赛&lt;/p&gt;
&lt;p&gt;当我醒来时才明白&lt;/p&gt;
&lt;p&gt;请你不要离开&lt;/p&gt;
&lt;p&gt;这里胜似花开&lt;/p&gt;
&lt;p&gt;没有人能够掩盖&lt;/p&gt;
&lt;p&gt;梦境中的色彩&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    
    <category term="歌词" scheme="https://levyya.github.io/tags/%E6%AD%8C%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>EndNote 使用</title>
    <link href="https://levyya.github.io/2023/02/04/EndNote-%E4%BD%BF%E7%94%A8/"/>
    <id>https://levyya.github.io/2023/02/04/EndNote-%E4%BD%BF%E7%94%A8/</id>
    <published>2023-02-04T03:17:57.000Z</published>
    <updated>2023-02-04T09:12:30.592Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Import-Option"><a href="#Import-Option" class="headerlink" title="Import Option"></a>Import Option</h1><p>Web of science：ISI-CE</p><p>pubmed、scopus、ScienceDirect ： Reference Manager(RIS)</p><p>知网、万方、维普：EndNote import</p><p>google scholar： EndNote generated XML</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Import-Option&quot;&gt;&lt;a href=&quot;#Import-Option&quot; class=&quot;headerlink&quot; title=&quot;Import Option&quot;&gt;&lt;/a&gt;Import Option&lt;/h1&gt;&lt;p&gt;Web of science：ISI-CE&lt;/p&gt;
</summary>
      
    
    
    
    
    <category term="EndNote" scheme="https://levyya.github.io/tags/EndNote/"/>
    
  </entry>
  
  <entry>
    <title>歌词--浪子回头</title>
    <link href="https://levyya.github.io/2023/02/02/%E6%AD%8C%E8%AF%8D-%E6%B5%AA%E5%AD%90%E5%9B%9E%E5%A4%B4/"/>
    <id>https://levyya.github.io/2023/02/02/%E6%AD%8C%E8%AF%8D-%E6%B5%AA%E5%AD%90%E5%9B%9E%E5%A4%B4/</id>
    <published>2023-02-02T07:03:56.000Z</published>
    <updated>2023-02-02T07:13:36.625Z</updated>
    
    <content type="html"><![CDATA[<p>浪子回头(歌词音译)</p><blockquote><p>烟一支一支一支地点</p><p>婚~叽 gī 叽 gī 叽 gī 爹 diang</p><p>酒一杯一杯一杯地干</p><p>就~叽 boy 叽 boy 叽 boy 爹 荡</p><p>请你要体谅我</p><p>呛 哩 爱咿 体 谅 哇</p><p>我酒量不好别给我挖坑</p><p>哇 酒 量 波 候 麦 嘎 卧 葱 坑</p><p>时间一天一天一天的走</p><p>西干-叽 刚 叽 刚 叽 刚 爹 走~乌</p><p>汗一滴一滴一的流</p><p>锅~叽 滴 叽 滴 叽 滴 爹 老~乌</p><p>有一天我们都老</p><p>乌叽刚-囊隆老</p><p>带妻子一起</p><p>cua 波 gian 到 丁</p><p>浪子回头</p><p>隆 子 were 涛</p><p>亲爱的 可爱的 英俊的 朋友</p><p>亲爱爹 阔爱爹 颠多爹~扁优</p><p>垃圾的 没品的 没出息的 朋友</p><p>公扫爹 摸拼爹 摸多优泥~扁优</p><p>在坎坷的路骑我二流摩托车</p><p>滴 康 K 诶 哆 咔 哇 luang 嘭 O 哆 哇 咿</p><p>反正我的人生像是狗屎</p><p>怀 滴 歪 井 熊 刚 那 搞 赛</p><p>我没钱没妻没子只有一条命</p><p>瓦 波 七 波 窝 波 giang 刚 那 几 掉 命</p><p>朋友阿</p><p>扁 优 哇</p><p>一起来赌吧</p><p>到 丁 来 boy 啊</p><p>烟一支一支一支地点</p><p>婚~叽 gī 叽 gī 叽 gī 爹 diang</p><p>酒一杯一杯一杯地干</p><p>就~叽 boy 叽 boy 叽 boy 爹 荡</p><p>请你要体谅我</p><p>呛 哩 爱咿 体 谅 哇</p><p>我酒量不好别给我挖坑</p><p>哇 酒 量 波 候 麦 嘎 卧 葱 坑</p><p>时间一天一天一天的走</p><p>西干-叽 刚 叽 刚 叽 刚 爹 走~乌</p><p>汗一滴一滴一滴的流</p><p>锅<del>叽 滴 叽 滴 叽 滴 爹老\</del>乌</p><p>有一天我们都老</p><p>乌叽刚-囊隆老</p><p>带妻子一起</p><p>cua 波 gian 到 丁</p><p>浪子回头</p><p>隆 子 were 涛</p><p>在坎坷的路骑我二流摩托车</p><p>滴 康 K 诶 哆 咔 哇 luang 嘭 O 哆 哇 咿</p><p>反正我的人生像是狗屎</p><p>怀 滴 歪 井 熊 刚 那 搞 赛</p><p>我没钱没妻没子只有一条命</p><p>瓦 波 七 波 窝 波 giang 刚 那 几 掉 命</p><p>朋友阿</p><p>扁 优 哇</p><p>一起来赌吧</p><p>到 丁 来 boy 啊</p><p>烟一支一支一支地点</p><p>婚~叽 gī 叽 gī 叽 gī 爹 diang</p><p>酒一杯一杯一杯地干</p><p>就~叽 boy 叽 boy 叽 boy 爹 荡</p><p>请你要体谅我</p><p>呛 哩 爱咿 体 谅 哇</p><p>我酒量不好别给我挖坑</p><p>哇 酒 量 波 候 麦 嘎 卧 葱 坑</p><p>时间一天一天一天的走</p><p>西干-叽 刚 叽 刚 叽 刚 爹 走~乌</p><p>汗一滴一滴一滴的流</p><p>锅~叽 滴 叽 滴 叽 滴 爹老~乌</p><p>有一天我们都老</p><p>乌叽刚-囊隆老</p><p>带妻子一起</p><p>cua 波 gian 到 丁</p><p>带妻子一起</p><p>cua 波 gian 到 丁</p><p>带妻子一起</p><p>cua 波 gian 到 丁</p></blockquote><p>参考：</p><ul><li><a href="https://zhidao.baidu.com/question/1388880860497644060.html">https://zhidao.baidu.com/question/1388880860497644060.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;浪子回头(歌词音译)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;烟一支一支一支地点&lt;/p&gt;
&lt;p&gt;婚~叽 gī 叽 gī 叽 gī 爹 diang&lt;/p&gt;
&lt;p&gt;酒一杯一杯一杯地干&lt;/p&gt;
&lt;p&gt;就~叽 boy 叽 boy 叽 boy 爹 荡&lt;/p&gt;
&lt;p&gt;请你要体谅我&lt;/p</summary>
      
    
    
    
    
    <category term="歌词" scheme="https://levyya.github.io/tags/%E6%AD%8C%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--Quantum Natural Language Processing: Challenges and Opportunities</title>
    <link href="https://levyya.github.io/2023/01/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Natural-Language-Processing-Challenges-and-Opportunities/"/>
    <id>https://levyya.github.io/2023/01/31/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Natural-Language-Processing-Challenges-and-Opportunities/</id>
    <published>2023-01-31T08:05:16.000Z</published>
    <updated>2023-02-02T07:04:12.828Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href="https://www.mdpi.com/2076-3417/12/11/5651">click</a></p><p>作者：</p><blockquote><p>Raffaele Guarasci, Giuseppe De Pietroand Massimo Esposito *</p><p>Institute for High Performance Computing and Networking (ICAR), National Research Council (CNR),80131 Naples, Italy; <a href="mailto:&#114;&#97;&#102;&#102;&#97;&#x65;&#x6c;&#x65;&#46;&#103;&#117;&#97;&#x72;&#x61;&#115;&#x63;&#105;&#x40;&#105;&#99;&#x61;&#x72;&#x2e;&#x63;&#110;&#114;&#46;&#x69;&#x74;">&#114;&#97;&#102;&#102;&#97;&#x65;&#x6c;&#x65;&#46;&#103;&#117;&#97;&#x72;&#x61;&#115;&#x63;&#105;&#x40;&#105;&#99;&#x61;&#x72;&#x2e;&#x63;&#110;&#114;&#46;&#x69;&#x74;</a> (R.G.); <a href="mailto:&#x67;&#x69;&#x75;&#115;&#101;&#112;&#112;&#101;&#x2e;&#100;&#x65;&#x70;&#x69;&#101;&#116;&#114;&#x6f;&#64;&#105;&#99;&#97;&#114;&#x2e;&#x63;&#x6e;&#x72;&#46;&#x69;&#x74;">&#x67;&#x69;&#x75;&#115;&#101;&#112;&#112;&#101;&#x2e;&#100;&#x65;&#x70;&#x69;&#101;&#116;&#114;&#x6f;&#64;&#105;&#99;&#97;&#114;&#x2e;&#x63;&#x6e;&#x72;&#46;&#x69;&#x74;</a> (G.D.P.)*Correspondence: <a href="mailto:&#x6d;&#97;&#x73;&#115;&#x69;&#x6d;&#111;&#x2e;&#101;&#115;&#x70;&#111;&#x73;&#x69;&#x74;&#x6f;&#64;&#105;&#99;&#x61;&#x72;&#x2e;&#99;&#110;&#114;&#x2e;&#105;&#x74;">&#x6d;&#97;&#x73;&#115;&#x69;&#x6d;&#111;&#x2e;&#101;&#115;&#x70;&#111;&#x73;&#x69;&#x74;&#x6f;&#64;&#105;&#99;&#x61;&#x72;&#x2e;&#99;&#110;&#114;&#x2e;&#105;&#x74;</a></p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>近年来，自然语言处理(NLP)和量子计算(QNLP)的结合非常成功，导致了所谓的量子自然语言处理(QNLP)的几种方法的发展。这是一个混合领域，其中量子力学的潜力被开发并应用于语言处理的关键方面，涉及不同的NLP任务。迄今为止开发的方法从那些仅在理论层面上证明量子优势的方法到在量子硬件上实现算法的方法。本文旨在列出迄今为止开发的方法，按类型进行分类，即理论工作和在经典或量子硬件上实现的工作;按任务，即一般目的，如语法-语义表示或特定的NLP任务，如情绪分析或问题回答;以及评估阶段使用的资源，即使用基准数据集还是自定义数据集。讨论了QNLP在性能和方法方面的优势，并给出了一些关于使用QNLP方法取代最先进的基于深度学习的方法的可能考虑。</p><p>关键词:量子计算;自然语言处理</p></blockquote><p>一篇关于量子自然语言处理的综述</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>简介近年来，基于深度学习架构的神经语言模型的爆发使得所有NLP任务[1-4]都有了显著的改善，从机器翻译[5]，文本分类[6]，共指解析[7,8]或多语言语法分析[9-11]。特别是，基于transformer的模型(如BERT)已被证明优于上一代最先进的体系结构，如长短期记忆(LSTM)循环神经网络(RNN)。然而，与性能的提高相匹配的是模型的复杂性不断增加，这导致了一个悖论。模型需要大量的数据来进行有效的训练，在时间、资源和计算上都有巨大的成本。这是目前基于transformer的方法的主要缺点，例如这类神经网络的参数数量达到了数千亿数量级(数据参考OpenAI GPT模型)[12,13]。此外，它需要大量的资源用于训练阶段(例如，整个维基百科语料库的几种语言)。除了这些方面，还有一些固有的开放性问题，这些模型真正从语言中学习了什么[14,15]，它们如何对这些信息进行编码，以及学习到的信息有多少是真正可解释的。文献已经产生了一些研究，重点是神经语言模型是否能够编码某种语言信息，或者它们是否只是复制书面文本中观察到的模式。近年来备受关注的另一种方法是源于量子计算，特别是量子机器学习子领域。这个想法是利用从量子力学中借来的强大方面来克服当前方法的计算限制。经典统计学的主导范式可以用量子力学扩展，用复数矩阵表示对象。</p><p>在量子计算中，比特被量子比特所取代，量子比特能够利用量子系统的叠加[19]特性处理非二进制状态下的信息。与经典方法相比，量子算法可以使用量子比特的固有属性(称为超多项式加速)执行更小的计算复杂度[18,20,21]。最近的工作观察到了量子算法的发展，这些算法可以作为机器学习应用的基础。在某些情况下，量子特性被用于简单地提高机器学习方法的性能，而在其他情况下，机器学习的问题已经使用量子理论重新表述[22,23]。利用量子系统的机器学习算法处理经典数据已经产生了大量的研究。量子机器学习已被用于不同的目的:深入研究量子现象在学习系统中的使用，探索量子计算机在量子数据上学习的能力，以及在量子硬件上重新制定和实现机器学习算法的可能性。就像以前在经典机器学习的情况下发生的那样，量子机器学习算法的这种快速增长，无论是从硬件还是软件方面(就设备和算法而言)，都涉及到自然语言处理(NLP)领域。这就产生了所谓的量子nlp (QNLP)[24]，定义为在量子硬件上实现自然语言。它将组合语言结构(语法和语义)与量子系统实现的组合相匹配，以便对自然语言建模，并利用源自量子机器学习的算法执行简单的NLP任务。</p><p>QNLP的核心思想是，将语言意义和语法结构结合在一起的最有效方式是由范畴量子力学框架提供的。此外，它还承诺更多地了解建模中涉及的语言的含义。使用可视化表示语言的优点是可以表示一个词的意思和它与句子中其他词的关系，以及它的结构。这将从经典的基于依赖项的表示法向前迈进一步，这种表示法依赖于树结构，而与所涉及的语法元素的含义没有显式连接。事实上所有QNLP方法的共同理论框架是自然语言[25]的分类分布成分(DisCoCat)模型。DisCoCat允许将单词和短语的含义编码为量子态，并随后在专用硬件或模拟器中作为量子电路实现。目前，开发的方法主要可以通过混合方法进行，将经典操作与量子力学的操作相结合。这是因为，尽管基于量子的方法的优势在理论水平上得到了证明[25,26]，但硬件的可用性目前仅限于小型到中型规模的机器(特别是NISQ计算机:有噪声的中型规模)[27]。鉴于NLP社区对QNLP提供的可能性越来越感兴趣，本文旨在提供解决自然语言处理任务的量子方法的全面概述。在简要介绍了量子力学和NLP的概念和性质之后，描述了所提出的方法的理论背景。随后，列出了不同的工作，区分它们的类型，即尚未真正实现的完整理论工作，基于量子力学但在经典硬件上运行的工作，以及可以在目前可用的量子硬件上运行并已在真实数据上测试的工作。</p><p>请注意，尽管QNLP很有前途，可以解决许多与当前成熟的NLP机制相关的关键问题，但目前还不可能在经典方法和量子方法之间进行真正的比较。这是因为许多作品只有理论上已经证明了性能优势，而那些已经实现的性能优势只能在非常小的数据部分上运行。本文的结构如下:第二部分分别从量子计算和语言学的角度介绍了本文的理论背景。首先，在2.1节中详细介绍了QNLP中使用的基本量子力学概念、算法和硬件。第2.2节简要介绍了量子计算和NLP之间的交集。之后，在第3节中列举了迄今为止提出的QNLP方法，将其分为理论方法(第4节)、经典硬件方法(第5节)和在真实量子硬件上运行的方法(第6节)。第7节和第8节致力于回顾所列出的工作，并强调了QNLP方法的优点和缺点。</p></blockquote><p>* </p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><blockquote><p>尽管QNLP仍然是一个新的研究领域，到目前为止，它的应用主要集中在相对简单的任务上，但它已经在许多方面显示出了许多机会和优势。</p><p>首先，从理论的角度来看，基于量子的方法可以更好地处理自然语言的机制:这就是所谓的QNLP的“量子原生”观点，得到了许多研究的支持。因此，量子语言模型将更适合以一种更符合人类真实认知过程的方式来理解和描述自然语言现象。然而，尽管这一说法很吸引人，但目前没有具体的证据来支持它，除了在管理特定情况下，即使用有限和受控词汇的简单句子。这既不能反映人类学习语言的实际方式，也不能反映语言的产生。从应用的角度来看，随着量子计算机的功率和存储容量的增加，未来可能会出现一种解决方案，这可能是处理越来越大的向量空间的最现实和最合适的选择。从理论的角度来看，使用CFG和预组引入不同结构的不同类型的句子是昂贵的，因为在某些情况下，这意味着这些资源必须从头开始构建。此外，还有一些关于这些类型的语法是否能够近似所有语法的问题语言现象的种类。</p><p>关于所谓的“量子优势”，与最先进的基线相比，一些运行在经典硬件上的QNLP模型在各种任务中取得了类似甚至更好的性能。作为这些语句应用的一个例子，QNLP模型被应用于管理NLP的各个方面，这些方面对于处理经典概率模型一直是至关重要的，例如信息检索中的干扰现象，术语依赖或歧义消解。</p><p>然而，量子加速所提供的性能提升仅在理论上得到了证明，依赖于量子硬件，该硬件可以利用尚未实现且昂贵的QRAM。尽管已经找到了替代解决方案来弥补这一缺点，但NLP任务上的实现还不能与文献中众所周知的经典解决方案相比。事实上，这些方法所测试的数据非常少(在中等规模的实验中大约有100个自然语言句子)，这是由于目前NISQ计算机施加的限制，也是由于需要为所涉及的句子创建一个特别的语法，因此任务非常有限。</p><p>关于未来的工作，QNLP无疑可以解决依赖大型数据集和具有大量参数的复杂模型的理想结局，但仍有许多问题尚未解决。从理论的角度来看，在模型基础上构建cfg以及为不同语言创建引人注目的形式化描述的可能性方面仍然存在可伸缩性的问题。相反，从应用的角度来看，它可以考虑更重要的现实世界数据的使用，以及更复杂的QNLP任务的实现，如句子相似性和所有那些涉及含糊性的任务，这些任务可能从QNLP的量子原生方法中受益最大。然而，尽管QNLP方法可能需要数年时间才能大规模执行，就像目前基于深度学习的模型一样，QNLP可以提供独特的机会，使用量子特性以更类似于自然语言的工作方式来处理具有挑战性的语言现象。利用量子叠加来模拟语言中的不确定性和歧义或纠缠来有效地描述语法和语义的组成和分布已经成功地证明了这一点。</p></blockquote><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href=&quot;https://www.mdpi.com/2076-3417/12/11/5651&quot;&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Raffaele Guarasci, Giuseppe De Pietroand </summary>
      
    
    
    
    
    <category term="默认标签" scheme="https://levyya.github.io/tags/%E9%BB%98%E8%AE%A4%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 常用命令</title>
    <link href="https://levyya.github.io/2023/01/27/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>https://levyya.github.io/2023/01/27/hexo%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</id>
    <published>2023-01-27T09:46:36.605Z</published>
    <updated>2023-01-10T12:35:53.942Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>Q: Hexo 图片无法加载问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image -- save</span><br></pre></td></tr></table></figure><p>TODO:</p><p><a href="https://cloud.tencent.com/developer/article/1970544">https://cloud.tencent.com/developer/article/1970544</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="博客" scheme="https://levyya.github.io/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
    <category term="博客" scheme="https://levyya.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
    <category term="Blog" scheme="https://levyya.github.io/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--QNLP in Pratice</title>
    <link href="https://levyya.github.io/2023/01/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-QNLP-in-Pratice/"/>
    <id>https://levyya.github.io/2023/01/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-QNLP-in-Pratice/</id>
    <published>2023-01-17T15:07:03.000Z</published>
    <updated>2023-01-18T15:44:04.413Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href>click</a></p><p>作者：</p><blockquote><p>Robin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri Kartsaklis, Bob Coecke</p><p>剑桥量子计算，牛津大学</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>量子自然语言处理(QNLP)处理旨在在量子硬件上运行的NLP模型的设计和实现。在本文中，我们介绍了在噪声中等尺度量子(NISQ)计算机上对大小为&gt; &#x3D;100句的数据集进行的第一次NLP实验的结果。利用Coecke等人(2010)利用量子理论的意义组成模型的形式相似性，我们创建了与量子电路有自然映射的句子表示。我们使用这些表示来实现并成功训练两个NLP模型，在量子硬件上解决简单的句子分类任务。我们以NLP研究人员易于理解的方式详细描述了这些实验的主要原理、过程和挑战，从而为实际的量子自然语言处理铺平了道路。</p></blockquote><p>关键词：量子自然语言处理，100句数据集，文本分类</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>量子计算带来的计算速度比目前的标准高出几倍，正在迅速发展成为计算机科学中最受欢迎的前沿领域之一。虽然直到最近，量子计算的大部分工作都是纯理论的或与经典硬件上的模拟有关，但第一批可供研究人员使用的量子计算机(被称为噪声中尺度量子(NISQ)设备)的出现已经导致了一些有前途的实际结果和应用，涵盖了广泛的主题，如密码学(Pirandola等人，2020年)，化学(Cao等人。和生物医学(Cao et al.， 2018)。</p><p>一个显而易见的问题是，这是否是新的计算范式也可以用于自然语言处理。这种适用性可能会一直持续到杠杆化结束与语言相关的计算加速问题，以及如何研究量子系统，它们的数学描述和信息被“定量”编码的方式可能导致概念上和实践上的进步处理语言的意义计算加速。</p><p>受这些前景的启发，量子自然语言处理(QNLP)，一个仍处于起步阶段的研究领域，旨在开发明确设计为在量子硬件上执行的NLP模型。在这方面有一些令人印象深刻的理论工作，但所提出的实验是经典的模拟。一个值得注意的例外是两位作者(Meichanetzidis et al.， 2020)最近的工作，他们首次在量子硬件上进行了非常小规模的概念证明实验。</p><p>在这篇论文中，我们提出了两个完整的中等规模的实验，包括在量子硬件上运行的语言动机NLP任务。这些实验的目标不是要证明在NLP任务中与经典实现相比的某种形式的“量子优势”;我们认为，由于目前可用的量子计算机的能力有限，这是不可能的。在这项工作中，我们最感兴趣的是向NLP社区提供一个详细的说明，说明QNLP在实践中需要什么。我们展示了传统的建模和编码范式如何转变为量子友好的形式，并探索了当前NISQ计算机所带来的挑战和限制。</p><p>从NLP的角度来看，这两项任务都涉及某种形式的句子分类:每一种句子在数据集中，我们使用组合Coecke等人(2010)的模型-通常被称为分布构成范畴-计算状态向量，即转换为二进制标签。对模型进行训练一个标准的二元交叉熵目标，使用被称为Simultaneous PerarXiv的优化技术:2102.12846 v1 [cs。2021年2月25日扰动随机逼近(SPSA)。的附带了从量子运行中获得的结果通过各种经典模拟表明系统的预期长期行为没有硬件限制。</p><p>选择DISCOCAT的动机是，它产生的导数本质上形成了一个张量网络，这意味着它们已经非常接近量子计算机处理数据的方式。此外，该模型对语法和语义之间的相互作用进行了严格的处理，并使用了方便的图表语言。在第5节中，我们将看到生成的图表如何自然地转换为量子电路——量子计算机的基本计算单元以及不同语法结构的句子如何映射到不同的量子电路。我们还将解释张量收缩(复合函数)如何以量子门的形式表达。我们进一步讨论了噪声在NISQ器件上的作用，以及它如何影响我们对电路的设计选择。实验在IBM量子体验平台提供的IBM NISQ计算机上进行。</p><p>在我们的实验中，我们使用了两个不同的数据集。第一个句子(130个句子)是由一个简单的上下文无关语法自动生成的，其中一半句子与食物有关，一半与IT有关(一个二进制分类任务)。另一个(105个名词短语)是从RELPRON数据集中提取的(Rimell et al.， 2016)，模型的目标是预测一个名词短语是否包含基于主语的关系从句或基于对象的关系从句(同样是一个二元分类任务)。我们证明了模型平滑地收敛，并且在量子和模拟运行中都产生了良好的结果(给定数据集的大小)。综上所述，本文的贡献如下:首先，我们深入概述了在量子计算机上训练和运行NLP模型的过程、技术性和挑战;其次，我们提供了一个强有力的概念证明，量子NLP是在我们的范围内。</p><p>本文的结构如下讨论了最重要的相关工作实验量子计算与QNLP;部分3描述DISCOCAT;第4节规定量子计算入门;部分5给出了一般QNLP的高级概述管道;第6节解释了任务;第7节提供了实验所需的所有细节最后;第8节总结了我们的发现和指向未来的工作。</p></blockquote><p>量子计算，计算速度</p><p>量子自然语言处理，量子硬件，</p><p>DISCOCAT，接近量子计算机处理的数据</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Robin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri Kartsaklis, Bob Coec</summary>
      
    
    
    
    
    <category term="默认标签" scheme="https://levyya.github.io/tags/%E9%BB%98%E8%AE%A4%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>Jsonnet 语法</title>
    <link href="https://levyya.github.io/2023/01/11/Jsonnet-%E8%AF%AD%E6%B3%95/"/>
    <id>https://levyya.github.io/2023/01/11/Jsonnet-%E8%AF%AD%E6%B3%95/</id>
    <published>2023-01-11T09:28:00.000Z</published>
    <updated>2023-01-12T08:15:05.936Z</updated>
    
    <content type="html"><![CDATA[<h2 id="支持"><a href="#支持" class="headerlink" title="支持"></a>支持</h2><ul><li>字符串格式化</li><li>数组拼接，切片</li><li>python 表达式</li><li>条件表达式<ul><li><code>if condition then [] else []</code></li></ul></li><li>变量<ul><li>::  隐藏字段</li><li>local</li><li>self  当前对象</li><li>$  根对象</li></ul></li><li>函数，与python函数类似，可以有默认值</li><li>对象<ul><li>super</li><li>+ (覆盖组合)</li><li>+: (组合，不覆盖)</li></ul></li><li>import</li></ul><h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><ol><li>多行注释: &#x2F;* *&#x2F;</li><li>单行注释: &#x2F;&#x2F;</li></ol><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">local batch_size = 32;</span><br><span class="line">local cuda_device = 2;</span><br><span class="line">local num_epochs = 2;</span><br><span class="line">local seed = 42;</span><br><span class="line"></span><br><span class="line">local embedding_dim = 768;</span><br><span class="line">local hidden_dim = 128;</span><br><span class="line">local dropout = 0.2;</span><br><span class="line">local lr = 0.00001;</span><br><span class="line">local output_dim = 128;</span><br><span class="line">local model_name = &#x27;RoBERTa&#x27;;</span><br><span class="line">local ptm_name = </span><br><span class="line">  if model_name == &#x27;RoBERTa&#x27;</span><br><span class="line">  then &#x27;roberta-base&#x27;</span><br><span class="line">  else &#x27;bert-base-cased&#x27;;</span><br><span class="line">  </span><br><span class="line">local data_dir = &#x27;/workspace/Wei_lai/NLP/Mine_Project/NLP/Mine_Project/AllenNLP/Learning/Baseline_Allennlp/data/&#x27;;</span><br><span class="line">local get_train_path(task_name=&#x27;SST&#x27;) = </span><br><span class="line">  if task_name == &#x27;SST&#x27; </span><br><span class="line">  then &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-train&#x27;</span><br><span class="line">  else data_dir + task_name + &#x27;/&#x27; + task_name + &#x27;_train.txt&#x27;;</span><br><span class="line">local get_val_path(task_name=&#x27;SST&#x27;) = </span><br><span class="line">  if task_name == &#x27;SST&#x27; </span><br><span class="line">  then &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-dev&#x27;</span><br><span class="line">  else data_dir + task_name + &#x27;/&#x27; + task_name + &#x27;_test.txt&#x27;;</span><br><span class="line"></span><br><span class="line">local SST2_train_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-train&#x27;;</span><br><span class="line">local SST2_dev_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Binary/sentiment-dev&#x27;;</span><br><span class="line">local SST5_train_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Fine-Grained/sentiment-train&#x27;;</span><br><span class="line">local SST5_dev_path = &#x27;/workspace/Wei_lai/NLP/data/SST/Fine-Grained/sentiment-dev&#x27;;</span><br><span class="line"></span><br><span class="line">// 如果数据集为CR, MPQA, MR, SUBJ, 使用get_train_path</span><br><span class="line">// local task_name = &#x27;SUBJ&#x27;;</span><br><span class="line">// local train_path = get_train_path(task_name);</span><br><span class="line">// local val_path = get_val_path(task_name);</span><br><span class="line"></span><br><span class="line">// 如果数据集为SST2，SST5，使用根变量引用</span><br><span class="line">local train_path = SST5_train_path;</span><br><span class="line">local val_path = SST5_dev_path;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  numpy_seed: seed,</span><br><span class="line">  pytorch_seed: seed,</span><br><span class="line">  random_seed: seed,</span><br><span class="line">  </span><br><span class="line">  dataset_reader: &#123;</span><br><span class="line">    type: &#x27;my_dataset_reader&#x27;,</span><br><span class="line">    tokenizer: &#123;</span><br><span class="line">      type: &#x27;pretrained_transformer&#x27;,</span><br><span class="line">      model_name: ptm_name</span><br><span class="line">    &#125;,</span><br><span class="line">    token_indexers: &#123;</span><br><span class="line">      tokens: &#123;</span><br><span class="line">        type: &#x27;pretrained_transformer&#x27;,</span><br><span class="line">        model_name: ptm_name</span><br><span class="line">      &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">  &#125;,</span><br><span class="line">  datasets_for_vocab_creation: [&#x27;train&#x27;],</span><br><span class="line">  train_data_path: train_path,</span><br><span class="line">  validation_data_path: val_path,</span><br><span class="line">  </span><br><span class="line">  model: &#123;</span><br><span class="line">    type: &#x27;text_classifier&#x27;,</span><br><span class="line">    embedder: &#123;</span><br><span class="line">      token_embedders: &#123;</span><br><span class="line">        tokens: &#123;</span><br><span class="line">          type: &#x27;pretrained_transformer&#x27;,</span><br><span class="line">          model_name: ptm_name</span><br><span class="line">        &#125;,</span><br><span class="line">      &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    encoder: &#123;</span><br><span class="line">      type: &#x27;bert_pooler&#x27;,</span><br><span class="line">      pretrained_model: ptm_name</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  data_loader: &#123;</span><br><span class="line">    shuffle: true,</span><br><span class="line">    batch_size: batch_size,</span><br><span class="line">  &#125;,</span><br><span class="line">  trainer: &#123;</span><br><span class="line">    cuda_device: cuda_device,</span><br><span class="line">    num_epochs: num_epochs,</span><br><span class="line">    optimizer: &#123;</span><br><span class="line">      lr: lr,</span><br><span class="line">      type: &#x27;adamw&#x27;,</span><br><span class="line">    &#125;,</span><br><span class="line">    validation_metric: &#x27;+fscore&#x27;,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;支持&quot;&gt;&lt;a href=&quot;#支持&quot; class=&quot;headerlink&quot; title=&quot;支持&quot;&gt;&lt;/a&gt;支持&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;字符串格式化&lt;/li&gt;
&lt;li&gt;数组拼接，切片&lt;/li&gt;
&lt;li&gt;python 表达式&lt;/li&gt;
&lt;li&gt;条件表达式&lt;ul&gt;
&lt;</summary>
      
    
    
    
    
    <category term="Jsonnet" scheme="https://levyya.github.io/tags/Jsonnet/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--Quantum-Inspired Complex-valued Language Models...</title>
    <link href="https://levyya.github.io/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/"/>
    <id>https://levyya.github.io/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/</id>
    <published>2023-01-10T14:03:22.000Z</published>
    <updated>2023-01-11T09:31:56.887Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href>click</a></p><p>作者：</p><blockquote><p>Qin Zhao 1,† , Chenguang Hou 2,† and Ruifeng Xu 1,3,*</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><blockquote><p>基于方面的情感分析(ABSA)是一种细粒度的情感分析方法，其目的是对面向方面的极性进行分类。当前模型的向量表示通常受限于实际值。基于量子理论的数学公式，量子语言模型越来越受到人们的关注。这样的模型中的单词可以投射为量子系统中的物理粒子，并且自然地由希尔伯特空间中具有丰富表示的复值向量表示，而不是实值向量。本文研究了ABSA模型的Hilbert空间表示，构造了三个强实值基线的复化。实验结果证明了复值嵌入的有效性和复值嵌入模型的卓越性能，说明复值嵌入可以携带超出真实嵌入的额外信息。特别是，复值RoBERTa模型在三个标准基准数据集上优于或接近先前的最先进技术。</p></blockquote><p>关键词：量子语言模型；复数值网络；方面级情感分析</p><p>后面重点关注复值RoBERTa的构建</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><blockquote><p>基于方面的情感分析(Aspect-Based Sentiment Analysis, ABSA)是一种细粒度的情感分析任务，其目的是将句子的情感极性按一个或多个方面进行分类[1-5]。ABSA任务中涉及的基本情感要素有方面类别、方面项、意见项和情感极性。方面类别定义了方面术语应该属于哪个类别。例如，在餐厅领域中，“食物”和“服务”是方面类别。方面项是在给定文本中显示的意见目标。意见术语表达了一个人对方面术语的看法。情感极性描述了情感在一个方面类别或方面术语上的取向，通常是积极的、消极的或中性的。例如，在“the food was great and the service was severe slow”这句话中，第一个方面词是“food”，属于“food”范畴，意见词是“great”。因此，“食物”方面术语和“食物”方面类别的情感极性为正。第二个方面的术语是“服务”，属于“服务”范畴，意见术语是“慢”。因此，方面术语“服务”和方面类别“服务”的情感极性是消极的。ABSA通常由两个子任务组成:方面提取(AE)和方面级情感分类(ALSC)。本文只关注ALSC，即预测不同方面术语在上下文中的确切情感极性，而不是在句子层面或文档层面上对整体情感极性进行分类。也就是说，我们的任务是在给定的上下文中对方面术语的情感极性进行分类，比如上面例子中提到的“食物”和“服务”。这种面向方面的情感极性分类可以更好地研究评论中细粒度的情感倾向，从而为决策者提供更准确的建议。</p><p>此前，为了避免设计手工制作的特征，大量基于深度学习的神经网络模型被提出来解决ALSC任务，如基于rnn的模型[6-9]，基于cnn的模型[10,11]。为了更好地对方面级情感进行分类，在构建神经网络时也考虑了目标信息。注意机制在图像识别、机器翻译、句子摘要等方面已被证明是有效的[12-14]，在ALSC任务[9]中也引入了注意机制，考虑词与方面的关系，动态计算不同词的注意权重。最近，预训练的基于变压器的模型在各种NLP任务中占据主导地位，在ALSC任务中也受到了很多关注。基于BERT和roberta的模型在各种ABSA基准数据集上取得了突出的成功[15-19]。</p><p>然而，在大多数模型中使用的向量表示都被限制为真实值。复值向量是一个广泛应用于信号处理、量子物理、医学图像处理等各个领域的基本概念，它是由一对正交维上相互关联的实向量和虚向量组成。由于复值向量具有更丰富的表示能力，复化神经网络已被应用于信号处理[20,21]、计算机视觉[22]、自然语言处理[23]等众多领域。从量子理论的角度，将多维实值输入向量映射到频率空间或波向量空间，可以自然地表示为复值输入向量。</p><p>受量子理论启发的量子语言模型(QLMs)具有与复值向量兼容的特性，受到越来越多的关注。在量子语言模型中，每个单词都自然地表示为希尔伯特空间中量子系统中的一个观测状态，并由义素的叠加表示[24,25]。从这样的量子视角出发，将量子算符与神经网络计算进行类比，用量子物理学发展的方法来弥补NLP中神经网络可解释性的不足，可以更有原则地构建神经网络。基于此背景，Li et al.[25]构造了用于问答任务的复值网络，Zhao et al.[26]提出了基于量子期望值的语言模型。受量子语言模型的出色工作及其与复值向量的兼容性的激励，我们研究使用量子物理数学框架中的复值表示来解决ABSA任务。为此，通过引入语义希尔伯特空间构建复值神经网络，其中ABSA模型的一个词被视为物理状态，编码为复值向量。为了对结果模型的性能进行基准测试，每个模型都根据其相应的实值基线进行评估。在本研究中，我们构建了三个强实基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。从数学角度看，实向量空间V的复化是通过取V与复数的张量积来定义的。这里，实值基线的复化遵循类似的操作。值得注意的是，通过将复值向量的虚部设为零，复值模型只是缩小到它们的实值等价。</p><p>在三个基准数据集(即Twitter、Restaurant 14和Laptop 14)上的实验结果表明，我们的复值嵌入模型表现出色，并说明复值嵌入可以携带超出真实嵌入的额外信息。具体而言，LSTM模型的复值版本和基于注意的LSTM模型都优于原始模型。同时，复值RoBERTa模型在三个标准基准上优于或接近于之前的SOTA模型。结果表明，由量子物理粒子表示扩展而来的复杂化有可能被封装到ABSA的通用语言模型中。</p></blockquote><h3 id="基于方面的情感分析"><a href="#基于方面的情感分析" class="headerlink" title="基于方面的情感分析"></a>基于方面的情感分析</h3><p>（Aspect-Based Sentiment Analysis, ABSA）</p><p>基本情感要素：方面类别、方面项、意见项、情感极性</p><p>两个子任务：方面提取（AE）；方面级情感分类（ALSC）</p><h3 id="复数值向量"><a href="#复数值向量" class="headerlink" title="复数值向量"></a>复数值向量</h3><p>由一对<strong>正交</strong>维上<strong>相互关联</strong>的实向量和虚向量组成</p><p>优势：具有更丰富的表示能力</p><p>应用：信号处理、计算机视觉、自然语言处理</p><p>与量子理论的联系：将多维实数值输入向量映射到频率空间或波向量空间</p><h3 id="量子语言模型"><a href="#量子语言模型" class="headerlink" title="量子语言模型"></a>量子语言模型</h3><p>每个单词表示为希尔伯特空间中量子系统的一个观测状态，并由义素的叠加表示</p><p>优势：用量子物理学发展的方法，可以弥补NLP中神经网络解释性不足的问题</p><p>相关工作：Li 构造用于问答任务的复值网络；Zhao 量子期望值语言模型</p><h3 id="该文研究内容"><a href="#该文研究内容" class="headerlink" title="该文研究内容"></a>该文研究内容</h3><ol><li><p>构建了三个模型：复值LSTM；复值注意力LSTM；复值BERT&#x2F;RoBERTa</p></li><li><p>实验数据集：Twitter；Restaurant 14; Laptop 14</p></li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><blockquote><p>本节将介绍ALSC任务和QLMs的相关研究。</p><p>为了解决ALSC任务，循环神经网络和卷积神经网络是最常用的深度神经网络架构。结合目标词的信息，Tang et al.[8]首次提出了两种单向LSTMs (TD-LSTMs)，分别独立处理目标词的左右上下文。随后，在ALSC任务中引入了注意机制。MemNet由Tang et al.[27]设计，它使用多跳注意力来揭示每个上下文单词相对于方面目标的重要性。Wang等[9]设计了ATAE-LSTM，以方面词嵌入和上下文词嵌入的串联为输入，应用注意机制动态计算注意权重。Chen等人发布的内存循环注意(Recurrent Attention on memory, RAM)是一种双向LSTM，利用具有记忆层的多重注意机制来获取全局语义特征。</p><p>近年来，基于变压器的模型在各种NLP任务中占据主导地位，ALSC任务也引起了广泛的关注。Song et al.[15]提出的BERT- SPC是一种纯BERT文本对分类模型，并取得了优异的性能。通过预先训练的BERT提供输入词嵌入，他们同时设计了一个注意编码器网络(AEN)，该网络可以利用注意机制获得语义词-上下文交互。Yang等[16]提出了一种基于多头自注意的局部上下文焦点(LCF)机制。BERT- ADA表明，通过在任务相关语料库[17]上进行微调，可以进一步改进只适应特定任务的预训练BERT。</p><p>第一个QLM是由Sordoni, Nie和Bengio[29]在信息检索(IR)中提出的，将Hilbert空间简化为实空间。受到他们工作的启发，人们对量子语言模型进行了广泛的研究[24,30]。受限于真实空间，Zhang et al.[24]提出了一种端到端的基于神经网络的类量子语言模型(Quantum-like Language Model, NNQLM)来解决问答任务。在该模型中，每个单词都被视为系统中的一个纯量子态，问答句分别由其对应的密度矩阵表征。Zhang et al.[31]采用张量积来描述整个词序列之间的相互作用，建立了一个仅采用实值嵌入的量子多体波函数启发的语言模型。</p><p>值得注意的是，上述模型中使用的向量表示仅局限于实值神经网络。特别是对于qlm，他们将希尔伯特空间过度简化为真正的子空间。因此，后来又提出了复值量子语言模型，将量子系统的物理状态恰当地表示为复值函数。在问答任务中，复值量子语言模型已被成功应用，并证明了复杂形式的单词表示在额外的虚数部分的支持下，可以提供比真实单词更多的信息。在这些模型中，Li et al.[25]建立了复值匹配网络(complex-value Network for Matching, CNM)，其中每个词在极坐标系统中编码为一个复值向量，其长度和方向分别表示相对词权和叠加。Zhao等人[26]提出了一种基于量子期望值的语言模型。在此框架下，语言模型具有良好的可解释性和性能。李等人。研究了复值神经网络在视频情感分析任务中的应用，其模型与目前最先进的模型[32]取得了相当的效果。</p><p>然而，据笔者所知，复值框架还没有在ABSA中得到任何应用。因此，受量子语言模型令人兴奋的工作及其与复值向量的兼容性的启发，我们从量子物理的数学框架中研究了复值表示在ABSA任务中的应用。</p></blockquote><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>首先介绍方面级情感分类的一些工作：TD-LSTMs，RAM（Atthention），BERT-SPC，BERT-ADA；</p><p>然后介绍QLM相关工作：NNQLM，量子多体波函数语言模型，CNM，量子期望值语言模型。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>此处简单介绍了一下量子计算理论基础，翻译略。</p><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224306577.png" class alt="image-20230110224306577"><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><blockquote><p>复值语言模型受量子语言模型的启发，一个词被视为量子系统中的一个物理可观测物，它由一个复值向量表示。在此范围下，将三个均以复值嵌入为输入的QLMs进一步构造为三个强实值基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。将得到的三种复值模型与相应的实值模型进行比较，以衡量其性能。通过对比，我们希望看到嵌入的虚部可以携带超出实部的额外信息，进一步强调引入量子语言模型的重要性。首先，我们愿做一个澄清。本文基于三个典型的实基线，建立了三个复值语言模型。然而，除了被选中的，还有大量不同类型的神经网络用于语言任务。我们希望这三种类型的模型能够阐明复值结构对提高模型性能的影响，并显示进一步探索其他类型神经网络结构的可能性。</p><p>在本节中，我们首先介绍了实向量空间的复化，将单词编码为复值向量的过程，然后介绍了构造三个实基线的复化的方法。</p></blockquote><h3 id="复数值语言模型"><a href="#复数值语言模型" class="headerlink" title="复数值语言模型"></a>复数值语言模型</h3><h4 id="Complex-valued-LSTM"><a href="#Complex-valued-LSTM" class="headerlink" title="Complex-valued LSTM"></a>Complex-valued LSTM</h4><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224831764.png" class alt="image-20230110224831764"><h4 id="Complex-Valued-Attention-Based-LSTM"><a href="#Complex-Valued-Attention-Based-LSTM" class="headerlink" title="Complex-Valued Attention-Based LSTM"></a>Complex-Valued Attention-Based LSTM</h4><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224820862.png" class alt="image-20230110224820862"><h3 id="Complex-Valued-BERT-x2F-RoBERTa-Model"><a href="#Complex-Valued-BERT-x2F-RoBERTa-Model" class="headerlink" title="Complex-Valued BERT&#x2F;RoBERTa Model"></a>Complex-Valued BERT&#x2F;RoBERTa Model</h3><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110224804949.png" class alt="image-20230110224804949"><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验数据集"><a href="#实验数据集" class="headerlink" title="实验数据集"></a>实验数据集</h3><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110225048766.png" class alt="image-20230110225048766"><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><img src="/2023/01/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quantum-Inspired-Complex-valued-Language-Models/image-20230110225134938.png" class alt="image-20230110225134938"><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>这篇工作是利用复数值网络构建了三个模型：C-LSTM，C-ATAE-LSTM，C-BERT&#x2F;C-RoBERTa，实验通过方面级情感分析来验证模型结果。</p><p>与我提出来的ComplexQNN的区别：</p><ul><li>相同点：都利用了复数值网络去构建量子启发式模型</li><li>不同点：ComplexQNN尝试构建了量子系统模块，如词语以量子态表示，句子以密度矩阵表示，中间特征提取通过演化（维度不发生改变），最后结果通过测量得到预测结果</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Qin Zhao 1,† , Chenguang Hou 2,† and Ruifeng Xu 1,3,*&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;摘要&quot;&gt;&lt;</summary>
      
    
    
    
    
    <category term="论文" scheme="https://levyya.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>hexo图片加载问题</title>
    <link href="https://levyya.github.io/2023/01/09/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E9%97%AE%E9%A2%98/"/>
    <id>https://levyya.github.io/2023/01/09/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E9%97%AE%E9%A2%98/</id>
    <published>2023-01-09T11:50:18.000Z</published>
    <updated>2023-01-10T12:35:53.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Typora设置"><a href="#Typora设置" class="headerlink" title="Typora设置"></a>Typora设置</h2><p>修改图像保存路径为相对路径（以文件名为路径）</p><img src="/2023/01/09/hexo%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD%E9%97%AE%E9%A2%98/image-20230109195354353.png" class alt="image-20230109195354353"><h2 id="Hexo设置"><a href="#Hexo设置" class="headerlink" title="Hexo设置"></a>Hexo设置</h2><p>修改hexo根路径下配置文件<code>_config.yml</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure><p>（创建博客时，自动创建对应存放图片的文件夹）</p><h2 id="安装两个插件"><a href="#安装两个插件" class="headerlink" title="安装两个插件"></a>安装两个插件</h2><ol><li><p>hexo-asset-image</p><ol><li>使用下面命令安装</li></ol><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>此时生成的html链接中，图片路径不对，需要修改该模块的index.js内容</p><ol start="2"><li>替换node_modules&#x2F;hexo-asset-image&#x2F;index.js</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> cheerio = <span class="built_in">require</span>(<span class="string">&#x27;cheerio&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">getPosition</span>(<span class="params">str, m, i</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> str.<span class="title function_">split</span>(m, i).<span class="title function_">join</span>(m).<span class="property">length</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> version = <span class="title class_">String</span>(hexo.<span class="property">version</span>).<span class="title function_">split</span>(<span class="string">&#x27;.&#x27;</span>);</span><br><span class="line">hexo.<span class="property">extend</span>.<span class="property">filter</span>.<span class="title function_">register</span>(<span class="string">&#x27;after_post_render&#x27;</span>, <span class="keyword">function</span>(<span class="params">data</span>)&#123;</span><br><span class="line">  <span class="keyword">var</span> config = hexo.<span class="property">config</span>;</span><br><span class="line">  <span class="keyword">if</span>(config.<span class="property">post_asset_folder</span>)&#123;</span><br><span class="line">    <span class="keyword">var</span> link = data.<span class="property">permalink</span>;</span><br><span class="line"><span class="keyword">if</span>(version.<span class="property">length</span> &gt; <span class="number">0</span> &amp;&amp; <span class="title class_">Number</span>(version[<span class="number">0</span>]) == <span class="number">3</span>)</span><br><span class="line">   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">   <span class="keyword">var</span> beginPos = <span class="title function_">getPosition</span>(link, <span class="string">&#x27;/&#x27;</span>, <span class="number">3</span>) + <span class="number">1</span>;</span><br><span class="line"><span class="comment">// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span></span><br><span class="line"><span class="keyword">var</span> endPos = link.<span class="title function_">lastIndexOf</span>(<span class="string">&#x27;/&#x27;</span>) + <span class="number">1</span>;</span><br><span class="line">    link = link.<span class="title function_">substring</span>(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> toprocess = [<span class="string">&#x27;excerpt&#x27;</span>, <span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;content&#x27;</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i = <span class="number">0</span>; i &lt; toprocess.<span class="property">length</span>; i++)&#123;</span><br><span class="line">      <span class="keyword">var</span> key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      <span class="keyword">var</span> $ = cheerio.<span class="title function_">load</span>(data[key], &#123;</span><br><span class="line">        <span class="attr">ignoreWhitespace</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">xmlMode</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">lowerCaseTags</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">decodeEntities</span>: <span class="literal">false</span></span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(<span class="string">&#x27;img&#x27;</span>).<span class="title function_">each</span>(<span class="keyword">function</span>(<span class="params"></span>)&#123;</span><br><span class="line"><span class="keyword">if</span> ($(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>))&#123;</span><br><span class="line"><span class="comment">// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span></span><br><span class="line"><span class="keyword">var</span> src = $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>).<span class="title function_">replace</span>(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>);</span><br><span class="line"><span class="keyword">if</span>(!<span class="regexp">/http[s]*.*|\/\/.*/</span>.<span class="title function_">test</span>(src) &amp;&amp;</span><br><span class="line">   !<span class="regexp">/^\s*\//</span>.<span class="title function_">test</span>(src)) &#123;</span><br><span class="line">  <span class="comment">// For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span></span><br><span class="line">  <span class="comment">// In addition, to support multi-level local directory.</span></span><br><span class="line">  <span class="keyword">var</span> linkArray = link.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">var</span> srcArray = src.<span class="title function_">split</span>(<span class="string">&#x27;/&#x27;</span>).<span class="title function_">filter</span>(<span class="keyword">function</span>(<span class="params">elem</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> elem != <span class="string">&#x27;&#x27;</span> &amp;&amp; elem != <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line">  &#125;);</span><br><span class="line">  <span class="keyword">if</span>(srcArray.<span class="property">length</span> &gt; <span class="number">1</span>)</span><br><span class="line">srcArray.<span class="title function_">shift</span>();</span><br><span class="line">  src = srcArray.<span class="title function_">join</span>(<span class="string">&#x27;/&#x27;</span>);</span><br><span class="line">  $(<span class="variable language_">this</span>).<span class="title function_">attr</span>(<span class="string">&#x27;src&#x27;</span>, config.<span class="property">root</span> + link + src);</span><br><span class="line">  <span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;update link as:--&gt;&quot;</span>+config.<span class="property">root</span> + link + src);</span><br><span class="line">&#125;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>(<span class="string">&quot;no src attr, skipped...&quot;</span>);</span><br><span class="line"><span class="variable language_">console</span>.<span class="property">info</span>&amp;&amp;<span class="variable language_">console</span>.<span class="title function_">info</span>($(<span class="variable language_">this</span>));</span><br><span class="line">&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.<span class="title function_">html</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>此时可以使用语法<code>&#123;% asset_img img_name.jpg img_description %&#125;</code> 来正常使用图片，但是用markdown语法还是无法显示。</p></li><li><p>hexo-simple-image</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure><p>安装完这个插件后，可以使用原生markdown图片语法（<code>![]()</code>）</p></li></ol><h2 id="参考致谢"><a href="#参考致谢" class="headerlink" title="参考致谢"></a>参考致谢</h2><ol><li><a href="https://cloud.tencent.com/developer/article/1600295?from=article.detail.1702112">https://cloud.tencent.com/developer/article/1600295?from=article.detail.1702112</a></li><li><a href="https://willern.gitee.io/2019/10/14/20191014/">https://willern.gitee.io/2019/10/14/20191014/</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Typora设置&quot;&gt;&lt;a href=&quot;#Typora设置&quot; class=&quot;headerlink&quot; title=&quot;Typora设置&quot;&gt;&lt;/a&gt;Typora设置&lt;/h2&gt;&lt;p&gt;修改图像保存路径为相对路径（以文件名为路径）&lt;/p&gt;
&lt;img src=&quot;/2023/01/</summary>
      
    
    
    
    <category term="Hexo" scheme="https://levyya.github.io/categories/Hexo/"/>
    
    
    <category term="Hexo" scheme="https://levyya.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>图片上传测试</title>
    <link href="https://levyya.github.io/2023/01/09/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95/"/>
    <id>https://levyya.github.io/2023/01/09/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95/</id>
    <published>2023-01-09T09:07:45.000Z</published>
    <updated>2023-01-10T12:35:54.012Z</updated>
    
    <content type="html"><![CDATA[<img src="/2023/01/09/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95/image-20230109170831787.png" class title="description"><img src="/2023/01/09/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95/image-20230109170831787.png" class alt="image-20230109170831787">]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;/2023/01/09/%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%E6%B5%8B%E8%AF%95/image-20230109170831787.png&quot; class title=&quot;description&quot;&gt;

&lt;img s</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>github本地项目上传</title>
    <link href="https://levyya.github.io/2022/12/06/github%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0/"/>
    <id>https://levyya.github.io/2022/12/06/github%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E4%B8%8A%E4%BC%A0/</id>
    <published>2022-12-06T07:24:42.000Z</published>
    <updated>2023-01-10T12:35:53.934Z</updated>
    
    <content type="html"><![CDATA[<p>查看系统</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/version</span><br><span class="line"></span><br><span class="line"># 查看内核</span><br><span class="line">uname -a</span><br><span class="line"></span><br><span class="line"># 查看系统版本信息的命令</span><br><span class="line">lsb_release -a</span><br></pre></td></tr></table></figure><p>连接github</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add [origin] git@github.com:xxxx</span><br><span class="line"></span><br><span class="line">查看所有远程仓库</span><br><span class="line">git remote -v</span><br></pre></td></tr></table></figure><p>拉取文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get fetch [origin]</span><br><span class="line">git merge name/branch</span><br></pre></td></tr></table></figure><p>添加文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m &quot;first commit&quot;</span><br></pre></td></tr></table></figure><p>第一次上传</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git push -u [origin] main</span><br><span class="line">git push --set-upstream name master</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">umount命令进行卸载</span><br><span class="line">sudo umount /dev/sda1</span><br><span class="line"></span><br><span class="line">确认这个分区的文件系统是什么</span><br><span class="line">sudo fdisk -l</span><br><span class="line"></span><br><span class="line">fsck -t ext4 /dev/sda1</span><br><span class="line">-t参数是指明文件系统是什么。/dev/sda1则是指定分区。</span><br><span class="line"></span><br><span class="line">另外一种输入法, fsck -t ext4这个命令就是在调用fsck.ext4这个命令。</span><br><span class="line">参数-f，让fsck对于没有错的档案也强行检测。这样大约可以修复一些分区的轻微的错误吧。</span><br><span class="line">fsck还有检测硬盘坏道的功能，参数是-c</span><br><span class="line">fsck.ext4 /dev/sda1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>进入单用户模式</p><p>ro -&gt; rw init&#x3D;&#x2F;sysroot&#x2F;bin&#x2F;sh</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># df -hT /data/</span><br><span class="line">umount /dev/sda1</span><br><span class="line">fsck.ext4  /dev/sda1</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;查看系统&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="github" scheme="https://levyya.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>ComplexQNN</title>
    <link href="https://levyya.github.io/2022/12/05/ComplexQNN/"/>
    <id>https://levyya.github.io/2022/12/05/ComplexQNN/</id>
    <published>2022-12-05T09:24:03.000Z</published>
    <updated>2023-02-21T07:57:42.102Z</updated>
    
    <content type="html"><![CDATA[<div style="font-family:verdana;font-size: 22px"> ComplexQNN: A Complex-valued Quantum-inspired Language Model </div><h2 id="中文版"><a href="#中文版" class="headerlink" title="中文版"></a>中文版</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>问题：现有的量子启发式模型都是基于振幅相位嵌入，将词语映射到希尔伯特空间中。而量子计算理论中，量子态对应的向量都是复数值，所以需要使用欧拉公式进行转换。</p><p>想法：复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等。实际上，复数值神经网络在表示能力上要优于实数值神经网络，适合对复杂的自然语言进行建模。另一方面，量子启发式模型是定义在希尔伯特空间下的，它同样是一个复数空间，因此可以很自然地基于复数值神经网络构建量子启发式模型。于是我们提出了一种新的模型ComplexQNN，它可以在实部和虚部嵌入异构的语义信息，从而提升模型的表达能力。</p><p>实验：我们在六个情感分类数据集上进行了实验。相比于一些经典的模型如ELMo，BERT和RoBERTa，ComplexQNN能够取得竞争性的结果。</p><hr><h3 id="第一章-引言"><a href="#第一章-引言" class="headerlink" title="第一章 引言"></a>第一章 引言</h3><blockquote><p>引用：”NLP Meets Quantum Physis”</p><p>量子启发式神经网络模型和现有的神经网络模型之间的主要区别在于，前者使用描述语言特征的量子理论的数学框架，然后将量子力学概念来描述的这些特征作为神经网络的输入。</p><p>使用量子力学概念来描述特征具有更好的可解释性，因为他们具有更透明的物理解释，也更有利于后续的神经网络提取有用的信息。</p><p>适用性，借鉴量子力学的数学框架，而处理数据时不受量子计算操作的限制。</p></blockquote><p>情感分析方法经历了早期的机器学习算法到目前流行的深度学习算法，早期的机器学习方法通过特征选择算法进行文本的特征向量学习，如词袋模型（Bag of Words, BOW)，词频-逆文档频率（Term Frequency-Inverse Document Frequency，TF-IDF），这些方法考虑了词语的出现频率，但是丢失了词语的顺序和上下文信息。目前的深度学习方法基于连续词袋模型（CBOW）和Skip-gram，通过词嵌入层（Word Embedding）将离散的词语映射到高维向量空间，再连接其它网络模块进行特征学习，能够保留文本的上下文信息。按处理的文本序列长度分类，可分为句子级情感分类和篇章级。</p><p>量子启发式模型是结合了量子计算理论和深度学习理论构造的一种新的神经网络模型。它是在NISQ时代量子硬件发展受限提出的一种启发式方法，借助量子计算理论的思想，而采用经典的计算方式实现。量子启发式语言模型是对自然语言进行建模，它受到自然语言和量子系统微粒的相似性启发，如语言的一词多义现象和微粒的叠加态，语言随时间、空间变化而变化和微粒随时间改变不断演化，语言在特定场景下语义被确定和微粒被测量后坍塌到某种基态等。量子启发式语言模型相比于经典的神经网络模型，量子启发式语言模型更加符合自然语言的特性，具有更好的可解释性和指数级加速的潜能，而且量子启发式语言模型是定义在希尔伯特空间下的，它具有更强的表示能力。</p><p>近年来，自然语言处理（Natural Language Processing, NLP）受益于深度学习的发展，在包括情感分析、问题回答、机器翻译、文本生成等诸多领域取得重大进展。循环神经网络（RNN）以及长短期记忆（LSTM）和循环门控单元（GRU），可以学习上下文之间的关联信息，是NLP中常用的神经网络模块，不过循环神经网络存在梯度爆炸和梯度消失的问题，难以构建深度神经网络。Transformer基于注意力机制（Self-Attention）和残差结构，能够关注到数据中的重点信息并且记忆前面学习的知识，非常适合构建深度神经网络。自然语言处理目前使用的主要技术是基于Transformer构建的预训练语言模型，常见预训练语言模型有BERT、RoBERTa、GPT等。这些预训练模型可用作文本编码器，再通过微调策略，可以轻易适应不同的下游任务。目前，NLP研究方向开始朝向更大的数据集，更大的模型方面发展，像GPT-3，训练的数据集规模达到750GB，参数量更是高达1750亿。虽然计算能力在逐年提升，但面对如此庞大的数据集以及参数量超千亿的模型，研究者仍然会感到束手束脚。</p><p>量子计算是一种全新的计算理论，已经有工作证明在一些任务上，量子计算机具有指数级计算复杂度加速优势，比如著名的Shor算法，能够以多项式复杂度完成整数的质因子分解，这威胁到了经典的基于RSA加密算法的通信过程。当前也有一些工作基于量子计算理论对自然语言建模，这主要有两方面原因，一是人类语言和量子系统具有很多的相似性，比如语言的歧义性和量子叠加态，语言的进化性和量子态的演化；二是量子计算具有潜在的指数级加速优势，这对目前需要花费大量资源训练网络的预训练方法来说非常有吸引力。目前已经有一些关于量子自然语言处理的工作，如剑桥的Bob Coecke 提出DisCoCat以及lambeq，将自然语言编码成字符串后，再编码为量子线路，通过参数化量子线路学习，可以实现文本分类任务。现阶段，量子机器学习的实验进展因为量子计算设备中量子比特与纠错能力的限制还停留在初步阶段。因此，运行在真实的量子计算机上，能够处理的文本规模还很小，只能处理包含10几个词的100条句子规模的数据集任务。另一方面，还有研究者基于量子计算理论数学框架构建量子启发式模型，这种算法模型不需要运行在真实的量子计算机上，只是借鉴了量子计算理论中的相关概念来帮助自然语言建模，因此不会受到硬件发展的束缚。</p><p>量子启发式模型是基于量子计算理论的数学框架对自然语言进行建模，将自然语言类比于量子系统，同时使用经典神经网络模型去模拟这一过程。然而，现有的量子语言模型如NNQLM，ICWE等，是通过振幅相位编码模拟量子态的构建，再利用欧拉公式得到量子态的复数值表示。实际上，直接使用复数值神经网络进行构建量子启发式模型也是可行的。有许多研究者在研究复数值神经网络，Chiheb Trabelsi et al. 提出了深度复数值卷积网络，并在图像分类、音乐转录和语音频谱预测任务取得了不错的性能。</p><p>正是考虑到量子计算理论是定义在希尔伯特空间上，量子态对应的向量表示中每一个维度都是复数值，而且很少有工作基于复数值神经网络来构造。因此，为了更加合理地构建量子启发式模型，以及丰富模型的表达能力，我们基于复数值神经网络，提出了复数值量子启发式模型（ComplexQNN），省略了振幅相位的模拟过程，简化了量子启发式模型的开发。我们基于提出的模型进行了广泛实验，使用多个情感分类数据集对模型进行测评，并和经典的模型进行比较，证明了我们模型的有效性。</p><p>综上所述，我们总结了这项工作的核心内容：完全基于复数神经网络构建了一个新的量子启发式模型ComplexQNN，它更符合量子计算理论。情感分类任务上的实验结果验证了ComplexQNN的有效性，在6个情感分类数据集上的实验结果证明，ComplexQNN具有良好的性能，与经典的TextCNN、BiLSTM、ELMo、BERT和RoBERTa相比都有竞争性的优势。</p><p>本文的其余部分组织如下。第2节概述了背景和相关工作。启发式程序在第3节中介绍，并在第4节中评估和讨论。第五节对全文进行了总结。</p><hr><h3 id="第二章-相关工作"><a href="#第二章-相关工作" class="headerlink" title="第二章 相关工作"></a><strong>第二章 相关工作</strong></h3><p>这一章，我们会介绍与复数值量子启发式神经网络有关的知识，包括量子计算,复数值神经网络,和量子启发式神经网络的研究进展。</p><p>量子计算是基于量子力学而非经典物理学的思想的一种新型计算方式。1984年，David Deutsch首次提出通用量子计算机的概念，量子计算机从速度上对经典计算机有本质的超越。传统计算机可以模拟量子计算机，但是效率不是很高。1994年，Peter Shor提出量子计算机可以解决寻找整数的素因子问题和解决所谓离散对数问题。1995年，Lov Grover证明量子计算机上可以加速在没有结构的搜索空间上进行搜索的问题。</p><p>我们将介绍量子计算中的一些基本概念，如量子态、量子系统、量子态演化和量子测量。</p><p><strong>量子机器学习</strong></p><p>将数据编码与纠缠量子线路相结合，通过测量来推理数据实例的类标签。对主体线路进行传统的特征描述和存储，并对线路中的参数进行混合量子或传统训练。</p><p>量子线路由单量子比特旋转门和双量子比特受控旋转门组成，其中旋转门中的旋转角度是可学习的参数。旋转门和受控旋转门对于量子计算来说是通用的，任何酉矩阵都可以分解成由一组特定量子门构成的线路。</p><p><strong>量子启发式模型</strong></p><p>Sordoni等人于2013年提出了量子语言模型，这是量子概率理论在自然语言处理的第一个实际应用。QLM将量子理论和自然语言处理语言模型相关研究结合了起来，在理论上具有重大的意义。然而，QLM在许多方面具有局限性。比如，term的表示采用的是one-hot形式的向量。相比于现在常用的分布式词向量(distributed representation), one-hot向量没有能力考虑文本的全局语义信息，会占用更多的存储空间，同时也由于自身独特的表示形式也会造成大量存储资源的浪费。其次，QLM难以将通过迭代计算得到的密度矩阵嵌入到一个端到端的可以通过反向传播算法进行训练的神经网络当中，因而很难得到实际的应用。2018年，Peng Zhang等人在QLM的基础之上，利用类量子力学理论建立深度学习网络，提出端到端的类量子神经网络模型(End-to-End Quantum-like Language Models, NNQLM)来完成NLP中的问答匹配任务，是最早的将类量子力学理论和深度学习技术结合，并实现端到端的神经网络模型在NLP中的一次应用。然而，由于模型采用的是实数词向量，因而未能真正意义上地模拟量子微粒量子态，也没能够充分利用量子力学密度矩阵的概率属性。因此NNQLM的研究和实现目的只是将量子力学和NLP的相关理论进行推广。</p><p>同样是2018年，Li等人采用复数词向量模拟量子态，提出量子启发式复数词嵌入，并结合量子力学理论基础，使用密度矩阵的形式表示文本语句，使用投影测量来对密度矩阵形式的文本语句进行观察测量，使用测量得到的概率值来推断文本语句的极性，进而完成NLP中的文本分类任务。使用复数词向量表示的密度矩阵完全契合了量子力学理论，基于量子力学基础提出的网络模型提高了网络在自然语言处理任务的可解释性。同时，相比于一些经典的机器学习模型(比如Unigram-TFIDF)，基于复数基向量模拟量子态设计实现的模型在五个英文二分类的数据集上具有更加优异的表现性能。因此，将量子力学理论和深度神经网络相结合的模型是时下的研究热点之一。2019年，Benyou Wang等人又在使用复数词向量模拟量子态的基础之上，实现端到端的神经网络模型来完成问答匹配任务，Benyou Wang等人分别使用复数形式的密度矩阵表示问句和答句，并分别对密度矩阵形式的问句和答句进行投影测量，最后基于投影测量得到的概率值来计算问句和答句的相似性，从而筛选出问题的正确答案。</p><p>2020年，Jiang和Zhang提出了一种量子干涉启发的神经网络匹配模型（QINM）用于处理信息提取任务，可以把干涉现象嵌入到信息提取过程中，实验结果显示要优于先前提出的量子启发式信息提取模型和一些神经网络信息提取模型。</p><p>2021年，Peng Zhang等人提出了TextTN，一种基于量子理论构建的文本张量网络，用于处理文本分类任务。TextTN可分为两个子模型，首先用将词生成张量网络（word-GTN）将词语编码为向量，随后用句子判别张量网络（sentence-DTN）对句子进行分类。Yazhou Zhang等人提出一种复数值模糊神经网络用于对话讽刺识别，将量子理论成功与模糊逻辑理论结合在一起。Jinjing Shi et al. 提出两种端到端的量子启发式深度神经网络ICWE-QNN 和 CICWE-QNN用于文本分类。模型采用了GRU，CNN和注意力机制改进了量子启发式模型，可以解决CE-Mix模型中忽略文本内部语言特性的问题。</p><blockquote><p>在量子复杂词嵌入的激励下，提出了可解释复值词嵌入(ICWE)，设计了两个端到端的量子深度神经网络(ICWE- qnn和CICWE-QNN代表基于ICWE的卷积复值神经网络)用于二进制文本分类。它们在NLP应用中具有被证明的可行性和有效性，可以解决CE-Mix[1]模型中由于忽略文本的重要语言特征而导致的文本信息丢失的问题，因为我们的模型采用深度学习算法进行语言特征提取，其中门控循环单元(GRU)提取句子的序列信息，注意机制使模型关注句子中重要的词，卷积层捕捉投影矩阵的局部特征。ICWE-QNN模型避免了词性符号的随机组合，CICWE-QNN充分考虑了投影矩阵的文本特征。在5个基准分类数据集上的实验表明，所提模型比CaptionRep Bow、DictRep Bow和para - phrase等传统模型有更高的准确率，并且在f1评分上也有很好的表现。特别是对于SST、SuBJ、CR和MPQA四组数据集，CICWE-QNN模型的精度均高于量子激励模型CE-Mix。设计量子深度神经网络以提高文本分类性能是一种有意义和有效的探索。</p></blockquote><p>量子启发式算法只是使用量子理论的数学框架，所以不用运行在真实的量子计算机上，在实用性上要比量子算法好。近年来不断涌现的量子启发式模型应用在NLP表明，这个研究方向是可行的。相较于神经网络模型，量子启发式算法的优势在于能赋予模型物理意义，使模型具有更好的可解释性。此外，量子启发式算法还能嵌入如量子干涉、量子纠缠等量子特性到模型中，进而增强模型的学习能力。</p><p>综上，将结合量子力学理论的神经网络应用到NLP领域是可行的。同时，现在的量子启发式模型很少基于复数值神经网络来构造，没有充分利用量子计算理论。这也是我们做这项工作的原因。</p><p><strong>1. 量子启发式模型</strong></p><p><strong>(1)量子态构建</strong></p><p>自然语言中的单词对应量子系统中的单个微粒，如同微粒存在上旋和下旋状态，以狄拉克符号可表示为$|\psi\rangle &#x3D; \alpha|\uparrow\rangle + \beta |\downarrow\rangle$,单词存在不同的词义或情感意义，如”long”可以表示一段遥远的时间记为$|S_1\rangle$,也可以表示相对较大的空间属性记为$|S_2\rangle$，单词”long”可用量子态表示为$|long\rangle &#x3D; \alpha |S_1\rangle + \beta |S_2\rangle$，其中$\alpha, \beta$都是复数，它们的模长$|\alpha|^2, |\beta|^2$表示对应量子基态的概率，且满足$|\alpha|^2+|\beta|^2&#x3D;1$。</p><p>对于任意一个单词$t$，其量子态可以表示为<br>$$<br>|t\rangle &#x3D; \sum w_i |e_i\rangle &#x3D; \sum \alpha_i e^{i\beta_i} |e_i\rangle<br>$$<br>上式中$w_i$为语义权重系数，$|e_i\rangle$为该单词的第$i$个语义，$\alpha_i, \beta_i$为$w_i$的极坐标表示，$\alpha_i$为振幅，$\beta_i$为相位。这样的量子态表示中，振幅表示的为语义信息，相位表示的是隐含的特征信息，如情感信息或者多义词信息。基态$|e_i\rangle$为基本的语义。最终，单词表示为多种基态词义构成的叠加态，可以表示词语的不确定性。</p><p><strong>(2)特征嵌入</strong></p><p>量子启发式模型相比于经典的NLP模型，一个显著区别就是引入了复数词向量。在实验中，复数词向量是通过构建振幅嵌入层和相位嵌入层来实现的，利用欧拉公式$re^{i\theta} &#x3D; r(cos\theta + i sin\theta)$，可以分别得到复数的实部和虚部系数。在先前的工作中，振幅表示语义信息，相位表示隐含特征，振幅往往采用Glove词向量，相位则是按Xavier正态分布随机初始化。这里限制了复数词向量的表示能力，且随机初始化的结果不适用于不同的任务。</p><p>我们改进相位的初始化方式，基于不同的下游任务，为相位嵌入层赋予不同的初始化参数。例如，处理情感分析任务中，相位嵌入层的初始化参数为包含情感信息的相位特征；处理词义消歧任务中，相位嵌入层的初始化参数为包含一次多义信息的相位特征。这种针对任务嵌入的特征能够显著提高模型的效果，在第4节实验结果能够看到这一点。要获得这样的特征参数，可以考虑两种方式：（a）选择公开的大规模语料库预训练词向量，如word2vec，Glove等，然后映射到$[0, 2\pi]$上；（b）训练一个朴素的量子启发式模型，抽取复数词向量中相位嵌入层的参数。在我们的实验中，我们采用的是第二种方式，因为第一种预训练词向量往往是固定词嵌入维度的，而第二种方式可以根据需求自定义词嵌入的维度大小。</p><p><strong>(3)量子态演化</strong></p><p>量子理论中，表示量子系统的量子态是随时间不断变化的，数学上表示为$|\psi\rangle&#x3D;U|\psi\rangle$，其中$U$是一个酉矩阵$U^\dagger U&#x3D;I$。这一特点也同样存在于自然语言中，某些词语会因为一些社会历史原因改变其原本的含义，如”silly”起源于古英语”saelig”，意为“快乐的、无忧无虑的”，而当今的意义已经变成“愚蠢的”。 这种语义的变化可以通过大型语料库训练学习，也可以根据文本上下文信息推测得到。在神经网络模型中，循环神经网络如长短期记忆（LSTM）、门控单元（GRU），能够学习时序数据中较长的内容，非常适合处理文本这样的前后相关联的数据。可使用循环神经网络模型如GRU来模拟量子态的演化，一个词语的量子态会因周围词语变化而改变，这种变化过程可用GRU模拟。GRU将先前词语的状态和当前词语的状态作为输入，中间经过选择性记忆门操作，输出开始至当前词语的状态向量。</p><p><strong>(4)量子态测量</strong></p><p>前面我们得到了一个句子中每个词语对应的量子态，记为$S&#x3D;{|t_i\rangle}$，下面我们需要得到整个句子的表示。量子系统中，多个微粒能够构成一个混合系统，通常表示为密度矩阵$\rho&#x3D;\sum w_i |\phi_i\rangle\langle \phi_i|$。同样的，我们将一个句子表示为密度矩阵$\rho&#x3D;\sum w_i |t_i\rangle \langle t_i|$，其中$w_i$表示的是第$i$个单词的权重系数，默认为$w_i&#x3D;\frac{1}{l}$（$l$为句子长度）。这一权重系数也可以通过注意力机制，学习到适合任务的权重。</p><p>得到表示句子的密度矩阵之后，我们将对句子进行测量操作。首先，我们选择一组满足完备性（$\sum M_m^\dagger M_m &#x3D; I$）的测量基$M_m &#x3D; |\lambda_m\rangle \langle \lambda_m|$。之后，我们利用该组测量基对密度矩阵进行测量，测量结果为$p(m)&#x3D;\langle \lambda_m| \rho |\lambda_m\rangle &#x3D; tr(\rho |\lambda_m\rangle \langle \lambda_m|)$。</p><p><strong>(5)分类器</strong></p><p>根据上一个过程得到的测量特征，我们使用一个全连接层将结果映射到一个结果集合大小的向量空间，并使用softmax激活函数将结果表示为预测概率。</p><p><strong>2. 复数值神经网络</strong></p><p>现有的深度学习技术绝大多数是基于实数值运算和表示的，实际上，复数可能具有更丰富的表示能力，有工作证明复数值神经网络具有一些独特的优势：有可能实现更容易地优化，更好的泛化特征，更快的学习，以及允许噪声鲁棒的记忆机制。2018年，Chiheb Trabelsi等人提出深度复数值神经网络，提出了复数值批量归一化和复数值权重初始化等训练复数值神经网络的关键模块，还提出了复数值卷积神经网络架构，并通过图像分类、音乐转录以及语音频谱预测实验进行了模型效果验证。下面简单介绍复数值神经网络的原理，包括复数值表示、复数值激活函数以及复数值卷积神经网络三部分内容：</p><p><strong>复数值线性层</strong></p><p>线性层也叫全连接层，每个神经元都和上一层所有神经元相连，是神经网络中最常见的网络结构。其计算公式描述为<br>$$<br>f&#x3D;WX+b<br>$$<br>其中$W$表示网络中的权重矩阵，$b$表示网络层中的偏置。C. Trabelsi et al.基于PyTorch库实现了复数值线性层。复数值线性层使用两个实数值线性层分别计算实部和虚部，在输出时基于复数计算原理得到新的实部和虚部。具体计算公式如下所示：<br>$$<br>f_r(X) &#x3D; W_r X + b_r\<br>f_i(X) &#x3D; W_i X + b_i\<br>f_c&#x3D; f_r(X_r) - f_i(X_i) + \bold{i} [f_r(X_i) + f_i(X_r)] \<br>   &#x3D; W_rX_r - W_iX_i + b_r - b_i + \bold{i}[W_rX_i + W_iX_r + b_r + b_i]<br>$$<br>复数值线性层是复数值神经网络的基本模块，后面复数值卷积神经网络和复数值循环神经网络都依赖这个模块。</p><p><strong>复数值激活函数</strong></p><p>We call Complex ReLU (CReLU) the complex activation that applies separate ReLUs on both of the real and the imaginary part of a neuron, i.e:<br>$$<br>CReLU(z) &#x3D; ReLU(R(z)) + i ReLU(I(z))<br>$$</p><p><strong>复数值批正则化</strong><br>$$<br>BN(\hat{x})&#x3D; \gamma \hat{x} + \beta<br>$$</p><p>深度网络通常依赖于批处理归一化(Ioffe和Szegedy, 201s)来加速学习。在某些情况下，批归一化是优化模型的必要条件。批处理归一化的标准公式只适用于实际值。在本节中，我们提出了一个可用于复值的批量归一化公式。要将一组复数标准化为标准正态复数分布，仅仅平移和缩放它们，使它们的均值为o，方差为l是不够的。这种归一化不能确保实分量和虚分量的方差相等，得到的分布也不能保证是圆形的;它将是椭圆形的，可能具有高离心率。相反，我们选择将这个问题视为二维矢量的美白问题，这意味着将数据按两个主成分方差的平方根缩放。这可以通过将以o为中心的数据(æ - E])乘以2x2协方差矩阵V的平方根倒数来实现</p><p><strong>复数值循环神经网络</strong></p><p>循环神经网络不同于线性层，每层的输出都依赖于前面的输出，能够学习前面数据中的信息。RNN常用来处理序列数据，在自然语言处理中有广泛的应用。LSTM和GRU是两种常用的循环神经网络，我们以复数值LSTM为例介绍复数值循环神经网络。如图1所示，LSTM中含有多个门：遗忘门、输入门和输出门，可以选择性地让信息通过。LSTM的计算过程如下所示：<br>$$<br>f_t &#x3D; \sigma(W_{xf} * X_t + W_{hf} * H_{t-1} + b_f), \<br>    i_t &#x3D; \sigma(W_{xi} * X_t + W_{hi} * H_{t-1} + b_i), \<br>    o_t &#x3D; \sigma(W_{xo} * X_t + W_{ho} * H_{t-1} + b_o), \<br>    \widetilde{c_t} &#x3D; \tanh(W_{xc} * x_t + W_{hc} * h_{t-1} + b_c), \<br>    c_t &#x3D; f_t \bigodot c_{t-1} + i_t \bigodot \widetilde{c_t}, \<br>    h_t &#x3D; o_t \bigodot \tanh(c_t)<br>$$<br>其中$\bigodot$表示向量点乘，$f_t$表示遗忘门，用于丢弃过去不重要的信息，$i_t$表示输入门，$o_t$表示输出门，$\widetilde{c_t}$表示候选记忆单元，$c_t$表示输出的细胞状态，$H_t$表示输出的隐状态。复数值LSTM需要将遗忘门、输入门和输出门使用复数值线性层实现。Sigmoid激活函数以及tanh函数以及向量乘法也要使用复数值计算。这些基于复数值线性层都是比较容易实现的。</p><p><strong>复数值卷积神经网路</strong></p><p>卷积神经网络常用在计算机视觉领域，如LeNet，AlexNet，VGG，ResNet，Yolo等，也有用于自然语言处理的TextCNN。卷积神经网络通常包括卷积层、池化层和全连接层。其中卷积层是计算机神经网络的核心，一个卷积层通常包括多个尺寸一致的卷积核，卷积核的个数决定输出的大小。类似于经典的CNN，复数值卷积神经网络也包含复数值卷积层。复数值卷积层包含实部卷积与虚部卷积，计算过程如图1所示。$M_R$和$M_I$分别为实部特征图和虚部特征图，$K_R$和$K_I$分别为实部卷积核和虚部卷积核。复数值卷积层的输出为$M_RK_R-M_IK_I+\bold{i}(M_RK_I+M_IK_R)$。</p><p><strong>3. 我们的想法</strong></p><p>量子启发式模型现有方法都是基于振幅相位嵌入，将词语映射到希尔伯特空间中，而量子计算理论中，量子态对应的向量都是复数值的，所以需要使用欧拉公式进行转换。复数值神经网络已经有一些研究，但是其实际应用还很少，更不必说在自然语言的下游任务如情感分析和语言模型等。实际上，复数值神经网络在表示能力上要优于实数值神经网络，适合对复杂的自然语言进行建模。另一方面，量子启发式模型是定义在希尔伯特空间下的，它同样是一个复数空间，因此可以很自然地基于复数值神经网络构建量子启发式模型。于是我们提出了一种新的模型ComplexQNN，它可以在实部和虚部嵌入异构的语义信息，从而提升模型的表达能力。我们在情感分类数据集上进行了实验，并和基线模型对比，实验结果现实ComplexQNN能够很好地完成任务，并且相比于XX模型有xx%的精度提升。</p><hr><h3 id="第三章-正文"><a href="#第三章-正文" class="headerlink" title="第三章 正文"></a>第三章 正文</h3><p><strong>数据集</strong></p><p>情感二分类数据集</p><p>CR, MPQA, MR, SST, SUBJ</p><p>情感多分类数据集</p><p>SST-5</p><p>【方面级情感分类】</p><p><strong>特征提取以及数据表示</strong></p><p>文本分词</p><p>TF-IDF</p><p>word embedding</p><p>GloVe</p><p>Contextual embedding</p><blockquote><p>《Quantum-Inspired Complex-Valued Language Models for Aspect-Based Sentiment Classification》</p><ol start="4"><li><p>Complex-Valued Language Models</p><p>受量子语言模型的启发，一个词被视为量子系统中的一个物理可观测物，它由一个复值向量表示。在此范围下，将三个均以复值嵌入为输入的QLMs进一步构造为三个强实值基线的复化，即复值LSTM模型、基于复值注意的LSTM模型和复值BERT&#x2F;RoBERTa模型。将得到的三种复值模型与相应的实值模型进行比较，以衡量其性能。通过对比，我们希望看到嵌入的虚部可以携带超出实部的额外信息，进一步强调引入量子语言模型的重要性。首先，我们愿做一个澄清。本文基于三个典型的实基线，建立了三个复值语言模型。然而，除了被选中的，还有大量不同类型的神经网络用于语言任务。我们希望这三种类型的模型能够阐明复值结构对提高模型性能的影响，并显示进一步探索其他类型神经网络结构的可能性。在本节中，我们首先介绍了实向量空间的复化，将单词编码为复值向量的过程，然后介绍了构造三个实基线的复化的方法。</p></li></ol></blockquote><p><strong>ComplexQNN</strong></p><p><strong>理论部分</strong></p><p>我们提出的复数值量子启发式模型同样也是基于量子计算的数学理论，因此模型中的模块都是定义在希尔伯特空间下。量子启发式模型是对自然语言以量子信息的方式进行建模的，因此首先需要将词语表示为量子态。在单个原子模型中，电子可以处于基态或者是激发态，或是介于两者之间的叠加态。类似的，自然语言因一词多义现象，同样可以表示为叠加态。比如词语$w$，具有$n$种不同的语义 ($n&#x3D;2^m, m\ge0$)，记为$e_i$，那么该词的量子态表示为<br>$$<br>|w\rangle &#x3D; \sum_{i&#x3D;1}^n \alpha|e_i\rangle<br>$$<br>其中$\alpha$是$n$维复向量，而$|\alpha_i|^2$表示词语$w$表示词义$e_i$的概率。以n&#x3D;4为例，<br>$$<br>|w\rangle &#x3D; \sum_{i&#x3D;1}^4 \alpha |e_i\rangle \<br>&#x3D; \alpha_{00}|e_{0}\rangle + \alpha_{01}|e_{1}\rangle + \alpha_{10}|e_{2}\rangle + \alpha_{11}|e_{3}\rangle\<br>&#x3D; [\alpha_{00}, \alpha_{01}, \alpha_{10}, \alpha_{11}] \left[\begin{array}{c} |00\rangle \ |01\rangle \|10\rangle \|11\rangle \\end{array}\right] \<br>&#x3D; \left[\begin{array}{c} \alpha_{00}\ \alpha_{01}\ \alpha_{10}\ \alpha_{11} \end{array}\right]<br>$$<br>从公式可以看到，词语$w$的量子态被映射到$n$维复向量空间中。</p><p>一句话通常是多个词语组成的，正如一个量子系统由多个微观粒子组成。一个量子系统在量子计算中通常用密度矩阵来表示。假设句子中含有$m$个词语，那么一个句子$S$的密度矩阵表示为<br>$$<br>\rho &#x3D; |S\rangle\langle S| &#x3D; \sum_{i&#x3D;1}^m\beta|w_i\rangle (\sum_{i&#x3D;1}^m\beta|w_i\rangle)^\dagger<br>$$<br>其中$\beta$是$n$维复向量，$|\beta_i|^2$表示词语$w_i$在句子$S$中的权重。类似于注意力机制，不同的权重有利于神经网络关注句子中重要的词语。在情感分类中，一些形容词如“棒”、“糟糕”、“优秀”等，对最终的预测结果有很大的影响，可以分配较大的权重。</p><p>句子被表示为密度矩阵之后，我们希望进一步学习句子内部词语的联系。对应于量子系统，这一操作叫做演化，即量子态随时间或是其它外界干扰而发生改变，通过公式表示为<br>$$<br>\rho^\prime&#x3D;U\rho<br>$$<br>其中，$U$是一个$n\times n$的复值矩阵，$\rho\prime$是经过演化后的系统状态。过去的量子启发式语言模型通常是抽取密度矩阵$\rho$中的实部和虚部，使用循环神经网络或者卷积神经网络分别进行训练，最后整合输出的特征。我们认为这一操作会割裂量子系统中的信息，会导致学习到的特征不完整，不能正确地模拟量子系统状态的改变。因此，在构造量子启发式模型时，我们通过复数值神经网络来模拟量子态的改变，整个演化过程将基于复数值进行计算，输出的结果也将保持复数状态。</p><p>最后，量子计算中的测量可以得到量子系统坍塌到一组基态下的概率值，这一过程应用到自然语言处理中的文本分类任务上。假设有一组测量算子${M_i},i\in{1,…,k}$，表示$k$种分类标签，那么句子对应第$i$个标签的测量概率为<br>$$<br>p_i &#x3D; \rho^\dagger M_i^\dagger M_i \rho<br>$$<br>。我们将模型应用到情感分析任务中，进行句子情感极性的验证，任务包含二分类和多分类，在第四章可以看到实验的细节。</p><p><strong>模型细节</strong></p><p>复数值量子启发式神经网络（ComplexQNN）是基于复数值神经网络和量子计算理论构建的语言模型，用于自然语言处理的下游任务，下面将从模型整体架构、模型具体模块的构造、模型适用的场景三个方面介绍该模型。</p><p>首先是ComplexQNN的模型整体架构，图片1描述了模型的整体架构，它包含三个模块：复数词嵌入、量子编码器、分类器。此外，我们还可以从 <strong>图片1</strong> 中了解到ComplexQNN处理文本的整个流程：</p><p>0）预处理过程：文本首先通过预处理过程（大小写转换、分词、词索引映射、填充和截断）得到令牌序列（token），另一方面，为了屏蔽掉填充序列带来了额外令牌序列，还需要构建由0和1组成的掩码序列（mask），令牌序列和掩码序列即为ComplexQNN的输入数据；</p><p>ComplexQNN的体系结构如图\ref{fig_ComplexQNN}所示。可以看到，它由四个模块组成:复杂嵌入，投影，演化和分类器。ComplexQNN的输入数据首先需要经过大小写转换、分词、词索引映射、填充和截断等预处理得到。此外，为了屏蔽填充序列带来的额外token序列，还需要构造由0和1组成的掩码序列。综上所述，token序列和mask序列是ComplexQNN的输入数据。下面介绍ComplexQNN的四个基本模块:</p><p>1）复数词嵌入：复数词嵌入层的作用是将词语对应的令牌号（这是一个整数对应词语在词表中的位置）映射到n维复数向量空间中，其对应于量子计算中的量子态构建过程，每一个词语从离散空间映射到高维希尔伯特空间下，对应一个复数值列向量。</p><p>2）投影：投影是把句子中多个离散的词语映射到$n\times n$的复数值空间中。上一步骤中，复数词嵌入已经把词语映射到$n$维复向量空间中。通过公式1，可以计算出句子的密度矩阵表示，其中词语的权重$\beta$可以通过注意力机制训练得到，默认所有的词语取相同的权重。</p><p>3）演化：演化过程是模拟量子系统的变化。在量子计算理论中，量子态和密度矩阵的变化是利用量子门实现的。量子门对应一个酉矩阵，矩阵的维度对应其操作的量子比特数（$n&#x3D;2^m$）。在ComplexQNN中，我们要通过复数值线性层、复数值循环神经网络以及复数值卷积神经网络来模拟量子系统的变化。我们在设计演化模块时，令输入和输出的维度都是$n\times n$。因此，在学习句子内部的特征后，不会改变原来量子系统的维度。</p><p>4）分类器：分类器的作用是利用之前模块学习到的高维特征预测分类结果。我们可以基于量子计算理论通过测量直接输出预测结果，如公式1所示。这里需要构造一组线性无关的测量基，基态的个数取决于要进行分类的数量。最终模型预测的结果取概率值最大对应测量基的标签。</p><p>以上，我们介绍了ComplexQNN的四个必要模块，并展示了文本序列从输入到预测输出的过程。接下来，将介绍模块的具体设计。</p><hr><p>ComplexQNN需要包含复数词嵌入、投影、演化以及分类器，其中复数词嵌入和演化是模型构建的核心。在我们的实现中，我们使用Allennlp库设计了三个模块：ComplexEmbedder，QuantumEncoder和Classifier。量子编码层中包含了投影和演化操作。</p><p>ComplexEmbedder是ComplexQNN的第一个模块，它的输入是被预处理后的文本Token序列，这是一个整数向量。复数词嵌入由实部嵌入层和虚部嵌入层组成。文本Token序列分别经过这两个嵌入层，最后通过公式<br>$$<br>[w_1, …, w_i, …, w_n] &#x3D; [r_1, …, r_i, …, r_n] + \bold{i} \times [i_1, …, i_i, …, i_n]<br>$$<br>得到每一个Token的复数词向量表示。实部嵌入层和虚部嵌入层可以方便地使用经典的词嵌入。</p><p>经典的词嵌入层有多种不同的嵌入方式。按照分词的级别，可分为字符级、wordpiece级以及词语级。按训练方式可分为非上下文词嵌入（静态词嵌入）和上下文词嵌入（动态词嵌入）。Word2vec 和 GloVe 是两种经典的非上下文词嵌入方法。上下文词嵌入是基于预训练模型思路，通过微调得到数据集相关的上下文词向量。</p><p>我们考虑了不同的方案来构造复数词嵌入层：（1）使用不同的预训练模型作为实部和虚部的嵌入层，如实部使用BERT，虚部使用RoBERTa，这样就可以将不同的信息编码到量子态中。（2）实部和虚部编码不同种类的文本信息，如实部编码wordpiece级别的词向量，虚部使用NLTK库编码词语极性等语义信息。（3）实部编码正向语序特征，虚部编码反向语序特征。</p><p>上面第一种方法同时使用了不同的预训练模型提取特征，可以取得最好的实验结果，但是训练需要占用很大的显存空间。第二种和第三种方法训练速度很快，但是效果不如预训练模型。综合考虑实验效果以及训练所需要的资源，最终ComplexEmbedder的实现采用实部使用RoBERTa，虚部采用自训练的词嵌入层。总的来说，我们希望实部和虚部嵌入不同类型的文本特征，充分利用复数值神经网络的异构特性，进而提升模型的语义表达能力。</p><p>ComplexQNN的第二个模块是量子编码层，负责投影和演化。前面我们提到过，进行编码前，需要使用投影先计算得到句子的密度矩阵表示，公式展示了计算密度矩阵的过程。演化是模拟量子门操作。这需要满足一些条件，即输入输出维度不变且维度是$2^n$。我们通过以下基本模块来构建我们的编码层：复数值全连接层、复数值循环神经网络和复数值卷积神经网络。我们构建了三种用于编码的中间模块层：复数值深度神经网络编码层、复数值循环神经网络编码层（基于ComplexLSTM）和复数值卷积神经网络编码层。</p><p>如图1所示，我们构造了ComplexTextCNN。该模块的输入为投影后的表示句子的密度矩阵$\rho$，我们使用了卷积核大小为$[3,4,5]$，每种卷积核数量为$2$，步长为$1$。然后通过最大池化提取特征，并将不同维度学习到的特征拼接起来。到目前为止，ComplexTextCNN和经典TextCNN最大的区别就是，所有的计算操作都是在复数值网络中进行计算的。最后，为了得到学习特征后的新的句子密度矩阵表示，我们使用复数值全连接网络层，把向量维度恢复成输入时的维度。通过外积操作，得到和输入一样维度的矩阵形式。</p><p>第三个模块是分类器，它利用量子计算理论的测量方法，预测模型的输出。具体是通过公式1来实现，我们可以设计不同数量的测量基态来进行文本多分类。后面实验中涉及了多分类，需要根据分类结果的种类数确定基向量的个数。最终的分类结果是取预测概率最大的那个作为最终的预测结果。此外，因为测量后的输出是实数值，我们可以取出每一个标签的概率值组成一组预测向量，然后和其它模型的预测结果拼接在一起，实现模型融合。</p><p>ComplexQNN 用于情感分类</p><p>情感分类是自然语言处理中的一项常见任务，旨在预测句子对应的情感极性。本文开展这项任务来验证ComplexQNN的实验性能。同时，对一些经典的网络模型进行比较。图\ref{fig_ComplexQNNforSA}是ComplexQNN用于情感分类任务的流程图，并对数据维度进行了标注。首先对文本进行预处理，将大小写归一化，分词并去除停用词;其次，通过复杂的词嵌入层模拟单词的量子态;然后，量子编码器层用于投影和演化。最后，通过分类器的模拟测量操作输出预测结果。</p><p><strong>评估指标</strong></p><p>在我们的情感分析实验中，五个数据集都是有两种分类结果。因此，我们使用准确率、精度、召回率和F1值作为评估指标去分析实验结果。</p><p>$$<br>Accuracy &#x3D; \frac{TP + TN}{TP + TN + FP + FN}<br>\<br>Precision &#x3D; \frac{TP}{TP + FP}<br>\<br>Recall &#x3D; \frac{TP}{TP + FN}<br>\<br>F1-score &#x3D; 2 \times \frac{Precision \times Recall}{Precision + Recall}<br>$$</p><p>对于多分类任务<br>$$<br>micro-F1 &#x3D; 2 \times \frac{Recall_m \times Precision_m}{Recall_m + Precision_m}<br>\<br>(3分类为例)<br>macro-F1 &#x3D; \frac{F1-score_1 + F1-score_2 + F1-score_3}{3}<br>$$</p><p>macro-F1</p><p><strong>损失函数</strong></p><p><strong>参数设置</strong></p><hr><h3 id="第四章-实验"><a href="#第四章-实验" class="headerlink" title="第四章 实验"></a>第四章 实验</h3><p>情感分析是自然语言处理中的常见任务，任务要求预测给定的句子的情感极性。实验使用了五个常见的数据级如<strong>表（）</strong>所示。为了衡量ComplexQNN模型的性能，我们使用TextCNN、TextGRU、BERT、RoBERTa作为对比模型。具体地实验结果如<strong>表（）</strong>所示。</p><p>对比模型</p><p><strong>GRU</strong></p><p><strong>TextCNN</strong></p><p><strong>ELMo</strong>（Embeddings from Language Models）由双向LSTM作为基本组件构成，以语言模型（Language Model）为训练目标，通过大语料进行预训练，得到通用的语义表示，再迁移到下游的NLP任务中，可以显著提升下游任务的模型性能。ELMo提供了词级别的语义表示，在很多下游任务中表现优异。</p><p><strong>BERT</strong>（Bidirectional Encoder Representations from Transformers）是一种预训练语言模型（pre-trained language model, PLM）。像ELMo和GPT都是自回归模型（Auto Regressive，AR），只能考虑单侧信息，也就是根据上文预测下一个单词，或者根据下文预测前一个单词。而BERT是利用上下文信息，从含噪音的数据中重建原始数据，属于自编码模型（Auto Encoding）。预训练过程中使用了两个任务：掩码语言模型（Masked Language Model）和下一句预测（Next Sentence Prediction）。BERT的输出是句子中每个Token对应的768维的向量，以及一个特殊Token（[CLS]）。</p><p><strong>RoBERTa</strong>（Robustly Optimized BERT Pretraining Approach）是BERT的精细调优版本，使用了更大的模型参数、更大的批处理大小、更多的预训练数据，同时改进了训练方法，去掉了下一句预测任务，使用动态掩码和BPE编码（Byte-Pair Encoding），在实验上取得了比BERT更好的效果。</p><p>实验结果分析：可以看到ComplexQNN相比于流行的预训练模型并不落于下风，在XX数据集上都有更好的预测精度。另外，我们使用了不同的预训练模型以及相同模型的不同尺寸构建ComplexQNN（GloVe, BERT, RoBERTa；BERT-tiny, BERT-base, BERT-large），实验结果如<strong>图（）</strong>所示。</p><blockquote><p>As shown in Tab , Model is superior to most quantum-inspired models and LSTM&#x2F;CNN-based models and achieves three best performances out of the four metrics on Wiki-QA and TREC-QA. However, it under-performed slightly worse than XX on XX metric about XX. The possible reason is that although Model has surpassed most models, models have larger parameter scales to ensure the effectiveness of learning.</p></blockquote><p>消融实验：</p><p>为了验证量子启发式模型以及复数值神经网络的有效性，我们构建了消融实验。我们构建了四个模型：</p><p>（1）经典卷积神经网络（TextCNN）；</p><p>（2）量子启发式模型（QNN）；</p><p>（3）复数值神经网络模型（ComplexNN）；</p><p>（4）复数值量子启发式模型（ComplexQNN）。</p><p><strong>表（）</strong>显示，复数值神经网络和量子启发式模型通常具有和TextCNN近似的实验结果；ComplexQNN具有比以上三个模型更好的性能。</p><table><thead><tr><th>model</th><th>SST-2</th><th>SST-5</th></tr></thead><tbody><tr><td>TextCNN</td><td>80.9</td><td>78.8</td></tr><tr><td>QNN</td><td>78.8</td><td>75.4</td></tr><tr><td>ComplexTextCNN</td><td>75.3</td><td>71.7</td></tr><tr><td>ComplexQNN</td><td>88.4</td><td>83.1</td></tr></tbody></table><p>实验结果（2022.11.28-…）</p><table><thead><tr><th align="left">model</th><th align="right">CR</th><th align="right">MPQA</th><th align="right">MR</th><th align="right">SST</th><th align="right">SUBJ</th></tr></thead><tbody><tr><td align="left">TextCNN</td><td align="right">acc: <strong>78.8</strong><br> prec: 70.2 <br>recall: 72.3<br> f1: 71.2<br>loss: 0.622</td><td align="right">acc: <strong>74.4</strong><br> prec: 72.4<br>recall: 78.8<br> f1: 75.5<br> loss: 0.638</td><td align="right">acc: <strong>75.0</strong><br> prec: 73.2<br> recall: 78.8<br> f1: 75.9<br> loss: 0.527</td><td align="right">acc: <strong>81.4</strong><br> prec: 81.5<br> recall: 80.3<br> f1: 80.9<br> loss: 0.493</td><td align="right">acc: <strong>90.3</strong><br> prec: 92.0<br> recall: 88.3<br> f1: 90.1<br> loss: 0.379</td></tr><tr><td align="left">ComplexTextCNN</td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td align="left">GRU</td><td align="right">acc: <strong>80.1</strong><br> prec: 75.7 <br>recall: 66.7<br> f1: 71.0<br>loss: 0.637</td><td align="right">acc: <strong>84.3</strong><br> prec: 78.3<br>recall: 69.9<br> f1: 73.3<br> loss: 0.424</td><td align="right">acc: <strong>76.0</strong><br> prec: 77.0<br>recall: 74.1<br> f1: 75.5<br> loss: 0.537</td><td align="right">acc: <strong>82.5</strong><br> prec: 81.6<br> recall: 83.1<br> f1: 82.3<br> loss: 0.512</td><td align="right">acc: <strong>91.7</strong><br> prec: 90.7<br> recall: 92.3<br> f1: 91.8<br> loss: 0.381</td></tr><tr><td align="left">ComplexGRU</td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td><td align="right"></td></tr><tr><td align="left">ComplexQNN</td><td align="right">acc: <strong>78.2</strong><br> prec: 66.3 <br>recall: 81.0<br> f1: 72.9<br>loss: 0.913</td><td align="right">acc: <strong>84.4</strong><br> prec: 76.8<br>recall: 71.7<br> f1: 74.2<br> loss: 0.580</td><td align="right">acc: <strong>73.6</strong><br> prec: 70.2<br>recall: 82.0<br> f1: 75.6<br> loss: 0.832</td><td align="right">acc: <strong>80.4</strong><br> prec: 80.4<br> recall: 79.4<br> f1: 79.9<br> loss: 0.623</td><td align="right">acc: <strong>90.1</strong><br> prec: 89.6<br> recall: 90.8<br> f1: 90.2<br> loss: 0.406</td></tr><tr><td align="left">ELMo</td><td align="right">acc: <strong>85.4</strong><br> prec: 80.0<br> recall: 79.8<br> f1: 79.9<br> loss: 0.381</td><td align="right">acc: <strong>90.5</strong><br> prec: 88.2<br> recall: 80.4<br> f1: 84.1<br> loss: 0.250</td><td align="right">acc: <strong>81.0</strong><br> prec: 77.0<br> recall: 88.3<br> f1: 82.3<br> loss: 0.415</td><td align="right">acc: <strong>88.8</strong><br> prec: 89.3<br> recall: 87.6<br> f1: 88.4<br> loss: 0.296</td><td align="right">acc: <strong>94.9</strong><br> prec: 95.6<br> recall: 94.1<br> f1: 94.9<br> loss: 0.140</td></tr><tr><td align="left">BERT</td><td align="right">acc: <strong>88.8</strong><br> prec: 81.8<br> recall: 88.0<br> f1: 85.2<br> loss: 0.289</td><td align="right">acc: <strong>89.5</strong><br> prec: 87.3<br> recall: 77.9<br> f1: 82.3<br> loss: 0.362</td><td align="right">acc: <strong>84.9</strong><br> prec: 86.0<br> recall: 83.3<br> f1: 84.6<br> loss: 0.360</td><td align="right">acc: <strong>88.9</strong><br> prec: 93.0<br> recall: 83.6<br> f1: 88.0<br> loss: 0.298</td><td align="right">acc: <strong>95.2</strong><br> prec: 95.5<br> recall: 94.9<br> f1: 95.2<br> loss: 0.142</td></tr><tr><td align="left">RoBERTa</td><td align="right">acc: <strong>90.4</strong><br> prec: 92.7<br> recall: 79.8<br> f1: 85.8<br> loss: 0.265</td><td align="right">acc: <strong>90.9</strong><br> prec: 85.0<br> recall: 86.1<br> f1: 85.6<br> loss: 0.257</td><td align="right">acc: <strong>89.8</strong><br> prec: 89.1<br> recall: 90.7<br> f1: 89.9<br> loss: 0.304</td><td align="right">acc: <strong>89.8</strong><br> prec: 88.8<br> recall: 90.7<br> f1: 89.7<br> loss: 0.238</td><td align="right">acc: <strong>96.7</strong><br> prec: 97.6<br> recall: 95.8<br> f1: 96.7<br> loss: 0.098</td></tr><tr><td align="left">Complex-QDNN(with <code>bert-tiny</code>)</td><td align="right">acc: <strong>83.7</strong><br> prec: 81.0<br> recall: 71.8<br> f1: 76.2<br> loss: 0.607</td><td align="right"></td><td align="right">acc: <strong>77.4</strong><br> prec: 79.9<br> recall: 73.3<br> f1: 76.4<br> loss: 0.849</td><td align="right">acc: <strong>77.2</strong><br> prec: 71.6<br> recall: 88.6<br> f1: 79.2<br> loss: 0.613</td><td align="right"></td></tr></tbody></table><p><strong>消融实验</strong></p><p>消融实验包括ComplexNN vs NN, ComplexNN vs ComplexQNN</p><p>通过复数值神经网络对比TextCNN、BiLSTM，验证复数值神经网络的有效性；</p><p>通过复数值神经网络对比复数值量子启发式神经网络，验证复数值量子启发式神经网络的有效性。</p><hr><p><strong>讨论</strong></p><p>% 关于ComplexQNN和RoBERTa的对比<br>% 对比主要从当前优势以及未来可能性</p><p>ChatGPT的出现给人们带来极大的震撼，同时它让很多人意识到，基于经典神经网络的自然语言处理技术已经达到了很高的水平。RoBERTa是一个很优秀的模型，在很多的数据集上它的表现都已经达到了人类水平。  因此我们并不意外我们提出的模型ComplexQNN在多个数据集上的结果与RoBERTa接近。下面我们讨论相比于流行的经典模型如RoBERTa，ComplexQNN具有的一些优势，我们总结出以下三点。</p><p>首先来看一下ComplexQNN和RoBERTa的区别。RoBERTa是在BERT模型的基础上进行改进，是多层Transformer结构，能够在实数值空间中进行特征提取。ComplexQNN是定义在希尔伯特空间中，词向量会被表示成复值，在表示空间上要大于实数值空间。实部可以表示上下文的语义信息，虚部可以用来表示语义之外的信息，比如句子中词语的位置信息、词语的情感信息和词语的歧义信息。 与实数空间相比，复数值空间给予深度学习算法更多的表示可能性，有利于拓展模型发展的边界。</p><p>其次经典模型大多是黑盒模型。现有的自然语言模型，将文本映射到向量后，再通过多层神经网络结构，中间向量的含义只能用文本的低维特征和高维特征描述。而基于量子计算的量子启发式模型，是把自然语言看作是一个量子系统：词语被表示为量子态，句子被表示为密度矩阵，句子中词语的相互作用被表示为量子态演化，句子对应的标签表示为量子态测量后坍塌到基态。这给模型带来了物理意义，有利于人们对模型的理解。自然语言的一些特性可以用量子现象解释，如词语的一词多义现象可以很好地用量子纠缠现象表示，这在一定程度上增加了模型的可解释性。</p><p>最后是计算复杂度的问题。从目前来看，要实现量子启发式复数值网络需要两倍的资源(实部和虚部)，但这是因为在经典计算机中模拟量子操作的原因。因为n个量子比特需要用$2^n$个经典比特来模拟(考虑复数值时，需要$2^{n+1}$)，$n$比特量子门需要$2^n\times 2^n$个经典比特来模拟。ComplexQNN设计的神经网络层是基于复数值神经网络，很容易迁移到未来的量子计算机中。而当我们的算法运行在真实的量子计算机中时，花费的存储和计算资源将会指数级减少。现有的量子计算机已经超过100多个量子比特，能处理小规模的自然语言处理数据集上的分类任务，我们期待未来在真实的量子计算机中实现我们提出的算法。</p><p>总言之，相比于经典神经网络模型如RoBERTa，ComplexQNN有更强的表示能力，更好的可解释性，复杂度指数级降低的可能性。</p><p>% [实验结果只是ComplexQNN的一部分，未来和可能性才是我们算法的优势。]</p><h3 id="第五章-结论"><a href="#第五章-结论" class="headerlink" title="第五章 结论"></a>第五章 结论</h3><p>本文中，我们基于复数值神经网络提出了一种新的量子启发式模型ComplexQNN，并在情感分类数据集上验证了模型的有效性。同时，通过消融实验，我们证明复数值量子启发式模型在语言建模上，具有比以往的量子启发式模型和复数值神经网络更加杰出的表现。</p><p>未来的研究方向可以考虑两个方向：第一是编码更深层的语义，比如开发出适合于数据集规模更大的复数值Tranformer网络模块，并应用在更加复杂的场景下，比如机器翻译，推荐系统等；第二是使用量子线路模型构建网络模块，虽然这在NISQ时代能处理的数据集是受限的。</p><hr><p>基金</p><p>This research was funded by the National Natural Science Foundation of China (Grant Nos.61872390, 61972418, 62272483) and the Special Foundation for Distinguished Young Scientists of Changsha (Grant Nos.kq1905058).</p><p>National Natural Science Foundation of China Nos.61872390, Nos.61972418, Nos.62272483</p><p>Special Foundation for Distinguished Young Scientists of Changsha Nos.kq1905058</p><p>This research was funded by the National Natural Science Foundation of China (Grant Nos.61872390, 61972418, 62272483) and the Special Foundation for Distinguished Young Scientists of Changsha (Grant Nos.kq1905058)</p><p>This work was supported by the National Natural Science Foundation of China(Grant Nos.61972418,61872390,61801522), the Natural Science Foundation of Hunan Province(Grant Nos. 2020JJ4750,2019JJ40352), the Special Foundation for Distinguished Young Scientists of Changsha(Grant Nos.kq1905058) and CCF-Baidu Open Fund (NO.2021031).</p><blockquote><p>Conceptualization, Wei Lai, Jinjing Shi and Yan Chang; Methodology, Wei Lai, Jinjing Shi and Yan Chang; Software, Wei Lai and Yan Chang; Validation, Wei Lai, Jinjing Shi and Yan Chang; Formal analysis, Wei Lai, Jinjing Shi and Yan Chang; Investigation, Wei Lai, Jinjing Shi and Yan Chang; Resources, Wei Lai and Jinjing Shi; Data curation, Wei Lai, Jinjing Shi and Yan Chang; Writing – original draft, Wei Lai; Writing – review &amp; editing, Wei Lai and Jinjing Shi; Visualization, Wei Lai and Yan Chang; Supervision, Jinjing Shi and Yan Chang; Project administration, Wei Lai and Yan Chang; Funding acquisition, Jinjing Shi and Yan Chang.</p></blockquote><hr><p>Cover Letter</p><blockquote><p>本文在量子理论的激励下，提出了一种量子预训练特征嵌入方法(QPFE)，设计了两种高效的量子深度神经网络(QPFE- ernie)用于情感分类和词义消歧。在情感分类和词义消歧方面，本文提出的模型对模糊词的叠加状态进行建模，并合并从ERNIE中学习到的语义特征，在情感分类和词义消歧方面都优于之前的量子模型和经典方法BiLSTM和TextCNN。我们相信，本文的上述介绍将使MDPI公理的普通读者感兴趣。</p></blockquote><p>本文基于复数值神经网络构造了一种全新的，更加符合量子计算理论的量子启发式模型，即ComplexQNN。ComplexQNN没有使用振幅相位编码，而是完全基于复数值神经网络构建复数值嵌入层，量子编码层以及测量层。本文在情感分类任务上进行了实验，包含文本二分类和多分类，并且对比于经典的TextCNN，BiLSTM-CRF，以及ELMo、BERT、RoBERTa都有竞争性的优势。</p><hr><p>一些细节：</p><ol><li>量子启发式语言模型（预训练+微调）：基于复数值神经网络 &amp; 预训练语言模型</li><li>方法：Complex encode<ol><li>ComplexPyTorch</li><li>ComplexLinear</li><li>ComplexConv2d</li><li>ComplexBatchNorm</li></ol></li><li>评估：<ol><li>sentiment classification</li><li>GLUE + CLUE</li></ol></li></ol><p>模型构建</p><p>embedder: [batch_sz, seq_len] -&gt; [batch_sz, seq_len, embedding_dim]</p><p>encoder: [batch_sz, seq_len, embedding_dim]</p><ul><li>complexcnn: </li><li>qnn</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处可以考虑给tokens随机加上一个虚部，或者虚部使用另外一种embedding!!!</span></span><br><span class="line"><span class="comment"># TODO</span></span><br><span class="line"><span class="comment"># embedding_type: random, GloVe or Word2vec, contextual embedding</span></span><br><span class="line"><span class="comment"># 思路：a + 1j * b</span></span><br><span class="line"><span class="comment"># tokens = ptm_tokens + 1j * ptm_tokens</span></span><br><span class="line"><span class="comment"># 实现：通过real_embedder将tokens -&gt; real_part, 通过imag_encoder将tokens -&gt; imag_part</span></span><br><span class="line"><span class="comment"># final_encoder = real_part + 1j * imag_part</span></span><br><span class="line"><span class="comment"># 注：这是一种简单的实现方式，另外还可以构造complex-valued embedding，但前者更符合模型构建的意义</span></span><br><span class="line"><span class="comment"># 意义：复数词向量空间/希尔伯特空间能够表示更加复杂的意义（相比于经典实数空间），提高模型的异构性，尝试使用实部、虚部分别编码不同的信息，一方面不同信息可以相互纠缠，另一方面模型表达能力得到提升</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># encoder: complex_tokens -&gt; vector</span></span><br><span class="line"><span class="comment"># Process:</span></span><br><span class="line"><span class="comment"># token: [batch_size, seq_len]</span></span><br><span class="line"><span class="comment"># complex_embedder: ptm_tokens + 1j * ptm_tokens</span></span><br><span class="line"><span class="comment"># token: [batch_size, seq_len, emb_dim]</span></span><br><span class="line"><span class="comment"># 分两种方向：其一量子启发式模型，其二复数值神经网络</span></span><br><span class="line"><span class="comment"># 其一：量子启发式模型</span></span><br><span class="line"><span class="comment"># 前面token对应词语的量子态表示，通过外积、求和（加权），可以得到句子的密度矩阵表示</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, emb_dim, emb_dim]</span></span><br><span class="line"><span class="comment"># 演化：学习文本内部特征，不改变维度 [emb_dim, emb_dim]；gru, cnn, fc, transformer</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, emb_dim, emb_dim]</span></span><br><span class="line"><span class="comment"># 测量：tr(PV), 其中P是测量算子对应的密度矩阵</span></span><br><span class="line"><span class="comment"># prediction: [batch_size, num_labels]</span></span><br><span class="line"><span class="comment"># 其二：复数值神经网络</span></span><br><span class="line"><span class="comment"># encoder: [seq2seq], seq2vec; 具体地，complexcnn, complexgru, complexdnn</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, hidden_size]</span></span><br><span class="line"><span class="comment"># classifer: nn.Linear()</span></span><br><span class="line"><span class="comment"># sequence: [batch_size, num_labels]</span></span><br></pre></td></tr></table></figure><p><strong>Ｑ　＆　Ａ</strong></p><p>Q: datasets processing</p><p>A: Allennlp pipeline.</p><p>Q: CR, MPQA, MR, SUBJ</p><p>A: 先读取所有数据，然后打乱，最后按7:3分割训练集和测试集</p><p>Q: version of BERT, RoBERTa</p><p>A: RoBERTa: <code>roberta-base</code>, BERT: <code>bert-base-cased</code></p><p>Q: RoBERTa 训练出现精度无法提升的问题</p><p>A: 学习率不能调的太高，适合lr&#x3D;1e-5</p><p>Q: ComplexNN 效果不好，解决方法</p><p>A: 解决中…思路：ComplexEmbedding, ComplexCNN, ComplexGRU, ComplexTransformer</p><p><strong>TODO</strong></p><p><strong>表：情感分类数据集描述，对比实验表（总），消融实验表</strong></p><p><strong>图：对比实验结果图</strong></p><p><strong>ComplexQNN</strong></p><p>量子启发式模型：Embedder, Encoder, Classifier</p><ul><li><input checked disabled type="checkbox"> ComplexQNN整体模型<ul><li><input disabled type="checkbox"> 画一张整体模型图</li><li><input checked disabled type="checkbox"> 预处理过程：分词器、实例化、批处理</li><li><input disabled type="checkbox"> Embedder:  BagOfWord, SingleId</li><li><input disabled type="checkbox"> Encoder: </li><li><input disabled type="checkbox"> Classifer: nn.Linear</li></ul></li><li><input checked disabled type="checkbox"> ComplexQNN的每一个模块介绍<ul><li><input disabled type="checkbox"> ComplexEmbedder: real_embedder &amp; imag_embedder</li><li><input disabled type="checkbox"> QuantumEncoder: Projection &amp; Evolution</li><li><input disabled type="checkbox"> Measure: tr(PV)</li></ul></li><li><input checked disabled type="checkbox"> 训练细节<ul><li><input checked disabled type="checkbox"> 硬件信息：2080ti GPU 12G</li><li><input checked disabled type="checkbox"> 环境：py3.8, torch&#x3D;1.12.1, allennlp&#x3D;2.10.1</li><li><input checked disabled type="checkbox"> epoch: [3 - 30]</li><li><input checked disabled type="checkbox"> batch: [16, 32, 64]</li><li><input checked disabled type="checkbox"> optimizer：Adam, Adamw</li><li><input checked disabled type="checkbox"> loss: cross_entropy</li><li><input checked disabled type="checkbox"> metric: accuracy, precision,</li></ul></li></ul><h2 id="English-Version"><a href="#English-Version" class="headerlink" title="English Version"></a>English Version</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h3><p>(最多200words：背景，方法，结果，结论)</p><p>Existing methods of quantum heuristic model are all based on amplitude phase embedding to map words into Hilbert space, while in quantum computing theory, the vectors corresponding to quantum states are complex numerical values, so Euler’s formula is needed for transformation. Complex numerical neural networks have been studied, but their practical applications are few, let alone in the downstream tasks of natural language such as sentiment analysis and language modeling. In fact, the representation capability of complex numerical neural networks is better than that of real numerical neural networks, which is suitable for modeling complex natural languages. On the other hand, quantum heuristic models are defined under Hilbert space, which is also a complex space, so it is natural to construct quantum heuristic models based on complex numerical neural networks. Therefore, we propose a new model ComplexQNN, which can embed heterogeneous semantic information in the real and imaginary parts to improve the representation ability of the model. We conduct experiments on emotion classification data sets, which are compared to the baseline model, and the experimental results show that ComplexQNN is very good at accomplishing the task, and has xx% accuracy improvement compared to xx model.</p><p>key words:（三到十个）</p><p>sentiment analysis; machine learning; natural language processing</p><p>complex neural networks, sentiment analysis, language model, quantum theory.</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>A quick background, question thrown out: NLP has benefited from the development of deep learning and has made significant progress in many areas. The main technology currently in use is a pre-training language model built on transformer that ADAPTS to different downstream tasks through fine-tuning strategies. The research direction began to develop towards larger data sets and larger models. Although the computing power was improved year by year, it still felt constrained. Are there any new computational models that can be modeled more reasonably, save resources and learn more knowledge at the same time?</p><p>Existing solutions and shortcomings: Quantum computing is a brand new computing theory. It has been proved that in some tasks, quantum computers have exponential computational complexity acceleration advantages. There is also currently some work on speech quantum computing theory for modeling natural language, such as the lambeq work at Cambridge. Limited by the NISQ era, the size of the text processed is small, and can only handle data set tasks of 100 sentence size containing 10 or more words.</p><p>Based on what revelation, I came up with my plan, what work I did: trying to benefit from the theory of quantum computing and another way of thinking about it, building quantum heuristic models. The quantum heuristic model is based on the mathematical framework of quantum computing theory to model the natural language, comparing the natural language to the quantum system, and using the classical neural network model to simulate the process. At present, there are some quantum heuristic models such as NNQLM, ICWE, etc., which simulate the construction of quantum states by amplitude phase coding. In fact, quantum states are defined in Hilbert Spaces, where each dimension is a complex value. Therefore, in order to construct a more reasonable quantum heuristic model and enrich the expression ability of the model, we propose complex numerical quantum Heuristic model (ComplexQNN). We have carried out extensive experiments on the proposed model, using multiple sentiment classification data sets to evaluate the model, and compared it with the classical model, which proves the effectiveness of our model.</p><p>My method can be summarized into three points: In summary, we summarize the contents of this work, including the following: Based on complex numerical neural network, we construct a new quantum heuristic model which is more consistent with the theory of quantum computing, namely ComplexQNN. The above model is a theoretical framework. In the actual modeling, the model structure can be adjusted adaptively according to the real scene, which will be elaborated in the third chapter. The experimental results in emotion classification prove that our model has a good effect.</p><ul><li><input disabled type="checkbox"> 加内容</li></ul><hr><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>**Preliminary about Quantum Computation **</p><p>Quantum theory. Quantum state, quantum evolution and quantum measurement.</p><p><strong>Preliminary</strong><br>In quantum physics \cite{dirac1928quantum}, unlike classical physics, the quantum is the smallest unit that cannot be divided, which is the first property of quantum. For instance, the energy of an electron in an atom only can be one of several values (known as energy levels). The second property of quantum is transition that a quantum system can change its state. For example, electrons in an atom can jump to different energy levels when they gain or lose energy. The third property is superposition. A particle can be in a superposition state of upspin and downspin. Apart that, quantum states change over time, which calls quantum evolution. A superposition can be measured by a measure operator, after which the superposition state collapses and loses superposition property.</p><p>With the mathematical framework of quantum theory, quantum states can be represented by vectors with Dirac notation \cite{dirac1939new}, marking quantum states with $|$ and $\rangle$. Vectors describing quantum states are called as state vectors, which can be divided into ket and bra. Ket is column vector and bra is row vector. For a quantum state, its ket and bra are transpose conjugate of each other. Any quantum state in a quantum system can be represented by a linear combination of one basis. The quantum state evolution of a closed quantum system can be described by unitary transformation. Related details about quantum theory are described in the rest of this section.</p><p><strong>Quantum state</strong><br>A quantum state \cite{bennett2000quantum} defined in a Hilbert Space is noted as a ket ($|\psi\rangle$) and its transpose can be noted as a bra ($\langle \psi|$). A quantum bit (qubit) different from classical bit can be in superposition state of $|0\rangle$ and $|1\rangle$. The superposition state can be noted as $|\phi\rangle &#x3D; \alpha |0\rangle + \beta |1\rangle$, where both $\alpha$ and $\beta$ are complex numbers, as well as $\alpha, \beta$ satisfying $|\alpha |^2 + |\beta |^2 &#x3D; 1$. In addition, $|\alpha|^2$ ($|\beta|^2$) represents the probability of state $|0\rangle$ ($|1\rangle$).</p><p>In quantum mechanism, the density matrix of single particle represented as a pure state $\rho$ can be described as $\rho &#x3D; |\psi\rangle \langle \psi|$, where $\rho$ satisfies $\rho &#x3D; \rho^2$. For multiple particles, the total system is a mixed state that can not be represented by a superposition state, which only can be described by a density matrix $\rho &#x3D; \sum_i w_i |\psi_i \rangle \langle \psi_i |$. Similarly, a word consists of different basic senses, which can be viewed as a superposition state. For example, $|apple\rangle &#x3D; \alpha |fruit\rangle + \beta |company\rangle$. A sentence can be viewed as a mixed system for its several words in superposition.</p><p><strong>Evolution</strong><br>The quantum system always changes its state over time, which is called as evolution \cite{bennett2000quantum}. The evolution process can be mathematically described as<br>$$<br>    |\psi_2 \rangle &#x3D; U | \psi_1 \rangle,<br>$$<br>where $U$ is a unitary matrix satisfying $U^\dagger U&#x3D;I$.</p><p>A word or phrase changes its meaning based on historical reasons, which is similar with quantum state evolution. For instance, “silly” stems from Old English “saelig” representing happy and careless, while it represents fool nowadays. We can learn that it changes from context information or big corpus, which can be accomplished by RNN like long and short-term memory (LSTM) \cite{hochreiter1997lstm} and gated recurrent unit (GRU) \cite{chung2014empirical} as well as pretraining model such as BERT and ERNIE.</p><p><strong>Quantum Measurement</strong></p><p>Unlike evolution, measurement \cite{peres2002quantum} representing a non-unitary operation is an irreversible process. Quantum measurement are described with a group of measure operators ${M_m}$, satisfying $\sum_m M^\dagger_m M_m &#x3D; I$. Suppose quantum system state is $|\psi \rangle$ before measure operation, and the probability of measured result $m$ is shown as follows,<br>$$<br>p(m) &#x3D; \langle \psi | M^\dagger_m M_m | \psi \rangle.<br>$$</p><p>After measurement, the system state collapses into<br>$$<br>|\psi ^\prime \rangle &#x3D; \frac{M_m | \psi \rangle} {\sqrt{ \langle \psi | M^\dagger_m M_m | \psi \rangle }},<br>$$</p><p>and the sum of all the measured probabilities is 1:<br>$$<br>\sum_m p(m) &#x3D; \sum_m \langle \psi | M_m^\dagger M_m | \psi \rangle &#x3D; 1.<br>$$</p><p><strong>Complex Neural Network</strong></p><blockquote><p>% Complex nn basis: representation of complex numbers, complex convolution, complex differentiability, complex-valued activations, complex batch normalization, complex weight initialization, complex convolutional residual network.</p></blockquote><p><strong>representation of complex numbers</strong><br>We start by outlining the way in which complex numbers are represented in our framework. A complex number $z &#x3D; a + ib$ has a real component $a$ and an imaginary component $b$. We represent the real part $a$ and the imaginary part $b$ of a complex number as logically distinct real valued entities and simulate complex arithmetic using real-valued arithmetic internally. Consider a typical real-valued $2D$ convolution layer that has $N$ feature maps such that $N$ is divisible by 2; to represent these as complex numbers, we allocate the first $N &#x2F; 2$ feature maps to represent the real components and the remaining $N &#x2F; 2$ to represent the imaginary ones. Thus, for a four dimensional weight tensor $W$ that links $N_{in}$ input feature maps to $N_{out}$ feature maps and whose kernel size is $m \times m$ we would have  a weight tensor of size $(N_{out} \times N_{in} \times m \times m) &#x2F; 2$ complex weights.</p><p><strong>complex convolution</strong></p><p><strong>convolutional LSTM</strong><br>A Convolutional LSTM is similar to a fully connected LSTM. The only difference is that, instead of using matrix multiplications to perform computation, we use convolutional operations. The computation in a real-valued Convolutional LSTM is defined as follows:<br>$$<br>    i_t &#x3D; \sigma(W_{xi} * x_t + W_{hi} * W_{t-1} + b_i) \<br>    f_t &#x3D; \sigma(W_{xf} * x_t + W_{hf} * h_{t-1} + b_f) \<br>    c_t &#x3D; f_t \circ c_{t-1} + i_t \circ \tanh(W_{xc} * x_t + W_{hc} * h_{t-1} + b_c) \<br>    o_t &#x3D; \sigma(W_{xo} * x_t + W_{ho} * h_{t-1} + b_o) \<br>    t_t &#x3D; o_t \circ \tanh(c_t)<br>$$</p><p>Where $\sigma$ denotes the sigmoidal activation function, $\circ$ the elementwise multiplication and $*$ the real-valued convolution. $i_t, f_t, o_t$ represent the vector notation of the input, forget and output gates respectively. $c_t$ and $h_t$ represent the vector notation of the cell and hidden states respectively. the gates and states in a ConvLSTM are tensors whose last two dimensions are spatial dimensions. For each of the gates, $W_{xgate}$  and $W_{hgate}$ are respectively the input and hidden kernels.</p><p>For the Complex Convolutional LSTM, we just replace the real-valued convolutional operation by its complex countpart. We maintain the real-valued elementwise multiplication. The sigmoid and tanh are both performed separately on the real and the imaginary parts.</p><ul><li><p>ComplexGRU</p></li><li><p>complex differentiability</p></li><li><p>complex-valued activations</p></li><li><p>complex batch normalization</p></li><li><p>complex weight initialization</p></li><li><p>complex convolutional residual network</p></li></ul><p>Quantum-inspired Model</p><ul><li><input disabled type="checkbox"> 需要重写</li><li><input disabled type="checkbox"> </li></ul><hr><h3 id="ComplexQNN"><a href="#ComplexQNN" class="headerlink" title="ComplexQNN"></a>ComplexQNN</h3><p>In quantum language models, the Hilbert space is the mathematical foundation of physical events studied. Based on this background, our proposed models are constructed.</p><p>Since a quantum state is usually complex-valued, we therefore introduce the Semantic Hilbert Space $\mathbb{H}^n$ on a complex vector space $\mathbb{C}^n$, spanned by a set of orthogonal bases ${|e_j\rangle}_{j&#x3D;1}^n$. </p><p>Specifically, $ |e_j\rangle$ represents a sememe which is the minimum semantic unit of word meanings in language universals, and is an one-hot vector with only the j-th element in $|e_j\rangle$ being one while all the other elements being zero. A word $w$ is viewed as a physical state in such semantic Hilbert space, and hence can be represented as a superposition of sememes, written as follows:<br>$$<br>|w\rangle &#x3D; \sum_{j&#x3D;1}^n (w_{rj} + i w_{mj}) |e_j \rangle &#x3D; |w_r \rangle + i |w_i\rangle,<br>$$<br>where $|w_r\rangle &#x3D; \sum_{j&#x3D;1}^n w_{rj} |e_j\rangle$ and $|w_i\rangle &#x3D; \sum_{j&#x3D;1}^n w_{mj} |e_j\rangle$ are the real part and imaginary part of the state $|w\rangle$, respectively. And ${ w_{rj} }<em>{j&#x3D;1}^n$, ${w</em>{mj}}_{j&#x3D;1}^n$ are the real part and imaginary part of probability amplitudes along sememes respectively.</p><p>Complex numerical quantum Heuristic neural network (ComplexQNN) is a language model based on complex numerical neural network and quantum computing theory, which is used for downstream tasks of natural language processing. This model is introduced from three aspects: the overall architecture of the model, the construction of specific modules of the model, and the scenarios applicable to the model.</p><p>The first is the overall architecture of the ComplexQNN model, which is depicted in Figure 1 and consists of three modules: complex word embedding, quantum encoder, and classifier. In addition, you can see the ComplexQNN text processing process from Figure 1)Text preprocessing: The text first obtains token sequence through preprocessing (case conversion, word segmentation, word index mapping, filling and truncation). On the other hand, in order to mask the additional token sequence brought by filling sequence, a mask sequence composed of 0 and 1 is also needed to be constructed. The token sequence and mask sequence are the input data of ComplexQNN; 2) Complex word embedding: The function of complex word embedding layer is to map the token number (integer) corresponding to the word to the n-dimensional complex vector space, which corresponds to the construction process of quantum state in the quantum adjustment. Each word mapped from the discrete space to the high-dimensional Hilbert space corresponds to a column vector (namely the right vector). 3) Quantum encoder: the quantum state representation of words is obtained through the embedding layer, but the whole sentence is still composed of a single discrete word. Before the quantum state evolution encoding information, it is necessary to obtain the quantum state representation of sentences. Specifically, the density matrix representation of sentences is obtained through the formula (). The evolution of quantum systems will be simulated below. The evolution of quantum states and density matrices in quantum computing theory is realized by using quantum gates, which correspond to a unitary square matrix with dimensions corresponding to the number of qubits it operates $m&#x3D;2^n$. In the complex numerical neural network, we want to simulate this process through the complex numerical linear layer and the complex numerical cyclic neural network (Note: In a strict sense, linear layer and cyclic neural network guarantee the invariability of dimension by specifying the dimension of input and output, and cannot guarantee that the operation is reversible. In order to satisfy the unitary operation, it needs to use quantum computer to build the network layer, which is why our model is called quantum heuristic model). 4) Classifier: The function of the classifier is to use the encoded information to output the prediction result. There are two ways to construct the classifier. One is to output the prediction result directly through measurement based on the quantum computing theory; the other is to use XXX based on the previous quantum state and linear output is used to predict the results. The difference between the two ideas lies in that the first one complies with the characteristics of quantum computing theory, while the second one regards the representation of quantum states as the characteristics of coding and makes classification prediction through the classical neural network idea.</p><p>The next step is the detailed construction of each of the ComplexQNN modules, the first of which is ComplexEmbedder. As we all know, the classic word embedding layer can be divided into character level, wordpiece level and word level according to the level of word segmentation, and the common implementation is divided into context and non-context word embedding. word2vec and GloVe are two very classic non-context word vectors. Context word embedding is based on the idea of pre-training model. The data set - related contextual word vectors are obtained by fine tuning. We built a plural embedding into our model, Formula $[w_i w_1,…,…, w_n] &#x3D; [r_i r_1,…,…, r_n] + 1 j \ times [i_i i_1,…,…, i_n] $ shows the word embedded process, it consists of two parts, Real part word embedding and imaginary part word embedding, input token sequence, will output the corresponding complex word vector of the sequence. Specifically, we consider different schemes to construct complex word embedding layer. (1) Different pre-training models are used as embedding layers for the real and imaginary parts, BERT for the real part and RoBERTa for the imaginary part. In this way, different information can be encoded into the quantum state and further processed based on the output of complex word embedding in the following modules.</p><p>The second module is the quantum coding layer. The quantum coding layer needs to meet some conditions. The dimension is $2^n$and the input and output dimensions are unchanged. We mentioned above, before coding, need the density matrix of sentence, according to formula ($M &#x3D; \sum_i ^ n | w_i \rangle \langle w_i | $) shows the calculation process of density matrix. We construct our coding layer through the following basic modules: complex numerical fully connected layer, complex numerical cyclic neural network, complex numerical batch regularization. We construct two intermediate module layers for coding: deep coding, recurrent neural network coding (based on ComplexGRU), Graph and graph () are the two intermediate module layers we construct.</p><p>The third module is the classifier. As mentioned above, we have two ways to construct a classifier. Let’s first describe the first way. The first is a measurement method of using the theory of quantum computing, concrete is through formula ($p &#x3D; tr (PV) $), which the probability of p is predicted, and the tr is matrix trace, $P$ is the projection operator $|p\rangle$corresponding density matrix, the density matrix of V is a sentence said, is a random initialization of the nonzero vector projection operator, By formula (), we can perform text dichotomies. Obviously, a group of projection vector bases is needed for multiclassification. The following experiments involve multiclassification, so the number of basis vectors can be determined according to the number of categories of classification results. The final classification result is to take the one with the highest prediction probability as the final prediction result. Another classification idea is to build a cyclic neural network or use a convolutional neural network for pooling operation according to the classical deep learning method. Finally, the results are mapped to the vector dimension of the number of classification categories through the linear layer, and the subscript corresponding to the maximum value of the result is taken as the prediction result.</p><ul><li><input disabled type="checkbox"> 框架图</li><li><input disabled type="checkbox"> 模块细节图</li></ul><hr><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>ComplexTextCNN （这一块可以考虑放到后面写）</p><ul><li><input disabled type="checkbox"> 情感分类</li><li><input disabled type="checkbox"> 实验结果分析</li><li><input disabled type="checkbox"> 消融实验</li></ul><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>In this paper, we propose a new quantum-inspired model ComplexQNN based on complex numerical neural networks, and verify the effectiveness of the model on sentiment classification datasets. At the same time, through the ablation experiment, we prove that the complex numerical quantum-inspired model has more outstanding performance in language modeling than the previous quantum-inspired model and complex numerical neural network.</p><p>The future research direction can be considered in two directions: the first is to encode deeper semantics, such as developing complex numerical Tranformer network module suitable for larger data set and applying it in more complex scenarios, such as machine translation and recommendation system; The second is to build network modules using quantum wiring models, although this is limited in the data set that can be processed in the NISQ era.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;div style=&quot;font-family:verdana;font-size: 22px&quot;&gt; ComplexQNN: A Complex-valued Quantum-inspired Language Model &lt;/div&gt;

&lt;h2 id=&quot;中文版&quot;&gt;&lt;a href=</summary>
      
    
    
    
    <category term="论文" scheme="https://levyya.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
    <category term="论文" scheme="https://levyya.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Allennlp学习</title>
    <link href="https://levyya.github.io/2022/12/05/Allennlp%E5%AD%A6%E4%B9%A0/"/>
    <id>https://levyya.github.io/2022/12/05/Allennlp%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-12-05T09:20:28.000Z</published>
    <updated>2023-01-10T12:35:52.513Z</updated>
    
    <content type="html"><![CDATA[<h3 id="AllenNLP"><a href="#AllenNLP" class="headerlink" title="AllenNLP"></a><a href="https://guide.allennlp.org/common-architectures#2">AllenNLP</a></h3><p>AllenNLP是艾伦人工智能研究院开发的开源NLP平台。软件设计优秀，面对对象思想，值得阅读源码</p><p>入门学习：</p><ol><li><a href="https://www.jianshu.com/p/17abfefc1b5b">AllenNLP使用教程</a></li><li><a href="https://zhuanlan.zhihu.com/p/102324519">学习AllenNLP专栏目录</a></li></ol><p>文本处理过程学习：</p><ol><li><p>language to features</p></li><li><p>Tokenizers and TextFields</p></li><li><p>TokenIndexers</p></li><li><p>TextFieldEmbedders</p></li><li><p>Coordinating the three parts</p></li><li><p>pretrained contextualizers and embeddings</p></li><li><p>word-level modeling with a wordpiece transformer</p></li><li><p>padding and masking</p></li><li><p>Interacting with TextField outputs in your model code</p></li></ol><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><p><code>Field</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter, defaultdict</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> TextField, LabelField, SequenceLabelField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token</span><br><span class="line"><span class="keyword">from</span> allennlp.data.vocabulary <span class="keyword">import</span> Vocabulary</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DatasetReader.register(<span class="params"><span class="string">&#x27;classification-tsv&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassificationTsvReader</span>(<span class="title class_ inherited__">DatasetReader</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.tokenizer = SpacyTokenizer()</span><br><span class="line">        self.token_indexers = &#123;<span class="string">&#x27;tokens&#x27;</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">text_to_instance</span>(<span class="params">self, text: <span class="built_in">str</span>, label: <span class="built_in">str</span> = <span class="literal">None</span></span>) -&gt; Instance:</span><br><span class="line">        tokens = self.tokenizer.tokenize(text)</span><br><span class="line">        text_field = TextField(tokens, self.token_indexers)</span><br><span class="line">        fields = &#123;<span class="string">&#x27;text&#x27;</span>: text_field&#125;</span><br><span class="line">        <span class="keyword">if</span> label:</span><br><span class="line">            fields[<span class="string">&#x27;label&#x27;</span>] = LabelField(label)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_read</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>) -&gt; Iterable[Instance]:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, label = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">                text_field = TextField(self.tokenizer.tokenize(text),</span><br><span class="line">                                       self.token_indexers)</span><br><span class="line">                label_field = LabelField(label)</span><br><span class="line">                fields = &#123;<span class="string">&#x27;text&#x27;</span>: text_field, <span class="string">&#x27;label&#x27;</span>: label_field&#125;</span><br><span class="line">                <span class="keyword">yield</span> Instance(fields)</span><br></pre></td></tr></table></figure><h4 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Model.register(<span class="params"><span class="string">&#x27;simple_classifier&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleClassifier</span>(<span class="title class_ inherited__">Model</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                text: <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor],</span></span><br><span class="line"><span class="params">                label: torch.Tensor = <span class="literal">None</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits)</span><br><span class="line">        output = &#123;<span class="string">&#x27;probs&#x27;</span>: probs&#125;</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.accuracy(logits, label)</span><br><span class="line">            <span class="comment"># Shape: (1,)</span></span><br><span class="line">            output[<span class="string">&#x27;loss&#x27;</span>] = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>注：Model 是 torch.nn.Module 的一个子类，forward() 输出是字典</p><p>预测过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Predictor.register(<span class="params"><span class="string">&quot;sentence_classifier&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SentenceClassifierPredictor</span>(<span class="title class_ inherited__">Predictor</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, sentence: <span class="built_in">str</span></span>) -&gt; JsonDict:</span><br><span class="line">        <span class="comment"># This method is implemented in the base class.</span></span><br><span class="line">        <span class="keyword">return</span> self.predict_json(&#123;<span class="string">&quot;sentence&quot;</span>: sentence&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_json_to_instance</span>(<span class="params">self, json_dict: JsonDict</span>) -&gt; Instance:</span><br><span class="line">        sentence = json_dict[<span class="string">&quot;sentence&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> self._dataset_reader.text_to_instance(sentence)</span><br></pre></td></tr></table></figure><p>损失函数</p><p>查看模型预测结果 (评估标准) </p><p>保存和读取模型</p><p>正则化</p><hr><h4 id="公共结构"><a href="#公共结构" class="headerlink" title="公共结构"></a>公共结构</h4><p>Summarizing sequences</p><p><code>seq2vec</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.modules.seq2vec_encoders <span class="keyword">import</span> (</span><br><span class="line">Seq2VecEncoder,</span><br><span class="line">    CnnEncoder,</span><br><span class="line">    LstmSeq2VecEncoder,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Contextualizing sequences</p><p><code>seq2seq</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.modules.seq2seq_encoders <span class="keyword">import</span> (</span><br><span class="line">Seq2SeqEncoder,</span><br><span class="line">    PassThroughEncoder,</span><br><span class="line">    LstmSeq2SeqEncoder,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>思考：为什么使用 LstmSeq2VecEncoder 而不是 torch.nn.LSTM ?</p><blockquote><p>The reasons you might want to use a <code>Seq2SeqEncoder</code> instead are three-fold: </p><p>first, it encourages you to think at a higher level about what basic operations your model is doing (am I contextualizing, summarizing, or both?). </p><p>Second, using <a href="https://guide.allennlp.org/using-config-files#1">dependency injection</a> allows you to do controlled experiments easier, if you think you might one day want to try a different contextualizer in your model. Using an abstraction that encapsulates the options you want to experiment with is a powerful way to get very easy, controlled experiments. </p><p>Lastly, having a collection of models available that are written using higher-level abstractions lets component designers easily test their developments on a wide range of models.</p></blockquote><h4 id="文本表示"><a href="#文本表示" class="headerlink" title="文本表示"></a>文本表示</h4><p>将词语转成向量的几种方法 （非上下文、上下文Contextual）</p><ul><li>GloVe or word2vec embeddings</li><li>Character CNNs</li><li>POS tag embeddings</li><li>Combination of GloVe and character CNNs</li><li>wordpieces and BERT</li></ul><p><strong>Three steps</strong> (tokenize, index, embedding) in converting language to features</p><p>Text -&gt; Tokens -&gt; Ids -&gt; Vectors</p><p>具体地</p><ol><li>Tokenizer (Text -&gt; Tokens)</li><li>TextField, TokenIndexer and Vocabulary (Tokens -&gt; Ids)</li><li>TextFieldEmbedder (Ids -&gt; Vectors)</li></ol><p>Tokenizers</p><ul><li>Characters (“AllenNLP is great” → <code>[&quot;A&quot;, &quot;l&quot;, &quot;l&quot;, &quot;e&quot;, &quot;n&quot;, &quot;N&quot;, &quot;L&quot;, &quot;P&quot;, &quot; &quot;, &quot;i&quot;, &quot;s&quot;, &quot; &quot;, &quot;g&quot;, &quot;r&quot;, &quot;e&quot;, &quot;a&quot;, &quot;t&quot;]</code>)</li><li>Wordpieces (“AllenNLP is great” → <code>[&quot;Allen&quot;, &quot;##NL&quot;, &quot;##P&quot;, &quot;is&quot;, &quot;great&quot;]</code>)</li><li>Words (“AllenNLP is great” → <code>[&quot;AllenNLP&quot;, &quot;is&quot;, &quot;great&quot;]</code>)</li></ul><p>note: Wordpieces are similar to words, but further split words into <code>subword units</code>.</p><p>常用的分词器：<code>SpacyTokenizer</code>, <code>PretrainedTransformerTokenizer</code>, <code>CharacterTokenizer</code></p><ul><li>character</li><li>wordpiece</li><li>word</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = ...</span><br><span class="line">sentence = <span class="string">&quot;We are learning about TextFields&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(sentence)</span><br><span class="line">token_indexers = &#123;...&#125;</span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">...</span><br><span class="line">instance = Instance(&#123;<span class="string">&quot;sentence&quot;</span>: text_field, ...&#125;)</span><br></pre></td></tr></table></figure><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a><strong>模型训练</strong></h4><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">allennlp train \</span><br><span class="line">    my_text_classifier.jsonnet \</span><br><span class="line">    --serialization-<span class="built_in">dir</span> model-bert \</span><br><span class="line">    --include-package my_text_classifier</span><br></pre></td></tr></table></figure><p>train后面的参数指定了用哪个配置文件，-s后面的目录指定了训练日志、字典、模型等的存放位置，<code>--include-package</code>后面的参数指定了我们前面编写的python代码在哪里</p><p>train 参数</p><ul><li>-f  强制重写输出目录</li><li>–dry-run 加载数据不训练</li></ul><h4 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h4><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python -m allennlp.service.server_simple \</span><br><span class="line">    --archive-<span class="built_in">path</span> /<span class="built_in">path</span>/<span class="keyword">for</span>/model/and/log/model.tar.gz \</span><br><span class="line">    --predictor text_classifier \</span><br><span class="line">    --include-package AllenFrame.classification_code \</span><br><span class="line">    --<span class="built_in">title</span> &quot;classification&quot; \</span><br><span class="line">    --field-name sentence</span><br></pre></td></tr></table></figure><p>conda update -n base conda</p><h4 id="Trick"><a href="#Trick" class="headerlink" title="Trick"></a>Trick</h4><ul><li><p>多GPU</p></li><li><p><a href="https://medium.com/optuna/hyperparameter-optimization-for-allennlp-using-optuna-54b4bfecd78b">使用Optuna进行超参数优化</a></p></li></ul><p><a href="https://blog.allenai.org/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad">使用少量内存训练大批次</a></p><ol><li>梯度累计 num_gradient_accumulation_steps</li><li>梯度检查点 gradient_checkpointing&#x3D;True</li><li>自动混合精度 use_amp</li></ol><h4 id="Jsonnet"><a href="#Jsonnet" class="headerlink" title="Jsonnet"></a>Jsonnet</h4><p><a href="https://jsonnet.org/learning/tutorial.html">tutorial</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;AllenNLP&quot;&gt;&lt;a href=&quot;#AllenNLP&quot; class=&quot;headerlink&quot; title=&quot;AllenNLP&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://guide.allennlp.org/common-architectures#2&quot;&gt;Al</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    <category term="NLP" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/NLP/"/>
    
    
    <category term="NLP" scheme="https://levyya.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读--Quanthoven</title>
    <link href="https://levyya.github.io/2022/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quanthoven/"/>
    <id>https://levyya.github.io/2022/12/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Quanthoven/</id>
    <published>2022-12-05T06:57:23.000Z</published>
    <updated>2023-01-10T12:35:54.249Z</updated>
    
    <content type="html"><![CDATA[<p>链接：<a href="https://arxiv.org/pdf/2111.06741.pdf">click</a></p><p>作者：</p><blockquote><p>Eduardo Reck Miranda1,2, Richie Yeung2 , Anna Pearson2 , Konstantinos Meichanetzidis2 , and Bob Coecke2 </p><p>1 ICCMR, University of Plymouth, Plymouth, UK </p><p>2Cambridge Quantum, Oxford, UK </p><p>1 <a href="mailto:&#x65;&#100;&#117;&#x61;&#x72;&#100;&#x6f;&#46;&#x6d;&#105;&#x72;&#x61;&#x6e;&#100;&#x61;&#64;&#112;&#x6c;&#121;&#x6d;&#x6f;&#117;&#x74;&#x68;&#x2e;&#97;&#x63;&#46;&#117;&#x6b;">&#x65;&#100;&#117;&#x61;&#x72;&#100;&#x6f;&#46;&#x6d;&#105;&#x72;&#x61;&#x6e;&#100;&#x61;&#64;&#112;&#x6c;&#121;&#x6d;&#x6f;&#117;&#x74;&#x68;&#x2e;&#97;&#x63;&#46;&#117;&#x6b;</a> </p><p>2 {richie.yeung, anna.pearson, k.mei, bob.coecke}@cambridgequantum.com</p></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>音乐领域的人工智能(AI)已经取得了巨大的进展，特别是在音乐作曲和通过互联网获取大型数据库以实现商业化方面。我们有兴趣进一步推进这一领域，专注于作曲。与目前的“黑箱”人工智能方法相比，我们支持生成音乐系统的可解释的作曲观点。特别是，受音乐语法的激励，我们正在从分布式作曲分类(<strong>DisCoCat</strong>)建模框架(NLP)中引入方法。量子计算是一项新兴技术，很有可能在不久的将来影响音乐产业。因此，我们正在开创量子自然语言处理(QNLP)方法来开发新一代智能音乐系统。这项工作遵循了先前在量子硬件上实现DisCoCat语言模型的实验。在本章中，我们介绍了有史以来第一个概念证明——<strong>Quanthoven</strong>，它(a)证明了编程量子计算机来学习对传递不同含义的音乐进行分类是可能的，(b)说明了如何利用这种能力来开发一个系统来创作有意义的音乐片段。在讨论了我们目前对音乐作为一种通信媒介及其与自然语言的关系的理解之后，本章重点讨论了开发的技术(a)<strong>将音乐作品编码为量子电路</strong>(b)<strong>设计量子分类器</strong>。本章最后演示了用该系统创建的组合。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在本章中，我们介绍了有史以来第一个概念证明——<strong>Quanthoven</strong>，它(a)证明了编程量子计算机学习对传递不同含义的音乐进行分类是可能的，(b)说明了如何利用这种能力开发能够创作出有意义音乐的人工智能系统。本章其余部分的结构如下:</p><p>第二部分探讨了我们目前对音乐作为一种交流媒介及其与自然语言的关系的理解，这是我们研究的动力所在。</p><p>第3节介绍了我们在音乐和算法作曲的计算建模方法方面的工作。</p><p>第4节介绍了<strong>量子计算的基础知识</strong>，刚好可以跟上本章中讨论的量子分类器(即机器学习算法)的技术方面。</p><p>第5节介绍了<strong>分布式组合范畴建模</strong>(Distributional composition Categorical, DisCoCat)，该模型利用范畴理论将自然语言和量子力学统一起来。它还展示了DisCoCat如何模拟音乐。</p><p>•第6节描述了我们为这个项目开发的DisCoCat音乐模型，包括使用定制语法生成音乐片段并转换为量子电路的过程。阐述了机器学习算法和训练方法。</p><p>第7节说明如何利用分类的结果生成两类音乐作品。这一章以最后的评语结束。一些附录提供了补充信息。</p><p>讨论：</p><p>音乐能否传递信息？</p><p>观点：小调和弦传递悲伤，大调和弦传递快乐</p><p>音乐和语言的关系：</p><p>音乐和语言具有相似的内部结构，音乐有助于第二语言的学习，母语发音区分持续时间的要比发音不区分持续时间的在掌握音乐节奏中做的更好</p><p>音乐语法：</p><blockquote><p>我们根据自己的思维模式来分析和解释听觉流。我们不妨把这种结构称为音乐语法。我们给印象的标签就是意义。</p></blockquote><p>现有方法：</p><ol><li><p>人工智能</p><p>模拟现有的特定风格的音乐</p></li><li><p>算法方法</p><p>将看起来非音乐现象的数据转化为音乐，可以得到比较新奇的音乐</p></li></ol><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;链接：&lt;a href=&quot;https://arxiv.org/pdf/2111.06741.pdf&quot;&gt;click&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Eduardo Reck Miranda1,2, Richie Yeung2 , Anna </summary>
      
    
    
    
    <category term="论文阅读" scheme="https://levyya.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="论文" scheme="https://levyya.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="Quantum" scheme="https://levyya.github.io/tags/Quantum/"/>
    
  </entry>
  
  <entry>
    <title>云原生</title>
    <link href="https://levyya.github.io/2022/12/05/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    <id>https://levyya.github.io/2022/12/05/%E4%BA%91%E5%8E%9F%E7%94%9F/</id>
    <published>2022-12-05T01:35:48.000Z</published>
    <updated>2023-01-10T12:35:53.989Z</updated>
    
    <content type="html"><![CDATA[<img src="/2022/12/05/%E4%BA%91%E5%8E%9F%E7%94%9F/cloud-native-for-dev-ops-1.png" class alt="1"><p>三款云原生平台级产品</p><ol><li>Rancher</li><li>KubeSphere</li><li>Rainbond</li></ol><p>使用要求：</p><ul><li>微服务架构</li><li>kubernetes</li><li>容器</li></ul><p>技术概念</p><p>DN（网络自动化管理）、SDS（存储自动化管理）、Helm（复杂应用交付自动化）、Service Mesh（无侵入扩展服务治理能力）、Monitoring（监控自动化）、Logging（日志自动化）、Tracing（性能分析自动化）、Chaos engineering（容错自动化）、Gateway（网关自动化）、SPIFFE （应用访问安全自动化）</p><p>Workload、Pod、Service、Ingress、ConfigMap、PV</p><p>云原生实现的关键问题</p><ol><li><p>云原生落地的难点在使用</p></li><li><p>业务跟运维能力解耦，跟微服务框架解耦</p></li><li><p>根据不同客户类型，自定义交付流程和自动化交付</p></li></ol><p>参考</p><p><a href="http://docs.kubernetes.org.cn/">kubernetes中文文档</a></p><p><a href="https://www.kubernetes.org.cn/">kubernetes中文社区</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;/2022/12/05/%E4%BA%91%E5%8E%9F%E7%94%9F/cloud-native-for-dev-ops-1.png&quot; class alt=&quot;1&quot;&gt;

&lt;p&gt;三款云原生平台级产品&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rancher&lt;/li&gt;
&lt;l</summary>
      
    
    
    
    <category term="计算机" scheme="https://levyya.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    
    
    <category term="云原生" scheme="https://levyya.github.io/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
  </entry>
  
</feed>
